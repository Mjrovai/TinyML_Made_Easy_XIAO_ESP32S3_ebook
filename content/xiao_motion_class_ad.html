<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>TinyML Made Easy - 7&nbsp; Motion Classification and Anomaly Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../references.html" rel="next">
<link href="../content/dsp_spectral_features_block/dsp_spectral_features_block.html" rel="prev">
<link href="../cover.jpg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">TinyML Made Easy</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto tools-wide">
    <a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../TinyML-Made-Easy.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="../TinyML-Made-Easy.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools ms-auto-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../content/xiao_motion_class_ad.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Motion Classification and Anomaly Detection</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about_book.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/xiao_esp32s3_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">XIAO ESP32S3 Sense Setup</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/xiao_image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Image Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/xiao_object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Object Detection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Audio Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/xiao_kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Keyword Spotting (KWS)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">DSP - Spectral Features</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../content/xiao_motion_class_ad.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Motion Classification and Anomaly Detection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">7.1</span> Introduction</a></li>
  <li><a href="#installing-the-imu" id="toc-installing-the-imu" class="nav-link" data-scroll-target="#installing-the-imu"><span class="header-section-number">7.2</span> Installing the IMU</a></li>
  <li><a href="#the-tinyml-motion-classification-project" id="toc-the-tinyml-motion-classification-project" class="nav-link" data-scroll-target="#the-tinyml-motion-classification-project"><span class="header-section-number">7.3</span> The TinyML Motion Classification Project</a></li>
  <li><a href="#connecting-the-device-to-edge-impulse" id="toc-connecting-the-device-to-edge-impulse" class="nav-link" data-scroll-target="#connecting-the-device-to-edge-impulse"><span class="header-section-number">7.4</span> Connecting the device to Edge Impulse</a></li>
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">7.5</span> Data Collection</a></li>
  <li><a href="#data-pre-processing" id="toc-data-pre-processing" class="nav-link" data-scroll-target="#data-pre-processing"><span class="header-section-number">7.6</span> Data Pre-Processing</a></li>
  <li><a href="#model-design" id="toc-model-design" class="nav-link" data-scroll-target="#model-design"><span class="header-section-number">7.7</span> Model Design</a></li>
  <li><a href="#impulse-design" id="toc-impulse-design" class="nav-link" data-scroll-target="#impulse-design"><span class="header-section-number">7.8</span> Impulse Design</a></li>
  <li><a href="#generating-features" id="toc-generating-features" class="nav-link" data-scroll-target="#generating-features"><span class="header-section-number">7.9</span> Generating features</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">7.10</span> Training</a></li>
  <li><a href="#testing" id="toc-testing" class="nav-link" data-scroll-target="#testing"><span class="header-section-number">7.11</span> Testing</a></li>
  <li><a href="#deploy" id="toc-deploy" class="nav-link" data-scroll-target="#deploy"><span class="header-section-number">7.12</span> Deploy</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference"><span class="header-section-number">7.13</span> Inference</a></li>
  <li><a href="#conclusion." id="toc-conclusion." class="nav-link" data-scroll-target="#conclusion."><span class="header-section-number">7.14</span> Conclusion.</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/edit/main/content/xiao_motion_class_ad.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/Mjrovai/TinyML_Made_Easy_XIAO_ESP32S3_eBook/blob/main/content/xiao_motion_class_ad.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Motion Classification and Anomaly Detection</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/jpeg/motion_class_ad/ini.jpg" class="img-fluid figure-img" style="width:6.5in"></p>
<figcaption class="figure-caption"><em>DALL·E prompt - 1950s style cartoon illustration set in a vintage audio lab. Scientists, dressed in classic attire with white lab coats, are intently analyzing audio data on large chalkboards. The boards display intricate FFT (Fast Fourier Transform) graphs and time-domain curves. Antique audio equipment is scattered around, but the data representations are clear and detailed, indicating their focus on audio analysis.</em></figcaption>
</figure>
</div>
<section id="introduction" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">7.1</span> Introduction</h2>
<p>The XIAO ESP32S3 Sense, with its built-in camera and mic, is a versatile device. But what if you need to add another type of sensor, such as an IMU? No problem! One of the standout features of the XIAO ESP32S3 is its multiple pins that can be used as an I2C bus (SDA/SCL pins), making it a suitable platform for sensor integration.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590599/image_GstFLMyDUy.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="installing-the-imu" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="installing-the-imu"><span class="header-section-number">7.2</span> Installing the IMU</h2>
<p>When selecting your IMU, the market offers a wide range of devices, each with unique features and capabilities. You could choose, for example, the ADXL362 (3-axis), MAX21100 (6-axis), MPU6050 (6-axis), LIS3DHTR (3-axis), or the LCM20600Seeed Grove— (6-axis), which is part of the IMU 9DOF (lcm20600+AK09918). This variety allows you to tailor your choice to your project’s specific needs.</p>
<p>For this project, we will use an IMU, the MPU6050 (or 6500), a low-cost (less than 2.00 USD) 6-axis Accelerometer/Gyroscope unit.</p>
<blockquote class="blockquote">
<p>At the end of the lab, we will also comment on using the LCM20600.</p>
</blockquote>
<p>The <a href="https://invensense.tdk.com/download-pdf/mpu-6500-datasheet/">MPU-6500</a> is a 6-axis Motion Tracking device that combines a 3-axis gyroscope, 3-axis accelerometer, and a Digital Motion ProcessorTM (DMP) in a small 3x3x0.9mm package. It also features a 4096-byte FIFO that can lower the traffic on the serial bus interface and reduce power consumption by allowing the system processor to burst read sensor data and then go into a low-power mode.</p>
<p>With its dedicated I2C sensor bus, the MPU-6500 directly accepts inputs from external I2C devices. MPU-6500, with its 6-axis integration, on-chip DMP, and run-time calibration firmware, enables manufacturers to eliminate the costly and complex selection, qualification, and system-level integration of discrete devices, guaranteeing optimal motion performance for consumers. MPU-6500 is also designed to interface with multiple non-inertial digital sensors, such as pressure sensors, on its auxiliary I2C port.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590608/image_ZFuJgZIdRi.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>Usually, the libraries available are for MPU6050, but they work for both devices.</p>
</blockquote>
<p><strong>Connecting the HW</strong></p>
<p>Connect the IMU to the XIAO according to the below diagram:</p>
<ul>
<li>MPU6050 <strong>SCL</strong> –&gt; XIAO <strong>D5</strong></li>
<li>MPU6050 <strong>SDA</strong> –&gt; XIAO <strong>D4</strong></li>
<li>MPU6050 <strong>VCC</strong> –&gt; XIAO <strong>3.3V</strong></li>
<li>MPU6050 <strong>GND</strong> –&gt; XIAO <strong>GND</strong></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590645/drawing_Vp4G8xChAB.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Drawing.png</figcaption>
</figure>
</div>
<p><strong>Install the Library</strong></p>
<p>Go to Arduino Library Manager and type MPU6050. Install the latest version.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590642/pasted_graphic_16_CH1rHB6s2M.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 16.png</figcaption>
</figure>
</div>
<p>Download the sketch <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU/MPU6050_Acc_Data_Acquisition">MPU6050_Acc_Data_Acquisition.in</a>:</p>
<pre><code>/*
 * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class by Jeff Rowberg &lt;jeff@rowberg.net&gt;
 * and Edge Impulse Data Forwarder Exampe (Arduino) - https://docs.edgeimpulse.com/docs/cli-data-forwarder
 * 
 * Developed by M.Rovai @11May23
 */

#include "I2Cdev.h"
#include "MPU6050.h"
#include "Wire.h"

#define FREQUENCY_HZ        50
#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))
#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G

// convert factor g to m/s2 ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]
#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) 

static unsigned long last_interval_ms = 0;

MPU6050 imu;
int16_t ax, ay, az;

void setup() {
  
    Serial.begin(115200);

    
    // initialize device
    Serial.println("Initializing I2C devices...");
    Wire.begin();
    imu.initialize();
    delay(10);
    
//    // verify connection
//    if (imu.testConnection()) {
//      Serial.println("IMU connected");
//    }
//    else {
//      Serial.println("IMU Error");
//    }
    delay(300);
    
    //Set MCU 6050 OffSet Calibration 
    imu.setXAccelOffset(-4732);
    imu.setYAccelOffset(4703);
    imu.setZAccelOffset(8867);
    imu.setXGyroOffset(61);
    imu.setYGyroOffset(-73);
    imu.setZGyroOffset(35);
    
    /* Set full-scale accelerometer range.
     * 0 = +/- 2g
     * 1 = +/- 4g
     * 2 = +/- 8g
     * 3 = +/- 16g
     */
    imu.setFullScaleAccelRange(ACC_RANGE);
}

void loop() {

      if (millis() &gt; last_interval_ms + INTERVAL_MS) {
        last_interval_ms = millis();
        
        // read raw accel/gyro measurements from device
        imu.getAcceleration(&amp;ax, &amp;ay, &amp;az);

        // converting to m/s2
        float ax_m_s2 = ax * CONVERT_G_TO_MS2;
        float ay_m_s2 = ay * CONVERT_G_TO_MS2;
        float az_m_s2 = az * CONVERT_G_TO_MS2;

        Serial.print(ax_m_s2); 
        Serial.print("\t");
        Serial.print(ay_m_s2); 
        Serial.print("\t");
        Serial.println(az_m_s2); 
      }
}</code></pre>
<p><strong>Some comments about the code:</strong></p>
<p>Note that the values generated by the accelerometer and gyroscope have a range: [-32768, +32767], so for example, if the default accelerometer range is used, the range in Gs should be: [-2g, +2g]. So, “1G” means 16384.</p>
<p>For conversion to m/s2, for example, you can define the following:</p>
<pre><code>#define CONVERT_G_TO_MS2 (9.81/16384.0)</code></pre>
<p>In the code, I left an option (ACC_RANGE) to be set to 0 (+/-2G) or 1 (+/- 4G). We will use +/-4G; that should be enough for us. In this case.</p>
<p>We will capture the accelerometer data on a frequency of 50Hz, and the acceleration data will be sent to the Serial Port as meters per squared second (m/s2).</p>
<p>When you ran the code with the IMU resting over your table, the accelerometer data shown on the Serial Monitor should be around 0.00, 0.00, and 9.81. If the values are a lot different, you should calibrate the IMU.</p>
<p>The MCU6050 can be calibrated using the sketch: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU/mcu6050-calibration">mcu6050-calibration.ino</a>.</p>
<p>Run the code. The following will be displayed on the Serial Monitor:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590654/pasted_graphic_19_FhU4qX0dLU.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 19.png</figcaption>
</figure>
</div>
<p>Send any character (in the above example, “x”), and the calibration should start.</p>
<blockquote class="blockquote">
<p>Note that A message MPU6050 connection failed. Ignore this message. For some reason, imu.testConnection() is not returning a correct result.</p>
</blockquote>
<p>In the end, you will receive the offset values to be used on all your sketches:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590656/pasted_graphic_20_Tui5mRNqOL.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 20.png</figcaption>
</figure>
</div>
<p>Take the values and use them on the setup:</p>
<pre><code>//Set MCU 6050 OffSet Calibration 
imu.setXAccelOffset(-4732);
imu.setYAccelOffset(4703);
imu.setZAccelOffset(8867);
imu.setXGyroOffset(61);
imu.setYGyroOffset(-73);
imu.setZGyroOffset(35);</code></pre>
<p>Now, run the sketch <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU/MPU6050_Acc_Data_Acquisition">MPU6050_Acc_Data_Acquisition.in:</a></p>
<p>Once you run the above sketch, open the Serial Monitor:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590659/pasted_graphic_21_DTRap3UbE7.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 21.png</figcaption>
</figure>
</div>
<p>Or check the Plotter:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590660/pasted_graphic_23_hM0BpXdmeI.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 23.png</figcaption>
</figure>
</div>
<p>Move your device in the three axes. You should see the variation on Plotter:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590661/pasted_graphic_22_qOS34YmKic.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Pasted Graphic 22.png</figcaption>
</figure>
</div>
</section>
<section id="the-tinyml-motion-classification-project" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="the-tinyml-motion-classification-project"><span class="header-section-number">7.3</span> The TinyML Motion Classification Project</h2>
<p>For our lab, we will simulate mechanical stresses in transport. Our problem will be to classify four classes of movement:</p>
<ul>
<li><strong>Maritime</strong> (pallets in boats)</li>
<li><strong>Terrestrial</strong> (palettes in a Truck or Train)</li>
<li><strong>Lift</strong> (Palettes being handled by Fork-Lift)</li>
<li><strong>Idle</strong> (Palettes in Storage houses)</li>
</ul>
<p>So, to start, we should collect data. Then, accelerometers will provide the data on the palette (or container).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590536/data1_sg5MS6KfkM.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<p>From the above images, we can see that primarily horizontal movements should be associated with the “Terrestrial class,” Vertical movements with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to <a href="https://www.containerhandbuch.de/chb_e/stra/index.html?/chb_e/stra/stra_02_03_03.htm">Maritime class.</a></p>
</section>
<section id="connecting-the-device-to-edge-impulse" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="connecting-the-device-to-edge-impulse"><span class="header-section-number">7.4</span> Connecting the device to Edge Impulse</h2>
<p>For data collection, we should first connect our device to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.</p>
<blockquote class="blockquote">
<p>Follow the instructions <a href="https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-installation">here</a>to install the <a href="https://nodejs.org/en/">Node.js</a>and Edge Impulse CLI on your computer.</p>
</blockquote>
<p>Once the XIAO ESP32S3 is not a fully supported development board by Edge Impulse, we should, for example, use the <a href="https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder">CLI Data Forwarder t</a>o capture data from our sensor and send it to the Studio, as shown in this diagram:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590537/image_PHK0GELEYh.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>You can alternately capture your data “offline,” store them on an SD card or send them to your computer via Bluetooth or Wi-Fi. In this <a href="https://youtu.be/2KBPq_826WM">video</a>, you can learn alternative ways to send data to the Edge Impulse Studio.</p>
</blockquote>
<p>Connect your device to the serial port and run the previous code to capture IMU (Accelerometer) data, “printing them” on the serial. This will allow the Edge Impulse Studio to “capture” them.</p>
<p>Go to the Edge Impulse page and create a project.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590663/image_xUyC0uWhnG.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>The maximum length for an Arduino library name is <strong>63 characters</strong>. Note that the Studio will name the final library using your project name and include “_inference” to it. The name I chose initially did not work when I tried to deploy the Arduino library because it resulted in 64 characters. So, I need to change it by taking out the “anomaly detection” part.</p>
</blockquote>
<p>Start the <a href="https://docs.edgeimpulse.com/docs/edge-impulse-cli/cli-data-forwarder">CLI Data Forwarder</a>on your terminal, entering (if it is the first time) the following command:</p>
<p>$ edge-impulse-data-forwarder –clean</p>
<pre><code>$ edge-impulse-data-forwarder --clean</code></pre>
<p>Next, enter your EI credentials and choose your project, variables, and device names:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590664/image_qkRsm7A981.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>Go to your EI Project and verify if the device is connected (the dot should be green):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590667/image_a5J303wHbE.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="data-collection" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">7.5</span> Data Collection</h2>
<p>As discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1591091/boat_aOqDzqArqs.jpg?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">boat.jpg</figcaption>
</figure>
</div>
<p>Now imagine your container is on a boat, facing an angry ocean, on a truck, etc.:</p>
<ul>
<li><strong>Maritime</strong> (pallets in boats)
<ul>
<li>Move the XIAO in all directions, simulating an undulatory boat movement.</li>
</ul></li>
<li><strong>Terrestrial</strong> (palettes in a Truck or Train)
<ul>
<li>Move the XIAO over a horizontal line.</li>
</ul></li>
<li><strong>Lift</strong> (Palettes being handled by
<ul>
<li>Move the XIAO over a vertical line.</li>
</ul></li>
<li><strong>Idle</strong> (Palettes in Storage houses)
<ul>
<li>Leave the XIAO over the table.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590677/idle_OiZWwciVVh.jpg?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">idle.jpg</figcaption>
</figure>
</div>
<p>Below is one sample (raw data) of 10 seconds:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590541/image_E3mFL7tvSh.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<p>You can capture, for example, 2 minutes (twelve samples of 10 seconds each) for the four classes. Using the “3 dots” after each one of the samples, select 2, moving them for the Test set (or use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab). Below, you can see the result datasets:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590679/image_WB3eKzzN6R.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="data-pre-processing" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="data-pre-processing"><span class="header-section-number">7.6</span> Data Pre-Processing</h2>
<p>The raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590693/image_KQNIPcxqXV.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>We can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50Hz. A 2-second window will capture 300 data points (3 axis x 2 seconds x 50 samples). We will slide this window each 200ms, creating a larger dataset where each instance has 300 raw features.</p>
<blockquote class="blockquote">
<p>You should use the best SR for your case, considering Nyquist’s theorem, which states that a periodic signal must be sampled at more than twice the signal’s highest frequency component.</p>
</blockquote>
<p>Data preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.</p>
<p>On the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the <strong>Time Domain Statistical features</strong> per axis/channel are:</p>
<ul>
<li>RMS</li>
<li>Skewness</li>
<li>Kurtosis</li>
</ul>
<p>And the <strong>Frequency Domain Spectral features</strong> per axis/channel are:</p>
<ul>
<li>Spectral Power</li>
<li>Skewness</li>
<li>Kurtosis</li>
</ul>
<p>So, for example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).</p>
<p>Those 63 features will be the Input Tensor of a Neural Network Classifier and the Anomaly Detection model (K-Means).</p>
<blockquote class="blockquote">
<p>You can learn more by digging into the lab <em>DSP - Spectral Features</em></p>
</blockquote>
</section>
<section id="model-design" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="model-design"><span class="header-section-number">7.7</span> Model Design</h2>
<p>Our classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590702/image_ojSbkXrKse.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="impulse-design" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="impulse-design"><span class="header-section-number">7.8</span> Impulse Design</h2>
<p>An impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.</p>
<p>We also take advantage of a second model, the K-means, that can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590547/image_pFnNVK4Wjc.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>Imagine our XIAO rolling or moving upside-down, on a movement complement different from the one trained</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590548/image_iW1ygppsHi.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">img</figcaption>
</figure>
</div>
<p>Below is our final Impulse design:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590696/image_W8xMffuTwP.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="generating-features" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="generating-features"><span class="header-section-number">7.9</span> Generating features</h2>
<p>At this point in our project, we have defined the pre-processing method and the model designed. Now, it is time to have the job done. First, let’s take the raw data (time-series type) and convert it to tabular data. Go to the Spectral Features tab and select Save Parameters:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590697/image_bsHjHtleGs.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>At the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.</p>
<blockquote class="blockquote">
<p>The Feature Explorer will show those data in 2D using <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP.</a> Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but also for general non-linear dimension reduction.</p>
</blockquote>
<p>The visualization allows one to verify that the classes present an excellent separation, which indicates that the classifier should work well.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590706/image_fyynJu1laN.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>Optionally, you can analyze the relative importance of each feature for one class compared with other classes.</p>
</section>
<section id="training" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="training"><span class="header-section-number">7.10</span> Training</h2>
<p>Our model has four layers, as shown below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590707/image_0M4u1e4dJI.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>As hyperparameters, we will use a Learning Rate of 0.005 and 20% of data for validation for 30 epochs. After training, we can see that the accuracy is 97%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590709/image_cCscB5HMw9.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>For anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590710/image_8IOqOw1yoX.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="testing" class="level2" data-number="7.11">
<h2 data-number="7.11" class="anchored" data-anchor-id="testing"><span class="header-section-number">7.11</span> Testing</h2>
<p>Using 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was not that good (8%), mainly due to the terrestrial class. Once we have four classes (which output should add 1.0), we can set up a lower threshold for a class to be considered valid (for example, 0.4):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590714/image_ecSV5fIlPu.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>Now, the Test accuracy will go up to 97%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590715/image_TnLYYt60Vc.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>You should also use your device (which is still connected to the Studio) and perform some Live Classification.</p>
<blockquote class="blockquote">
<p>Be aware that here you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device).</p>
</blockquote>
</section>
<section id="deploy" class="level2" data-number="7.12">
<h2 data-number="7.12" class="anchored" data-anchor-id="deploy"><span class="header-section-number">7.12</span> Deploy</h2>
<p>Now it is time for magic˜! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and Build. A Zip file will be created and downloaded to your computer.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590716/image_d5jrYgBErG.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>On your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590717/image_6w7t1NYsBV.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
</section>
<section id="inference" class="level2" data-number="7.13">
<h2 data-number="7.13" class="anchored" data-anchor-id="inference"><span class="header-section-number">7.13</span> Inference</h2>
<p>Now, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.</p>
<p>In your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select nano_ble_sense_accelerometer:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1590718/image_M3k3wqDRto.png?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">image.png</figcaption>
</figure>
</div>
<p>Of course, this is not your board, but we can have the code working with only a few changes.</p>
<p>For example, at the beginning of the code, you have the library related to Arduino Sense IMU:</p>
<pre><code>/* Includes --------------------------------------------------------------- */
#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;
#include &lt;Arduino_LSM9DS1.h&gt;</code></pre>
<p>Change the “includes” portion with the code related to the IMU:</p>
<pre><code>#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;
#include "I2Cdev.h"
#include "MPU6050.h"
#include "Wire.h"</code></pre>
<p>Change the Constant Defines</p>
<pre><code>/* Constant defines ------------------------------------------------------- */
MPU6050 imu;
int16_t ax, ay, az;

#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G
#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))
#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE</code></pre>
<p>On the setup function, initiate the IMU set the off-set values and range:</p>
<pre><code>// initialize device
Serial.println("Initializing I2C devices...");
Wire.begin();
imu.initialize();
delay(10);

//Set MCU 6050 OffSet Calibration 
imu.setXAccelOffset(-4732);
imu.setYAccelOffset(4703);
imu.setZAccelOffset(8867);
imu.setXGyroOffset(61);
imu.setYGyroOffset(-73);
imu.setZGyroOffset(35);

imu.setFullScaleAccelRange(ACC_RANGE);</code></pre>
<p>At the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. On the original code, you have the line:</p>
<pre><code>IMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);</code></pre>
<p>Change it with this block of code:</p>
<pre><code>imu.getAcceleration(&amp;ax, &amp;ay, &amp;az);       
buffer[ix + 0] = ax;
buffer[ix + 1] = ay;
buffer[ix + 2] = az;</code></pre>
<p>You should change the order of the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (ms2)”, followed by the test regarding the maximum acceptance range (that here is in ms2, but on Arduino, was in Gs):</p>
<pre><code>buffer[ix + 0] *= CONVERT_G_TO_MS2;
buffer[ix + 1] *= CONVERT_G_TO_MS2;
buffer[ix + 2] *= CONVERT_G_TO_MS2;

for (int i = 0; i &lt; 3; i++) {
     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {
        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;
     }
}</code></pre>
<p>And that is it! You can now upload the code to your device and proceed with the inferences. The complete code is available on the <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU">project’s GitHub</a>.</p>
<p>Now you should try your movements, seeing the result of the inference of each class on the images:</p>
<p><img src="./images/png/motion_class_ad/idle-inference.png" class="img-fluid"></p>
<p><img src="./images/png/motion_class_ad/terrestrial-inference.png" class="img-fluid"></p>
<p><img src="./images/png/motion_class_ad/lift-inference.png" class="img-fluid"></p>
<p><img src="./images/png/motion_class_ad/maritime-inference.png" class="img-fluid"></p>
<p>And of course some “anomaly”, for example, puting the XIAO upside-down. The anomaly score will be over 1:</p>
<p><img src="./images/png/motion_class_ad/anomaly-inference.png" class="img-fluid"></p>
</section>
<section id="conclusion." class="level2" data-number="7.14">
<h2 data-number="7.14" class="anchored" data-anchor-id="conclusion."><span class="header-section-number">7.14</span> Conclusion.</h2>
<p>Regarding the IMU, this project used the low-cost MPU6050 but could also use other IMUs, for example, the LCM20600 (6-axis), which is part of the <a href="https://wiki.seeedstudio.com/Grove-IMU_9DOF-lcm20600+AK09918/">Seeed Grove - IMU 9DOF (lcm20600+AK09918)</a>. You can took advantage of this senso, witch has integrated a Grove connector, which can be helpful in the case you use the <a href="https://wiki.seeedstudio.com/Seeeduino-XIAO-Expansion-Board/">XIAO with an extension board</a>, as shown below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hackster.imgix.net/uploads/attachments/1591025/grove-icm2060-small_plZuu0oQ5W.jpg?auto=compress%2Cformat&amp;w=740&amp;h=555&amp;fit=max" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Grove-ICM2060-small.jpg</figcaption>
</figure>
</div>
<p>You can follow the instructions <a href="https://wiki.seeedstudio.com/Grove-IMU_9DOF-lcm20600+AK09918/#specification">here</a> to connect the IMU with the MCU. Only note that for using the Grove ICM20600 Accelerometer, it is essential to update the files <strong>I2Cdev.cpp</strong> and <strong>I2Cdev.h</strong> that you will download from the <a href="https://github.com/Seeed-Studio/Seeed_ICM20600_AK09918">library provided by Seeed Studio</a>. For that, replace both files from this <a href="https://github.com/jrowberg/i2cdevlib/tree/master/Arduino/I2Cdev">link</a>. You can find on the GitHub project a sketch for testing the IMU: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU/accelerometer_test">accelerometer_test.ino</a>.</p>
<blockquote class="blockquote">
<p>On the projet’s GitHub repository, you will find the last version of all codes and other docs: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/IMU">XIAO-ESP32S3 - IMU</a>.</p>
</blockquote>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../content/dsp_spectral_features_block/dsp_spectral_features_block.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">DSP - Spectral Features</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Edited by Prof.&nbsp;Marcelo Rovai (UNIFEI University)</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>