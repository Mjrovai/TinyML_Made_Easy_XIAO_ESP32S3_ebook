[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TinyML Made Easy",
    "section": "",
    "text": "Preface\nFinding the right info used to be the big issue for people involved in technical projects. Nowadays, there is a deluge of information, but it is not easy to determine what can and what cannot be trusted. A good pointer is to look for the credentials and the affiliation of the author of a document or that of the associated institutions. Since 1992 EsLaRed has been providing training in different aspects of Information Technologies in Latin America and the Caribbean, always striving to provide accurate and timely training materials, which have evolved accordingly to shifts in the technology focus. IoT and Machine Learning are poised to play a pivotal role, so the work of Professor Marcelo Rovai addressing Tiny Machine Learning is both timely and authoritative, in this rapidly changing field.\nTinyML Made Easy series is exactly what the title means. Written in a clear and concise style, with emphasis on practical applications and examples, drawing from many years of experience and overwhelming enthusiasm in sharing his knowledge, there is no doubt that Marcelo’s work is a very significant contribution to this very interesting field. The knowledge acquired from the exercises will enable the readers to undertake other projects that might interest them.\nErmanno Pietrosemoli, President Fundación Escuela Latinoamericana de Redes\nNovember 2023."
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "I extend my deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Ermanno Petrosemoli, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML) and Edge AI.\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has been a beacon and a cornerstone of our efforts from the beginning.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\n\nGoogle Nano Banana and OpenAI’s GPT were used to generate some of the images in the book. Claude Sonnet and Perplexity helped with code and text reviews."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Microcontrollers (MCUs) are not just cheap electronic components, but they are the backbone of our modern world. With just a few kilobytes of RAM, they are designed to consume small amounts of power. Today, MCUs can be found embedded in all residential, medical, automotive, and industrial devices. Over 40 billion microcontrollers are estimated to be marketed annually, and hundreds of billions are currently in service. Yet, these devices often go unnoticed, as they are used to replace functionalities in older electromechanical systems in cars, washing machines, or remote controls. Understanding the potential of these microcontrollers is key to unlocking the future of technology.\nMore recently, with the era of IoT (Internet of Things), a significant part of these MCUs is generating “quintillions” of data, which, in their majority, are not used due to the high cost and complexity of their data transmission (bandwidth and latency).\nOn the other hand, in the last decades, we have witnessed the development of Machine Learning models (sub-area of Artificial Intelligence) trained with “tons” of data and powerful mainframes. But now, it suddenly becomes possible for “noisy” and complex signals, such as images, audio, or accelerometers, to extract meaning from the same through neural networks. More importantly, we can execute these models of neural networks in microcontrollers and sensors using very little energy and extract much more meaning from the data generated by these sensors, which we are currently ignoring.\n\nTinyML, a new area of applied AI, allows for extracting “machine intelligence” from the physical world (where the data is generated).\n\nTinyML Made Ease is a foundational text designed to facilitate the understanding and application of Embedded Machine Learning, or TinyML. In an era where embedded devices boast ultra-low power consumption—measured in mere milliwatts—and machine learning frameworks such as TensorFlow Lite for Microcontrollers (TF Lite Micro) become increasingly tailored for embedded applications, the intersection of AI and IoT is rapidly expanding. This book demystifies the integration of AI capabilities into these devices, making it accessible and empowering for all, paving the way for a wide-reaching adoption of AI-enabled IoT, or “AioT.”"
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "This book is part of the open book Machine Learning Systems, which we invite you to read.\nTinyML Made Easy, an open-access eBook and part of the open book Machine Learning Systems, is an accessible resource for enthusiasts, professionals, and engineering students embarking on the transformative path of embedded machine learning (TinyML). This book offers a systematic introduction to integrating ML algorithms with microcontrollers, focusing on practicality and hands-on experiences.\nStudents are introduced to core concepts of TinyML through the setup and programming of the Seeed Studio XIAO ESP32-S3, a state-of-the-art microcontroller designed for ML applications. The book breaks down complex ideas into understandable segments, ensuring foundational knowledge is built from the ground up.\nTrue to the engineering ethos of ‘learning by doing,’ each chapter focuses on a project that challenges students to apply their knowledge in real-world scenarios. The projects are designed to reinforce learning and stimulate innovation, covering:\n\nMicrocontroller setup and sensor tests,\nImage classification,\nDataset labeling and Object detection,\nAudio processing and keyword spotting,\nTime-series data and Digital Signal Processing,\nMotion classification and Anomaly detection.\n\nTinyML Made Easy provides detailed walkthroughs of industry-standard tools such as Arduino IDE, Seeed SenseCraft, and Edge Impulse Studio. Students will gain hands-on experience with data collection, pre-processing, model training, and deployment, equipping them with the technical skills sought in the embedded systems industry.\nAs TinyML is poised to be a driving force in the evolution of IoT and smart devices, students will emerge from this book with an introductory theoretical understanding and the practical skills necessary to contribute to and shape the future of technology.\nWith its straightforward language, clarity, and focus on experiential learning, TinyML Made Easy is more than just a book—it’s a comprehensive toolkit for anyone ready to delve into the world of embedded machine learning. It empowers them to move beyond the classroom and into the lab or field, and they are well-prepared to tackle the challenges and opportunities TinyML presents.\n\nSupplementary Online Resources\nChapter 1 - Setup XIAO ESP32S3 Sense\n\nArduino Codes\n\nChapter 2 - Image Classification\n\nArduino Codes\nDataset\nEdge Impulse Project\n\nChapter 3 - Object Detection\n\nEdge Impulse Project\n\nChapter 4 - Audio Feature Engineering\n\nAudio_Data_Analysis Colab Notebook\n\nChapter 5 - Keyword Spotting (KWS)\n\nArduino Codes\nSubset of Google Speech Commands Dataset\nKWS_MFCC_Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino post-processing code\nEdge Impulse Project\n\nChapter 6 - DSP Spectral Features\n\nDSP - Spectral Features Colab Notebook\n\nChapter 7 - Motion Classification and Anomaly Detection\n\nArduino Codes\nEdge_Impulse_Spectral_Features_Block Colab Notebook\nEdge Impulse Project"
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html",
    "href": "content/xiaoml_kit/setup/setup.html",
    "title": "Setup",
    "section": "",
    "text": "Appendix"
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#overview",
    "href": "content/xiaoml_kit/setup/setup.html#overview",
    "title": "Setup",
    "section": "Overview",
    "text": "Overview\nThe XIAOML Kit is designed to provides hands-on experience with TinyML applications. The kit includes the powerful XIAO ESP32S3 Sense development board and an expansion board that adds essential sensors for machine learning projects.\nComplete XIAOML Kit Components:\n\nXIAO ESP32S3 Sense: Main development board with integrated camera sensor, digital microphone, and SD card support\nExpansion Board: Features a 6-axis IMU (LSM6DS3TR-C) and 0.42” OLED display for motion sensing and data visualization\nSD Card Toolkit: Includes SD card and USB adapter for data storage and model deployment\nUSB-C Cable: For connecting the board to your computer\nAntenna and Heat Sinks\n\n\n⚠️ Attention\nDo not install the heat sinks (or carefully, remove them) on/from the XIAO ESP32S3 if you want to use the XIAO ML Kit Expansion Board. See Appendix for more information.\n\n \n\nXIAO ESP32S3 Sense - Core Board Features\nThe XIAO ESP32S3 Sense serves as the heart of the XIAOML Kit, integrating embedded ML computing power with photography and audio capabilities, making it an ideal platform for TinyML applications in intelligent voice and vision AI.\n \nKey Features\n\nPowerful MCU: ESP32S3 32-bit, dual-core, Xtensa processor operating up to 240 MHz, with Arduino / MicroPython support\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 × 1200 resolution, compatible with OV5640 camera sensor, plus integrated digital microphone\nElaborate Power Design: Lithium battery charge management with four power consumption models, deep sleep mode with power consumption as low as 14 μA\nGreat Memory: 8 MB PSRAM and 8 MB FLASH, supporting SD card slot for external 32 GB FAT memory\nOutstanding RF Performance: 2.4 GHz Wi-Fi and BLE dual wireless communication, supports 100m+ remote communication with U.FL antenna\nCompact Design: 21 × 17.5 mm, adopting the classic XIAO form factor, suitable for space-limited projects\n\n \nBelow is the general board pinout:\n \n\nFor more details, please refer to the Seeed Studio Wiki page\n\n\n\nExpansion Board Features\nThe expansion board extends the XIAOML Kit’s capabilities for motion-based machine learning applications:\n \nComponents:\n\n6-axis IMU (LSM6DS3TR-C):\n\n3-axis accelerometer and 3-axis gyroscope for motion detection and classification\n\nAccelerometer range: ±2/±4/±8/±16 g\nGyroscope range: ±125/±250/±500/±1000/±2000 dps\nI2C interface (address: 0x6A)\n\n\n0.42” OLED Display\n\nMonochrome display (72×40 resolution) for real-time data visualization\n\nController: SSD1306\nI2C interface (address: 0x3C)\n\n\nRestart Button (EN)\nBattery Connector (BAT+, BAT- )\n\n\n\nComplete Kit Assembly\nThe expansion board connects seamlessly to the XIAO ESP32S3 Sense, creating a comprehensive platform for multimodal machine learning experiments covering vision, audio, and motion sensing.\n \nPlease pay attention to the mounting orientation of the module:\n \nNote that\n\nThe EN connection, shown at the bottom of the ESP32S3 Sense, is available on the expansion board via the RST button.\nThe BAT+ and BAT- connections are also available through the BAT3.7V white connector.\n\nXIAOML Kit Applications:\n\nVision: Image classification and object detection using the integrated camera\nAudio: Keyword spotting and voice recognition with the built-in microphone\nMotion: Activity recognition and anomaly detection using the IMU sensors\nMulti-modal: Combined sensor fusion for complex ML applications"
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "href": "content/xiaoml_kit/setup/setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "title": "Setup",
    "section": "Installing the XIAO ESP32S3 Sense on Arduino IDE",
    "text": "Installing the XIAO ESP32S3 Sense on Arduino IDE\n\nConnect the XIAOML Kit to your computer via the USB-C port. \nDownload and Install the stable version of Arduino IDE according to your operating system.\n[Download Arduino IDE]\nOpen the Arduino IDE and select the Boards Manager (represented by the UNO Icon).\nEnter “ESP32”, and select”esp32 by Espressif Systems.” You can install or update the board support packages.\n\n\nDo not select “Arduino ESP32 Boards by Arduino”, which are the support package for the Arduino Nano ESP32 and not our board.\n\n \n\n⚠️ Attention\nVersions 3.x may experience issues when using the XIAO ESP32S3 Sense with Edge Impulse deploy codes. If this is the case, use the last 2.0.x stable version (for example, 2.0.17) instead.\n\n\nClick Select Board, enter with xiao or esp32s3, and select the XIAO_ESP32S3 in the boards manager and the corresponding PORT where the ESP32S3 is connected.\n\n \nThat is it! The device should be OK. Let’s do some tests."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#testing-the-board-with-blink",
    "href": "content/xiaoml_kit/setup/setup.html#testing-the-board-with-blink",
    "title": "Setup",
    "section": "Testing the board with BLINK",
    "text": "Testing the board with BLINK\nThe XIAO ESP32S3 Sense features a built-in LED connected to GPIO21. So, you can run the blink sketch (which can be found under Files/Examples/Basics/Blink. The sketch uses the LED_BUILTIN Arduino constant, which internally corresponds to the LED connected to pin 21. Alternatively, you can change the Blink sketch accordingly.\n#define LED_BUILT_IN 21 // This line is optional\n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pins work with inverted logic\n// LOW to turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins operate with inverted logic: LOW turns on and HIGH turns off."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#sec-setup-microphone-test-1c2e",
    "href": "content/xiaoml_kit/setup/setup.html#sec-setup-microphone-test-1c2e",
    "title": "Setup",
    "section": "Microphone Test",
    "text": "Microphone Test\nLet’s start with sound detection. Enter with the code below or go to the GitHub project and download the sketch: XIAOML_Kit_Mic_Test and run it on the Arduino IDE:\n/*\n  XIAO ESP32S3 Simple Mic Test\n  (for ESP32 Library version 3.0.x and later)\n*/\n\n#include &lt;ESP_I2S.h&gt;\nI2SClass I2S;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n    }\n\n  // setup 42 PDM clock and 41 PDM data pins\n  I2S.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!I2S.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nOpen the Serial Plotter, and you will see the loudness change curve of the sound.\n \nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, using the onboard SD Card reader, we can save .wav audio files. To do that, we need first to enable the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip (The XIAO has 8 MB of PSRAM). The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\n\nTo turn it on, go to Tools-&gt;PSRAM:\"OPI PSRAM\"-&gt;OPI PSRAM\n\n \n\nXIAO ESP32S3 Sense supports microSD cards up to 32GB. If you are ready to purchase a microSD card for XIAO, please refer to the specifications below. Format the microSD card to FAT32 format before using it.\n\nNow, insert the FAT32 formatted SD card into the XIAO as shown in the photo below\n \n/*\n * WAV Recorder for Seeed XIAO ESP32S3 Sense\n * (for ESP32 Library version 3.0.x and later)\n*/\n\n#include \"ESP_I2S.h\"\n#include \"FS.h\"\n#include \"SD.h\"\n\nvoid setup() {\n  // Create an instance of the I2SClass\n  I2SClass i2s;\n\n  // Create variables to store the audio data\n  uint8_t *wav_buffer;\n  size_t wav_size;\n\n  // Initialize the serial port\n  Serial.begin(115200);\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"Initializing I2S bus...\");\n\n  // Set up the pins used for audio input\n  i2s.setPinsPdmRx(42, 41);\n\n  // start I2S at 16 kHz with 16-bits per sample\n  if (!i2s.begin(I2S_MODE_PDM_RX,\n                 16000,\n                 I2S_DATA_BIT_WIDTH_16BIT,\n                 I2S_SLOT_MODE_MONO)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n\n  Serial.println(\"I2S bus initialized.\");\n  Serial.println(\"Initializing SD card...\");\n\n  // Set up the pins used for SD card access\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1) ;\n  }\n  Serial.println(\"SD card initialized.\");\n  Serial.println(\"Recording 20 seconds of audio data...\");\n\n  // Record 20 seconds of audio data\n  wav_buffer = i2s.recordWAV(20, &wav_size);\n\n  // Create a file on the SD card\n  File file = SD.open(\"/arduinor_rec.wav\", FILE_WRITE);\n  if (!file) {\n    Serial.println(\"Failed to open file for writing!\");\n    return;\n  }\n\n  Serial.println(\"Writing audio data to file...\");\n\n  // Write the audio data to the file\n  if (file.write(wav_buffer, wav_size) != wav_size) {\n    Serial.println(\"Failed to write audio data to file!\");\n    return;\n  }\n\n  // Close the file\n  file.close();\n\n  Serial.println(\"Application complete.\");\n}\n\nvoid loop() {\n  delay(1000);\n  Serial.printf(\".\");\n}\n\nSave the code, for example, as Wav_Record.ino, and run it in the Arduino IDE.\nThis program is executed only once after the user turns on the serial monitor (or when the RESET button is pressed). It records for 20 seconds and saves the recording file to a microSD card as “arduino_rec.wav.”\nWhen the “.” is output every second in the serial monitor, the program execution is complete, and you can play the recorded sound file using a card reader.\n\n \nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this lab, but you can find an excellent description on the wiki page.\n\nTo know more about the File System on the XIAO ESP32S3 Sense, please refer to this link."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#testing-the-camera",
    "href": "content/xiaoml_kit/setup/setup.html#testing-the-camera",
    "title": "Setup",
    "section": "Testing the Camera",
    "text": "Testing the Camera\nFor testing (and using the camera, we can use several methods:\n\nThe SenseCraft AI Studio\nThe CameraWebServer app on Arduino IDE (See the next section)\nCapturing images and saving them on an SD card (similar to what we did with audio)\n\n\nTesting the camera with the SenseCraft AI Studio\nThe easiest way to see the camera working is to use the SenseCraft AI Studio, a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2.\n\nWe can also use the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nLet’s follow the steps below to start the SenseCraft AI:\n\nOpen the SenseCraft AI Vision Workspace in a web browser, such as Chrome, and sign in (or create an account).\n\n \n\nHaving the XIAOML Kit physically connected to the notebook, select it as below:\n\n \n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead. Also, confirm that the Arduino IDE or any other serial device is not connected to the XIAO.\n\nTo see the camera working, we should upload a model. We can try several Computer Vision models previously uploaded by Seeed Studio. Use the button [Select Model] and choose among the available models.\n \nPassing the cursor over the AI models, we can have some information about them, such as name, description, category or task (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and in some cases, metrics (Accuracy or mAP).\n \nWe can choose one of the ready-to-use AI models, such as “Person Classification”, by clicking on it and pressing the [Confirm] button, or upload our own model.\n \nIn the Preview Area, we can see the streaming generated by the camera.\n \n\nWe will return to the SenseCraft AI Studio in more detail during the Vision AI labs."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#testing-wifi",
    "href": "content/xiaoml_kit/setup/setup.html#testing-wifi",
    "title": "Setup",
    "section": "Testing WiFi",
    "text": "Testing WiFi\n\nInstallation of the antenna\nThe XIAOML Kit arrived fully assembled. First, remove the Sense Expansion Board (which contains the Camera, Mic, and SD Card Reader) from the XIAO.\nOn the bottom left of the front of XIAO ESP32S3, there is a separate “WiFi/BT Antenna Connector”. To improve your WiFi/Bluetooth signal, remove the antenna from the package and attach it to the connector.\nThere is a small trick to installing the antenna. If you press down hard on it directly, you will find it very difficult to press and your fingers will hurt! The correct way to install the antenna is to insert one side of the antenna connector into the connector block first, then gently press down on the other side to ensure the antenna is securely installed.\nRemoving the antenna is also the case. Do not use brute force to pull the antenna directly; instead, apply force to one side to lift, making the antenna easy to remove.\n \nReinstalling the expansion board is very simple; you just need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and hear a “click.” The installation is complete.\nOne of the XIAO ESP32S3’s differentiators is its WiFi capability. So, let’s test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nOpen the Arduino IDE and select our board and port. Go to Examples and look for WiFI ==&gt; WiFIScan under the “Examples for the XIAO ESP32S3”. Upload the sketch to the board.\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device’s range on the serial monitor. Here is what I got in the lab:\n \n\n\nSimple WiFi Server (Turning LED ON/OFF)\nLet’s test the device’s capability to behave as a Wi-Fi server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nGo to Examples and look for WiFI ==&gt; SimpleWiFIServer under the “Examples for the XIAO ESP32S3”.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nAnd modify pin 5 to pin 21, where the built-in LED is installed. Also, let’s modify the webpage (lines 85 and 86) to reflect the correct LED Pin and that it is active with LOW:\nclient.print(\"Click &lt;a href=\\\"/H\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 OFF.&lt;br&gt;\");\nclient.print(\"Click &lt;a href=\\\"/L\\\"&gt;here&lt;/a&gt; to turn the LED on pin 21 ON.&lt;br&gt;\");\nYou can monitor your server’s performance using the Serial Monitor.\n \nTake the IP address shown in the Serial Monitor and enter it in your browser. You will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\n \n\n\nUsing the CameraWebServer\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nOn the board_config.h tab, comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\n\nDo not forget to check the Tools to see if PSRAM is enabled.\n\n \nAs done before, in the CameraWebServer.ino tab, enter your wifi credentials and upload the code to the device.\nIf the code is executed correctly, you should see the address on the Serial Monitor:\nWiFi connecting....\nWiFi connected\nCamera Ready! Use 'http://192.168.5.60' to connect\nCopy the address into your browser and wait for the page to load. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds, depending on your connection. Using the [Save] button, you can save an image to your computer’s download area.\n \nThat’s it! You can save the images directly on your computer for use on projects."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#testing-the-imu-sensor-lsm6ds3tr-c",
    "href": "content/xiaoml_kit/setup/setup.html#testing-the-imu-sensor-lsm6ds3tr-c",
    "title": "Setup",
    "section": "Testing the IMU Sensor (LSM6DS3TR-C)",
    "text": "Testing the IMU Sensor (LSM6DS3TR-C)\nAn Inertial Measurement Unit (IMU) is a sensor that measures motion and orientation. The LSM6DS3TR-C on your XIAOML kit is a 6-axis IMU, meaning it combines:\n\n3-axis Accelerometer: Measures linear acceleration (including gravity) along X, Y, and Z axes\n3-axis Gyroscope: Measures angular velocity (rotation rate) around X, Y, and Z axes\n\n\nTechnical Specifications:\n\nCommunication: I2C interface at address 0x6A\nAccelerometer Range: ±2/±4/±8/±16 g (we use ±2g by default)\nGyroscope Range: ±125/±250/±500/±1000/±2000 dps (we use ±250 dps by default)\nResolution: 16-bit ADC\nPower Consumption: Ultra-low power design\n\n\n\nCoordinate System:\nThe sensor follows a right-hand coordinate system. When looking at the IMU sensor with the point mark visible (Expansion Board bottom view):\n \n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n \n\n\nRequired Libraries\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select Manage Libraries (represented by the Books Icon).\nFor the IMU library, enter “LSM6DS3”, and select”Seeed Arduino LSM6DS3 by Seeed”. You can INSTALL or UPDATE the board support packages.\n\n \n⚠️ Important: Do NOT install “Arduino_LSM6DS3 by Arduino” - that’s for different boards!\n\n\nTest Code\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\n// LSM6DS3TR-C sensor is located at I2C address 0x6A\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Variables to store sensor readings\nfloat accelX, accelY, accelZ;  // Accelerometer values (g-force)\nfloat gyroX, gyroY, gyroZ;     // Gyroscope values (degrees per second)\n\nvoid setup() {\n  // Initialize serial communication at 115200 baud rate\n  Serial.begin(115200);\n\n  // Wait for serial port to connect (useful for debugging)\n  while (!Serial) {\n    delay(10);\n  }\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU Sensor\");\n  Serial.println(\"=============================\");\n\n  // Initialize the IMU sensor\n  if (myIMU.begin() != 0) {\n    Serial.println(\"ERROR: IMU initialization failed!\");\n    Serial.println(\"Check connections and I2C address\");\n    while(1) {\n      delay(1000); // Halt execution if IMU fails to initialize\n    }\n  } else {\n    Serial.println(\"✓ IMU initialized successfully\");\n    Serial.println();\n\n    // Print sensor information\n    Serial.println(\"Sensor Information:\");\n    Serial.println(\"- Accelerometer range: ±2g\");\n    Serial.println(\"- Gyroscope range: ±250 dps\");\n    Serial.println(\"- Communication: I2C at address 0x6A\");\n    Serial.println();\n\n    // Print data format explanation\n    Serial.println(\"Data Format:\");\n    Serial.println(\"AccelX,AccelY,AccelZ,GyroX,GyroY,GyroZ\");\n    Serial.println(\"Units: g-force (m/s²), degrees/second\");\n    Serial.println();\n\n    delay(2000); // Brief pause before starting measurements\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force units)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format to Serial Monitor\n  Serial.print(\"Accelerometer (g): \");\n  Serial.print(\"X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n\n  Serial.print(\" | Gyroscope (°/s): \");\n  Serial.print(\"X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.print(gyroZ, 2);\n  Serial.println();\n\n  // Print CSV format for Serial Plotter\n  Serial.println(String(accelX) + \",\" + String(accelY) + \",\" +\n                 String(accelZ) + \",\" + String(gyroX) + \",\" +\n                 String(gyroY) + \",\" + String(gyroZ));\n\n  // Update rate: 10 Hz (100ms delay)\n  delay(100);\n}\nThe Serial monitor will show the values, and the plotter will show their variation over time. For example, by moving the Kit over the y-axis, we will see that value 2 (red line) changes accordingly. Note that z-axis is represented by value 3 (green line), which is near 1.0g. The blue line (value 1) is related to the x-axis.\n \nYou can select the values 4 to 6 to see the Gyroscope behavior."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#testing-the-oled-display-ssd1306",
    "href": "content/xiaoml_kit/setup/setup.html#testing-the-oled-display-ssd1306",
    "title": "Setup",
    "section": "Testing the OLED Display (SSD1306)",
    "text": "Testing the OLED Display (SSD1306)\nOLED (Organic Light-Emitting Diode) displays are self-illuminating screens where each pixel produces its own light. The XIAO ML kit features a compact 0.42-inch monochrome OLED display, ideal for displaying sensor data, status information, and simple graphics.\n\nTechnical Specifications:\n\nSize: 0.42 inches diagonal\nResolution: 72 × 40 pixels\nController: SSD1306\nInterface: I2C at address 0x3C\nColors: Monochrome (black pixels on white background, or vice versa)\nViewing: High contrast, visible in bright light\nPower: Low power consumption, no backlight needed\n\n\n\nDisplay Characteristics:\n\nPixel-perfect: Each of the 2,880 pixels (72×40) can be individually controlled\nFast refresh: Suitable for animations and real-time data\nNo ghosting: Instant pixel response\nWide viewing angle: Clear from multiple viewing positions\n\n\n\nRequired Libraries\nBefore uploading the code, install the required library:\n\nOpen the Arduino IDE and select the “Manage Libraries” (represented by the Books Icon).\nEnter u8g2 and select U8g2 by oliver. You can install or update the board support packages.\nℹ️ Note: U8g2 is a powerful graphics library supporting many display types\n\n \nThe U8g2 library is a monochrome graphics library with these features:\n\nSupport for many display controllers (including SSD1306)\nText rendering with various fonts\nDrawing primitives (lines, rectangles, circles)\nMemory-efficient page-based rendering\nHardware and software I2C support\n\n\n\nTest Code\nEnter with the code below at the Arduino IDE and uploaded it to Kit:\n#include &lt;U8g2lib.h&gt;\n#include &lt;Wire.h&gt;\n\n// Initialize the OLED display\n// SSD1306 controller, 72x40 resolution, I2C interface\nU8G2_SSD1306_72X40_ER_1_HW_I2C u8g2(U8G2_R2, U8X8_PIN_NONE);\n\nvoid setup() {\n  Serial.begin(115200);\n\n  Serial.println(\"XIAOML Kit - Hello World\");\n  Serial.println(\"==========================\");\n\n  // Initialize the display\n  u8g2.begin();\n\n  Serial.println(\"✓ Display initialized\");\n  Serial.println(\"Showing Hello World message...\");\n\n  // Clear the display\n  u8g2.clearDisplay();\n}\n\nvoid loop() {\n  // Start drawing sequence\n  u8g2.firstPage();\n  do {\n    // Set font\n    u8g2.setFont(u8g2_font_ncenB08_tr);\n\n    // Display \"Hello World\" centered\n    u8g2.setCursor(8, 15);\n    u8g2.print(\"Hello\");\n\n    u8g2.setCursor(12, 30);\n    u8g2.print(\"World!\");\n\n    // Add a simple decoration - draw a frame around the text\n    u8g2.drawFrame(2, 2, 68, 36);\n\n  } while (u8g2.nextPage());\n\n  // No delay needed - the display will show continuously\n}\nIf everything works fine, you should see at the display, “Hello World” inside a rectangle.\n \n\n\nOLED - Text Sizes and Positioning\n\nNote that the text is positioned with setCursor(x, y), in this case centered:\nu8g2.setCursor(8, 15);\nThe font used in the code was medium.\nu8g2.setFont(u8g2_font_ncenB08_tr);\nBut other font sizes are available:\n\nu8g2_font_4x6_tr: Tiny font (4×6 pixels)\nu8g2_font_6x10_tr: Small font (6×10 pixels)\nu8g2_font_ncenB08_tr: Medium bold font\nu8g2_font_ncenB14_tr: Large bold font\n\n\n\n\nShapes\nThe code added a simple decoration, drawing a frame around the text\nu8g2.drawFrame(2, 2, 68, 36);\nBut other shapes are available:\n\nRectangle outline: drawFrame(x, y, width, height)\nFilled rectangle: drawBox(x, y, width, height)\nCircle: drawCircle(x, y, radius)\nLine: drawLine(x1, y1, x2, y2)\nIndividual pixels: drawPixel(x, y)\n\n\n\nCoordinates\nThe display uses a coordinate system where:\n\nOrigin (0,0): Top-left corner\nX-axis: Increases from left to right (0 to 71)\nY-axis: Increases from top to bottom (0 to 39)\nText positioning: setCursor(x, y) where y is the baseline of text\n\n\n\nDisplay Rotation\n\nYou can change the rotation parameter by using:\n\nU8G2_R0: Normal orientation\nU8G2_R1: 90° clockwise\nU8G2_R2: 180° (upside down)\nU8G2_R3: 270° clockwise\n\n\n\n\nCustom Characters:\n// Draw custom bitmap\nstatic const unsigned char myBitmap[] = {0x00, 0x3c, 0x42, 0x42, 0x3c, 0x00};\nu8g2.drawBitmap(x, y, 1, 6, myBitmap);\n\n\nText Measurements:\nint width = u8g2.getStrWidth(\"Hello\");  // Get text width\nint height = u8g2.getAscent();         // Get font height\nThe OLED display is now ready to show your sensor data, system status, or any custom graphics you design for your ML projects!"
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#summary",
    "href": "content/xiaoml_kit/setup/setup.html#summary",
    "title": "Setup",
    "section": "Summary",
    "text": "Summary\nThe XIAOML Kit with ESP32S3 Sense represents a powerful, yet accessible entry point into the world of TinyML and embedded machine learning. Through this setup process, we have systematically tested every component of the XIAOML Kit, confirming that all sensors and peripherals are functioning correctly. The ESP32S3’s dual-core processor and 8MB of PSRAM provide sufficient computational power for real-time ML inference, while the OV2640 camera, digital microphone, LSM6DS3TR-C IMU, and 0.42” OLED display create a complete multimodal sensing platform. WiFi connectivity opens possibilities for edge-to-cloud ML workflows, and our Arduino IDE development environment is now properly configured with all necessary libraries.\nBeyond mere functionality tests, we’ve gained practical insights into coordinate systems, data formats, and operational characteristics of each sensor—knowledge that will prove invaluable when designing ML data collection and preprocessing pipelines for the upcoming projects.\nThis setup process demonstrates key principles that extend far beyond this specific kit. Working with the ESP32S3’s memory limitations and processing capabilities provides an authentic experience with the resource constraints inherent in edge AI—the same considerations that apply when deploying models on smartphones, IoT devices, or autonomous systems. Having multiple modalities (vision, audio, motion) on a single platform enables exploration of multimodal ML approaches, which are increasingly important in real-world AI applications.\nMost importantly, from raw sensor data to model inference to user feedback via the OLED display, the kit provides a complete ML deployment cycle in miniature, mirroring the challenges faced in production AI systems.\nWith this foundation in place, you’re now equipped to tackle the core TinyML applications in the following chapters:\n\nVision Projects: Leveraging the camera for image classification and object detection\nAudio Projects: Processing audio streams for keyword spotting and voice recognition\nMotion Projects: Using IMU data for activity recognition and anomaly detection\n\nEach application will build upon the hardware understanding and software infrastructure we’ve established, demonstrating how artificial intelligence can be deployed not just in data centers, but in resource-constrained devices that directly interact with the physical world.\nThe principles encountered with this kit—real-time processing, sensor fusion, and edge inference—are the same ones driving the future of AI deployment in autonomous vehicles, smart cities, medical devices, and industrial automation. By completing this setup successfully, you’re now prepared to explore this exciting frontier of embedded machine learning."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#resources",
    "href": "content/xiaoml_kit/setup/setup.html#resources",
    "title": "Setup",
    "section": "Resources",
    "text": "Resources\n\nXIAOML Kit Code\nXIAO ESP32S3 Sense manual & example code\nUsage of Seeed Studio XIAO ESP32S3 microphone\nFile System and XIAO ESP32S3 Sense\nCamera Usage in Seeed Studio XIAO ESP32S3 Sense"
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#heat-sink-considerations",
    "href": "content/xiaoml_kit/setup/setup.html#heat-sink-considerations",
    "title": "Setup",
    "section": "Heat Sink Considerations",
    "text": "Heat Sink Considerations\nIf you need to use the XIAO ESP32S3 Sense for camera applications WITHOUT the Expansion Board, you may install the heat sink.\nNote that having the heat sink installed, it is not possible to connect the XIAO ESP32S3 Sense with the Expansion Board."
  },
  {
    "objectID": "content/xiaoml_kit/setup/setup.html#installing-the-heat-sink",
    "href": "content/xiaoml_kit/setup/setup.html#installing-the-heat-sink",
    "title": "Setup",
    "section": "Installing the Heat Sink",
    "text": "Installing the Heat Sink\nTo ensure optimal cooling for your XIAO ESP32S3 Sense, you should install the provided heat sink during camera applications. Its design is specifically tailored to address cooling needs, particularly during intensive operations such as camera usage.\n\nTwo heat sinks are included in the kit, but you can use only one to guarantee access to the Battery pins.\n\nInstallation:\n\nEnsure your device is powered off and unplugged from any power source before you start.\nPrioritize covering the Thermal PAD with the heat sink, as it is directly above the ESP32S3 chip, the primary source of heat. Proper alignment ensures optimal heat dissipation, and it is essential to keep the BAT pins as unobstructed as possible.\n\nNow, let’s begin the installation process:\nStep 1. Prepare the Heat Sink: Start by removing the protective cover from the heat sink to expose the thermal adhesive. This will prepare the heat sink for a secure attachment to the ESP32S3 chip.\nStep 2. Assemble the Heat Sink:\n \n\nAfter installation, ensure everything is properly secured with no risk of short circuits. Verify that the heat sink is properly aligned and securely attached.\n\nIf one heat synk is not enough, a second one can be installed, sharing both the thermal pad, but in this situation, be aware that all pins became unavailable.\n\n⚠️ Attention\nRemove carefully the heat sinks before using the IMU expansion board again"
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#overview",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\nWe are increasingly facing an artificial intelligence (AI) revolution, where, as Gartner states, Edge AI and Computer Vision have a very high impact potential, and it is for now!\nWhen we look into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML’s Hello World that is both simple and profound!\nThe Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around theXIAO ESP32-S3 Sense, featuring an integrated OV3660 camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.\nIn this Lab, we will explore Image Classification using the non-code tool SenseCraft AI and explore a more detailed development with Edge Impulse Studio and Arduino IDE.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nDeploy Pre-trained Models using SenseCraft AI Studio for immediate computer vision applications\nCollect and Manage Image Datasets for custom classification tasks with proper data organization\nTrain Custom Image Classification Models using transfer learning with MobileNet V2 architecture\nOptimize Models for Edge Deployment through quantization and memory-efficient preprocessing\nImplement Post-processing Pipelines, including GPIO control and real-time inference integration\nCompare Development Approaches between no-code and advanced ML platforms for embedded applications"
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#image-classification",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#image-classification",
    "title": "Image Classification",
    "section": "Image Classification",
    "text": "Image Classification\nImage classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let’s start exploring the Person Classification model (“Person - No Person”), a ready-to-use computer vision application on the SenseCraft AI.\n \n\nImage Classification on the SenseCraft AI Workspace\nStart by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the SenseCraft AI Workspace to connect it.\n \nOnce connected, select the option [Select Model...] and enter in the search window: “Person Classification”. From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).\n \nClick on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.\n\nNote that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.\n\nAfter the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (Person or Not a Person) in the Preview area, along with the inference details displayed in the Device Logger.\n\nNote that we can also select our Inference Frame Interval, from “Real-Time” (Default) to 10 seconds, and the Mode (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).\n\n \nAt the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about 1.7 Frames per second (FPS).\n\nTo run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a power consumption of 830mW.\n\n\n\nPost-Processing\nAn essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.\n \nWith the SebseCraft AI, we can do it on the Output -&gt; GPIO section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal LED should be ON. In a real scenario, a GPIO, for example, D0, D1, D2, D11, or D12, would be used to trigger a relay to turn on a light.\n \nOnce confirmed, the created Trigger Action will be shown. Press Send to upload the command to the XIAO.\n \nNow, pointing the XIAO at a person will make the internal LED go ON.\n \n\nWe will explore more trigger actions and post-processing techniques further in this lab."
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#an-image-classification-project",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#an-image-classification-project",
    "title": "Image Classification",
    "section": "An Image Classification Project",
    "text": "An Image Classification Project\nLet’s create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.\n \nOn SenseCraft AI Studio: Let’s open the tab Training:\n \nThe default is to train a Classification model with a WebCam if it is available. Let’s select the XIAOESP32S3 Sense instead. Pressing the green button [Connect] will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button [Connect].\n \nThe image streamed from the Grove Vision AI V2 will be displayed.\n\nThe Goal\nThe first step, as we can see in the ML pipeline, is to define a goal. Let’s imagine that we have an industrial installation that should automatically sort wheels and boxes.\n \nSo, let’s simulate it, classifying, for example, a toy box and a toy wheel. We should also include a 3rd class of images, background, where there are no objects in the scene.\n \n\n\nData Collection\nLet’s create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n \nSelect one of the classes and keep pressing the green button (Hold to Record) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.\n \nAfter collecting the images, review them and delete any incorrect ones.\n \nCollect around 50 images from each class and go to Training Step.\n\nNote that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.\n\n\n\nTraining\nConfirm if the correct device is selected (XIAO ESP32S3 Sense) and press [Start Training]\n \n\n\nTest\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n \nNow is the time to really deploy the model in the device.\n\n\nDeployment\nSelect the trained model and XIAO ESP32S3 Sense at the Supported Devices window. And press [Deploy to device].\n \nThe SeneCrafit AI will redirect us to the Vision Workplace tab. Confirm the deployment, select the Port, and Connect it.\n \nThe model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a latency of approximately 426 ms, plus a pre-processing of around 110ms, corresponding to a frame rate of 1.8 frames per second (FPS).\nAlso, note that in Settings, it is possible to adjust the model’s confidence.\n \n\nTo run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a power consumption of 730mW.\n\nAs before, in the Output –&gt; GPIO, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.\n \n\n\nSaving the Model\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the Training tab and select the button [Save to SenseCraft]:\n \nFollow the instructions to enter the model’s name, description, image, and other details.\n \nNote that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using Netron. Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.\n \nAlso, the model can be deployed again to the device at any time. Automatically, the Workspace will be open on the SenseCraft AI."
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#image-classification-project-from-a-dataset",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#image-classification-project-from-a-dataset",
    "title": "Image Classification",
    "section": "Image Classification Project from a Dataset",
    "text": "Image Classification Project from a Dataset\nThe primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut as we alheady know, first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.\n\nAlternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. Kaggle fruit-and-vegetable-image-recognition is a good start.\n\nLet’s download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select Export Data.\n \nThe dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.\n \n\nOptionally, you can add some fresh images, using, for example, the code discussed in the setup lab."
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "Image Classification",
    "section": "Training the model with Edge Impulse Studio",
    "text": "Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\nData Acquisition\nNext, go to the Data acquisition section and there, select + Add data. A pop-up window will appear. Select UPLOAD DATA.\n \nAfter selection, a new Pop-Up window will appear, asking to update the data.\n\nIn Upload mode: select a folder and press [Choose Files].\nGo to the folder that contains one of the classes and press [Upload]\n\n \n\nYou will return automatically to the Upload data window.\nSelect Automatically split between training and testing\nAnd enter the label of the images that are in the folder.\nSelect [Upload data]\nAt this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select [no]\n\n \nRepeat the procedure for all classes. Do not forget to change the label’s name. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.\nClose the Upload Data window and return to the Data acquisition page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.\n \n\n\nImpulse Design\n\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach” or “model” each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as “Transfer Learning” (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n \nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them \\((96\\times 96)\\) Pixels are fed to our Transfer Learning block. Let’s create an Inpulse.\n\nAt this point, we can also define our target device to monitor our “budget” (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let’s consider the Espressif ESP-EYE, which is similar but slower.\n\n \nSave the Impulse, as shown above, and go to the Image section.\n\n\nPre-processing (Feature Generation)\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let’s select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\nModel Design, Training, and Test\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, α (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\nWe will use the MobileNet V2 0.35 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height,\n                                             new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nNow, let’s us define the hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nAnd, so, we have as a training result:\n \nThe model profile predicts 233 KB of RAM and 546 KB of Flash, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a latency of around 1160 ms, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.\n\nWith the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other."
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#model-deployment",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#model-deployment",
    "title": "Image Classification",
    "section": "Model Deployment",
    "text": "Model Deployment\nWe can deploy the trained model:\n\nAs .TFLITE to be used on the SenseCraft AI \nAs an Arduino Library in the Edge Impulse Studio.\n\nLet’s start with the SenseCraft, which is more straightforward and more intuitive.\n\nModel Deployment on the SenseCraft AI\nOn the Dashboard, it is possible to download the trained model in several different formats. Let’s download TensorFlow Lite (int8 quantized), which has a size of 623KB.\n \nOn SenseCraft AI Studio, go to the Workspace tab, select XIAO ESP32S3, the corresponding Port, and connect the device.\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will prompt you to enter the model name, the model file, and the class names (objects). We should use labels in alphabetical order: 0: background, 1: box, and 2: wheel, and then press [Send].\n \nAfter a few seconds, the model will be uploaded (“flashed”) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, corresponding to a frame rate of 3.4 frames per second (FPS), what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).\n\nThe total latency is around 4 times faster than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.\n\n \n\nPost-Processing\nIt is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the XIAO ESP32S3 Sense as an AI sensor. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.\n\nThe idea is similar to what we have done on the Seeed Grove Vision AI V2 Image Classification Post-Processing Lab.\n\nBelow is an example of a connection using the I2C bus.\n\n\n\nAs a Sensor | Seeed Studio Wiki\n\n\nPlease refer to the Seeed Studio Wiki for more information.\n\n\n\nModel Deployment as an Arduino Library at EI Studio\nOn the Deploy section at Edge Impulse Studio, Select Arduino library, TensorFlow Lite, Quantized(int8), and press [Build]. The trained model will be downloaded as a .zip Arduino library:\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add .ZIP Library. Next, select the file downloaded from Edge Impulse Studio and press [Open].\n \nGo to the Arduino IDE Examples and look for the project by its name (in this case: “Box_versus_Whell_…Interfering”. Open esp32 -&gt; esp32_camera. The sketch esp32_camera.ino will be downloaded to the IDE.\nThis sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let’s download the modified one from the project GitHub: Image_class_XIAOML-Kit.ino.\n\nXIAO ESP32S3 Image Classification Code Explained\nThe code captures images from the onboard camera, processes them, and classifies them (in this case, “Box”, “Wheel”, or “Background”) using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.\nIn short,:\nCamera → JPEG Image → RGB888 Conversion → Resize to 96x96 → Neural Network → Classification Results → Serial Output\n\nKey Components\n\nLibrary Includes and Dependencies\n\n#include &lt;Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h&gt;\n#include \"edge-impulse-sdk/dsp/image/image.hpp\"\n#include \"esp_camera.h\"\n\nEdge Impulse Inference Library: Contains our trained model and inference engine\nImage Processing: Provides functions for image manipulation\nESP Camera: Hardware interface for the camera module\n\n\nCamera Pin Configurations\n\nThe XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:\n// Configuration 1: Most common OV2640 configuration\n#define CONFIG_1_XCLK_GPIO_NUM    10\n#define CONFIG_1_SIOD_GPIO_NUM    40\n#define CONFIG_1_SIOC_GPIO_NUM    39\n// ... more pins\nThis flexibility allows the code to automatically try different pin mappings if the first one doesn’t work, making it more robust across different hardware revisions.\n\nMemory Management Settings\n\n#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   320\n#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   240\n#define EI_CLASSIFIER_ALLOCATION_HEAP      1\n\nFrame Buffer Size: Defines the raw image size (320x240 pixels)\nHeap Allocation: Uses dynamic memory allocation for flexibility\nPSRAM Support: The ESP32S3 has 8MB of PSRAM for storing large data like images\n\n\n\nsetup() - Initialization\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial);\n\n    if (ei_camera_init() == false) {\n        ei_printf(\"Failed to initialize Camera!\\r\\n\");\n    } else {\n        ei_printf(\"Camera initialized\\r\\n\");\n    }\n\n    ei_sleep(2000);  // Wait 2 seconds before starting\n}\nThis function:\n\nInitializes serial communication for debugging output\nInitializes the camera with automatic configuration detection\nWaits 2 seconds before starting continuous inference\n\n\n\nloop() - Main Processing Loop\nThe loop performs these steps continuously:\nStep 1: Memory Allocation\nsnapshot_buf = (uint8_t*)ps_malloc(EI_CAMERA_RAW_FRAME_BUFFER_COLS *\n                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS *\n                                   EI_CAMERA_FRAME_BYTE_SIZE);\nAllocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.\nStep 2: Image Capture\nif (ei_camera_capture((size_t)EI_CLASSIFIER_INPUT_WIDTH,\n                     (size_t)EI_CLASSIFIER_INPUT_HEIGHT,\n                     snapshot_buf) == false) {\n    ei_printf(\"Failed to capture image\\r\\n\");\n    free(snapshot_buf);\n    return;\n}\nCaptures an image from the camera and stores it in the buffer.\nStep 3: Run Inference\nei_impulse_result_t result = { 0 };\nEI_IMPULSE_ERROR err = run_classifier(&signal, &result, false);\nRuns the machine learning model on the captured image.\nStep 4: Output Results\nfor (uint16_t i = 0; i &lt; EI_CLASSIFIER_LABEL_COUNT; i++) {\n    ei_printf(\"  %s: %.5f\\r\\n\",\n              ei_classifier_inferencing_categories[i],\n              result.classification[i].value);\n}\nPrints the classification results showing confidence scores for each category.\n\n\nei_camera_init() - Smart Camera Initialization\nThis function implements an intelligent initialization sequence:\nbool ei_camera_init(void) {\n    // Try Configuration 1 (OV2640 common)\n    update_camera_config(1);\n    esp_err_t err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Try Configuration 2 (OV3660)\n    esp_camera_deinit();\n    update_camera_config(2);\n    err = esp_camera_init(&camera_config);\n    if (err == ESP_OK) goto camera_init_success;\n\n    // Continue trying other configurations...\n}\nThe function:\n\nTries multiple pin configurations\nTests different clock frequencies (10MHz or 16MHz)\nAttempts PSRAM first, then falls back to DRAM\nApplies sensor-specific settings based on detected hardware\n\n\n\nei_camera_capture() - Image Processing Pipeline\nbool ei_camera_capture(uint32_t img_width, uint32_t img_height, uint8_t *out_buf) {\n    // 1. Get frame from camera\n    camera_fb_t *fb = esp_camera_fb_get();\n\n    // 2. Convert JPEG to RGB888 format\n    bool converted = fmt2rgb888(fb-&gt;buf, fb-&gt;len, PIXFORMAT_JPEG, snapshot_buf);\n\n    // 3. Return frame buffer to camera driver\n    esp_camera_fb_return(fb);\n\n    // 4. Resize if needed\n    if (do_resize) {\n        ei::image::processing::crop_and_interpolate_rgb888(...);\n    }\n}\nThis function:\n\nCaptures a JPEG image from the camera\nConverts it to RGB888 format (required by the ML model)\nResizes the image to match the model’s input size (96x96 pixels)\n\n\n\n\n\nInference\n\nUpload the code to the XIAO ESP32S3 Sense.\n\n\n⚠️ Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools–&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Do not update it)\n\n\n \n\nOpen the Serial Monitor\nPoint the camera at the objects, and check the result on the Serial Monitor.\n\n \n\n\nPost-Processing\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn’t depend on external monitors or connections.\nThe XIAOML Kit tiny 0.42” OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let’s modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: XIAOML-Kit-Img_Class_OLED_Gen.\nRunning the code, we can see the result:"
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#summary",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#summary",
    "title": "Image Classification",
    "section": "Summary",
    "text": "Summary\nThe XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we’ve explored two distinct development approaches that cater to different skill levels and project requirements.\n\nThe SenseCraft AI Studio provides an accessible entry point with its no-code interface, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.\nFor more advanced applications, Edge Impulse Studio offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.\n\nKey insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.\nThe Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine—it becomes a complete AI sensor platform.\nThis foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems."
  },
  {
    "objectID": "content/xiaoml_kit/image_classification/image_classification.html#resources",
    "href": "content/xiaoml_kit/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\n\nGetting Started with the XIAO ESP32S3\nSenseCraft AI Studio Home\nSenseCraft Vision Workspace\nDataset example\nEdge Impulse Project\nXIAO as an AI Sensor\nSeeed Arduino SSCMA Library\nXIAOML Kit Code"
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-overview-905e",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-overview-905e",
    "title": "Object Detection",
    "section": "Overview",
    "text": "Overview\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.\n\nObject Detection versus Image Classification\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:\n \nBut what happens if there is no dominant category in the image?\n \nAn image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n \nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\nAn Innovative Solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-2666",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-object-detection-project-goal-2666",
    "title": "Object Detection",
    "section": "The Object Detection Project Goal",
    "text": "The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n \nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n \nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-data-collection-7201",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-data-collection-7201",
    "title": "Object Detection",
    "section": "Data Collection",
    "text": "Data Collection\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\nCollecting Dataset with the XIAO ESP32S3\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n \nIf the code is executed correctly, you should see the address on the Serial Monitor:\n \nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n \nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of \\(320\\times 240\\) and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-513d",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-edge-impulse-studio-513d",
    "title": "Object Detection",
    "section": "Edge Impulse Studio",
    "text": "Edge Impulse Studio\n\nSetup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n \n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n \n\n\nUploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n \n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n \nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\nLabeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n \nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n \nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n \nYou will be guided to replace the wrong label and correct the dataset.\n \n\n\nBalancing the dataset and split Train/Test\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-impulse-design-5734",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-impulse-design-5734",
    "title": "Object Detection",
    "section": "The Impulse Design",
    "text": "The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from \\(320\\times 240\\) to \\(96\\times 96\\) and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n \n\nPreprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n \nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual \\(96\\times 96\\times 1\\) images or 9,216 features.\n \nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-model-design-training-test-470c",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-model-design-training-test-470c",
    "title": "Object Detection",
    "section": "Model Design, Training, and Test",
    "text": "Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\n\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of \\(96\\times 96\\), the grid would be \\(12\\times 12\\) \\((96/8=12)\\). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n \nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250 KB of RAM and 80 KB of ROM (Flash), which suits well with our board.\n \nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n \n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n\nTest model with “Live Classification”\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n \nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n \nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-deploying-model-arduino-ide-b594",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-deploying-model-arduino-ide-b594",
    "title": "Object Detection",
    "section": "Deploying the Model (Arduino IDE)",
    "text": "Deploying the Model (Arduino IDE)\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n \nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!\n \nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n \nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1\n#define RESET_GPIO_NUM    -1\n#define XCLK_GPIO_NUM     10\n#define SIOD_GPIO_NUM     40\n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48\n#define Y8_GPIO_NUM       11\n#define Y7_GPIO_NUM       12\n#define Y6_GPIO_NUM       14\n#define Y5_GPIO_NUM       16\n#define Y4_GPIO_NUM       18\n#define Y3_GPIO_NUM       17\n#define Y2_GPIO_NUM       15\n#define VSYNC_GPIO_NUM    38\n#define HREF_GPIO_NUM     47\n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n \nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\n\nBackground\n \n\n\nFruits\n \n\n\nBugs\n \nNote that the model latency is 143 ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite \\(320\\times 320\\) model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps)."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-deploying-model-sensecraftwebtoolkit-4957",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-deploying-model-sensecraftwebtoolkit-4957",
    "title": "Object Detection",
    "section": "Deploying the Model (SenseCraft-Web-Toolkit)",
    "text": "Deploying the Model (SenseCraft-Web-Toolkit)\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n \n\nSelect the device/Port and press [Connect]:\n\n \n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n \n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n \n\nNote that you should use the labels trained on EI Studio and enter them in alphabetic order (in our case, background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n \nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n \nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms,\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit).\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screenshots:"
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-summary-782c",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-summary-782c",
    "title": "Object Detection",
    "section": "Summary",
    "text": "Summary\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices."
  },
  {
    "objectID": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-resources-f6d8",
    "href": "content/xiaoml_kit/object_detection/object_detection.html#sec-object-detection-resources-f6d8",
    "title": "Object Detection",
    "section": "Resources",
    "text": "Resources\n\nEdge Impulse Project"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#overview",
    "href": "content/xiaoml_kit/kws/kws.html#overview",
    "title": "Keyword Spotting (KWS)",
    "section": "Overview",
    "text": "Overview\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif’s ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity makes it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the “sense” part of the device, which has a camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will use the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We’ll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of “YES”), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine “under the hood” on the EI Studio), we’ll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we’ll break down each process stage – from data collection and preparation to model training and deployment – to provide a comprehensive understanding of implementing a KWS system on a microcontroller.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nUnderstand Voice Assistant Architecture including cascaded detection systems and the role of edge-based keyword spotting as the first stage of voice processing pipelines\nMaster Audio Data Collection Techniques using both offline methods (XIAO ESP32S3 microphone with SD card storage) and online methods (smartphone integration with Edge Impulse Studio)\nImplement Digital Signal Processing for Audio including I2S protocol fundamentals, audio sampling at 16kHz/16-bit, and conversion between time-domain audio signals and frequency-domain features using MFCC\nTrain Convolutional Neural Networks for Audio Classification using transfer learning techniques, data augmentation strategies, and model optimization for four-class classification (YES, NO, NOISE, UNKNOWN)\nDeploy Optimized Models on Microcontrollers through quantization (INT8), memory management with PSRAM, and real-time inference optimization for embedded systems\nDevelop Complete Post-Processing Pipelines including confidence thresholding, GPIO control for external devices, OLED display integration, and creating standalone AI sensor systems\nCompare Development Workflows between no-code platforms (Edge Impulse Studio) and traditional embedded programming (Arduino IDE) for TinyML applications"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#the-kws-project",
    "href": "content/xiaoml_kit/kws/kws.html#the-kws-project",
    "title": "Keyword Spotting (KWS)",
    "section": "The KWS Project",
    "text": "The KWS Project\n\nHow does a voice assistant work?\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as “ Hey Google” on the first one and “Alexa” on the second.\n \nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n \nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video shows an example in which I emulate a Google Assistant on a Raspberry Pi (Stage 2), using an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\nIf you want to go deeper into the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\nThe Inference Pipeline\nThe diagram below will give an idea of how the final KWS application should work (during inference):\n \nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOWN (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknown.”\n\n\n\nThe Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#dataset",
    "href": "content/xiaoml_kit/kws/kws.html#dataset",
    "title": "Keyword Spotting (KWS)",
    "section": "Dataset",
    "text": "Dataset\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, the classification differs because it involves, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16 kHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16 kHz/16 bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n \nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50 Hz to 100 Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16 kHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete’s dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\nCapturing (offline) Audio Data with the XIAO ESP32S3 Sense\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n \nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n \n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet’s start understanding how to capture raw data using the microphone. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test:\n\n⚠️ Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools–&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it.\n\n\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet’s dig into the code’s main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(–1, 42, 41, –1, –1): This sets up the I2S pins. The parameters are (–1, 42, 41, –1, –1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to –1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test “whispering” in two different tones.\n \n\n\nSave Recorded Sound Samples\nLet’s use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Psuedostatic RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n \nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n \n\nDownload the sketch Wav_Record_dataset, which you can find on the project’s GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet’s break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n\n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new\n                        basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\"\n                      + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files\n                    at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it’s currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n\n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                    rec_buffer,\n                    record_size,\n                    &sample_size,\n                    portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let’s dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize()\n               - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n         rec_buffer,\n         record_size,\n         &sample_size,\n         portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter\n                a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header,\n             uint32_t wav_size,\n             uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n \nSend the label (for example, yes). The program will wait for another command: rec\n \nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n \nUltimately, we will get the saved files on the SD card.\n \nThe files are ready to be uploaded to Edge Impulse Studio\n\n\nCapturing (offline) Audio Data Apps\nThere are many ways to capture audio data; the simplest one is to use a mobile phone or a PC as a connected device on the Edge Impulse Studio.\n\nThe PC or smartphone should capture audio data with a sampling frequency of 16 kHz and a bit depth of 16 Bits.\n\nAnother alternative is to use dedicated apps. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer."
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#training-model-with-edge-impulse-studio",
    "href": "content/xiaoml_kit/kws/kws.html#training-model-with-edge-impulse-studio",
    "title": "Keyword Spotting (KWS)",
    "section": "Training model with Edge Impulse Studio",
    "text": "Training model with Edge Impulse Studio\n\nUploading the Data\nWhen the raw dataset is defined and collected (Pete’s dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n \nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n \nAnd upload them to the Studio (You can automatically split data in train/test). Repeat to all classes and all raw data.\n \nThe samples will now appear in the Data acquisition section.\n \nAll data on Pete’s dataset have a 1 s length, but the samples recorded in the previous section have 10 s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n \nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n \nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard – Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\nCreating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n \nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500 ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, \\(13\\times 49\\times 1\\)). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n \nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\nPre-Processing (MFCC)\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n \nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240 kHz clock (same as our device), but with a smaller CPU (XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let’s keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n \n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: Audio Raw Data Analysis and digging into the lab KWS Feature Engineering\n\n\n\nModel Design and Training\nWe will use a Convolution Neural Network (CNN) model. The basic architecture consists of two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after flattening four neurons, one for each class:\n \nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n \nIf you want to understand what is happening “under the hood,” you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n \nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking “Under the hood.”"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#testing",
    "href": "content/xiaoml_kit/kws/kws.html#testing",
    "title": "Keyword Spotting (KWS)",
    "section": "Testing",
    "text": "Testing\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n \nInspecting the F1 score, we can see that for YES, we got 0.95, an excellent result once we used this keyword to “trigger” our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n \nPoint your phone to the barcode and select the link.\n \nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#deploy-and-inference",
    "href": "content/xiaoml_kit/kws/kws.html#deploy-and-inference",
    "title": "Keyword Spotting (KWS)",
    "section": "Deploy and Inference",
    "text": "Deploy and Inference\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. Select the Arduino Library option, then choose Quantized (Int8) from the bottom menu and press Build.\n \nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n \nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n \nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n \nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0,\n                 (void*)sampleBuffer,\n                 i2s_bytes_to_read,\n                 &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n \nFinally, on static void microphone_inference_end(void) function, replace line 243:\n \nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\n⚠️ Attention\n\nThe Xiao ESP32S3 MUST have the PSRAM enabled. You can check it on the Arduino IDE upper menu: Tools–&gt; PSRAM:OPI PSRAM\nThe Arduino Library (esp32 by Espressif Systems should be version 2.017. Please do not update it."
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#postprocessing",
    "href": "content/xiaoml_kit/kws/kws.html#postprocessing",
    "title": "Keyword Spotting (KWS)",
    "section": "Postprocessing",
    "text": "Postprocessing\nIn edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn’t depend on external monitors or connections.\nLet’s explore two post-processing approaches. Using the internal XIAO’s LED and the OLED on the XIAOML Kit.\n\nWith LED\nNow that we know the model is working by detecting our keywords, let’s modify the code to see the internal LED go on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification,\n     result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n \nAs shown in the video, the idea is that the LED will turn ON whenever the keyword YES is detected. Similarly, instead of turning on an LED, this could serve as a “trigger” for an external device, as we saw in the introduction.\n\n\nWith OLED Display\nThe XIAOML Kit tiny 0.42” OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.\nSo, let’s modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. Download the code from GitHub: xiaoml-kit_kws_oled.\nRunning the code, we can see the result:"
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#summary",
    "href": "content/xiaoml_kit/kws/kws.html#summary",
    "title": "Keyword Spotting (KWS)",
    "section": "Summary",
    "text": "Summary\nThis lab demonstrated the complete development cycle of a keyword spotting system using the XIAOML Kit, showcasing how modern TinyML platforms make sophisticated audio AI accessible on resource-constrained devices. Through hands-on implementation, we’ve bridged the gap between theoretical machine learning concepts and practical embedded AI deployment.\nTechnical Achievements:\nThe project successfully implemented a complete audio processing pipeline from raw sound capture through real-time inference. Using the XIAO ESP32S3’s integrated digital microphone, we captured audio data at professional quality (16kHz/16-bit) and processed it using Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. The deployed CNN model achieved excellent accuracy in distinguishing between our target keywords (“YES”, “NO”) and background conditions (“NOISE”, “UNKNOWN”), with inference times suitable for real-time applications.\nPlatform Integration:\nEdge Impulse Studio proved invaluable as a comprehensive MLOps platform for embedded systems, handling everything from data collection and labeling through model training, optimization, and deployment. The seamless integration between cloud-based training and edge deployment exemplifies modern TinyML workflows, while the Arduino IDE provided the flexibility needed for custom post-processing implementations.\nReal-World Applications:\nThe techniques learned extend far beyond simple keyword detection. Voice-activated control systems, industrial safety monitoring through sound classification, medical applications for respiratory analysis, and environmental monitoring for wildlife or equipment sounds all leverage similar audio processing approaches. The cascaded detection architecture demonstrated here—using edge-based KWS to trigger more complex cloud processing—is fundamental to modern voice assistant systems.\nEmbedded AI Principles:\nThis project highlighted crucial TinyML considerations, including power management, memory optimization through PSRAM utilization, and the trade-offs between model complexity and inference speed. The successful deployment of a neural network performing real-time audio analysis on a microcontroller demonstrates how AI capabilities, once requiring powerful desktop computers, can now operate on battery-powered devices.\nDevelopment Methodology:\nWe explored multiple development pathways, from data collection strategies (offline SD card storage versus online streaming) to deployment options (Edge Impulse’s automated library generation versus custom Arduino implementation). This flexibility is crucial for adapting to various project requirements and constraints.\nFuture Directions:\nThe foundation established here enables the exploration of more advanced audio AI applications. Multi-keyword recognition, speaker identification, emotion detection from voice, and environmental sound classification all build upon the same core techniques. The integration capabilities demonstrated with OLED displays and GPIO control illustrate how KWS can serve as the intelligent interface for broader IoT systems.\nConsider that Sound Classification encompasses much more than just voice recognition. This project’s techniques apply across numerous domains:\n\nSecurity Applications: Broken glass detection, intrusion monitoring, gunshot detection\nIndustrial IoT: Machinery health monitoring, anomaly detection in manufacturing equipment\nHealthcare: Sleep disorder monitoring, respiratory condition assessment, elderly care systems\nEnvironmental Monitoring: Wildlife tracking, urban noise analysis, smart building acoustic management\nSmart Home Integration: Multi-room voice control, appliance status monitoring through sound signatures\n\nKey Takeaways:\nThe XIAOML Kit proves that professional-grade AI development is achievable with accessible tools and modest budgets. The combination of capable hardware (ESP32S3 with PSRAM and integrated sensors), mature development platforms (Edge Impulse Studio), and comprehensive software libraries creates an environment where complex AI concepts become tangible, working systems.\nThis lab demonstrates that the future of AI isn’t just in massive data centers but in intelligent edge devices that can process, understand, and respond to their environment in real time—opening possibilities for ubiquitous, privacy-preserving, and responsive artificial intelligence systems."
  },
  {
    "objectID": "content/xiaoml_kit/kws/kws.html#resources",
    "href": "content/xiaoml_kit/kws/kws.html#resources",
    "title": "Keyword Spotting (KWS)",
    "section": "Resources",
    "text": "Resources\n\nXIAO ESP32S3 Codes\nXIAOML Kit Code\nSubset of Google Speech Commands Dataset\nKWS Feature Engineering\nKWS MFCC Analysis Colab Notebook\nKWS CNN training Colab Notebook\nXIAO ESP32S3 Post-processing Code\nEdge Impulse Project"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#overview",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#overview",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Overview",
    "text": "Overview\nTransportation is the backbone of global commerce. Millions of containers are transported daily by ships, trucks, and trains to destinations worldwide. Ensuring the safe and efficient transit of these containers is a monumental task that requires leveraging modern technology, and TinyML is undoubtedly a key solution.\nIn this hands-on lab, we will work to solve real-world transportation problems. We will develop a Motion Classification and Anomaly Detection system using the XIAOML Kit, the Arduino IDE, and the Edge Impulse Studio. This project will help us understand how containers experience different forces and motions during various phases of transportation, including terrestrial and maritime transit, vertical movement via forklifts, and periods of stationary storage in warehouses.\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nSetting up the XIAOML Kit\nData Collection and Preprocessing\nBuilding the Motion Classification Model\nImplementing Anomaly Detection\nReal-world Testing and Analysis\n\n\n\nBy the end of this lab, you’ll have a working prototype that can classify different types of motion and detect anomalies during the transportation of containers. This knowledge can serve as a stepping stone to more advanced projects in the burgeoning field of TinyML, particularly those involving vibration."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#installing-the-imu",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#installing-the-imu",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Installing the IMU",
    "text": "Installing the IMU\nThe XIAOML Kit comes with a built-in LSM6DS3TR-C 6-axis IMU sensor on the expansion board, eliminating the need for external sensor connections. This integrated approach offers a clean and reliable platform for motion-based machine learning applications.\nThe LSM6DS3TR-C combines a 3-axis accelerometer and 3-axis gyroscope in a single package, connected via I2C to the XIAO ESP32S3 at address 0x6A that provides:\n\nAccelerometer ranges: ±2/±4/±8/±16 g (we’ll use ±2g by default)\nGyroscope ranges: ±125/±250/±500/±1000/±2000 dps (we’ll use ±250 dps by default)\nResolution: 16-bit ADC\nCommunication: I2C interface at address 0x6A\nPower: Ultra-low power design\n\n \nCoordinate System: The sensor operates within a right-handed coordinate system. When looking at the expansion board from the bottom (where you can see the IMU sensor with the point mark):\n\nX-axis: Points to the right\nY-axis: Points forward (away from you)\nZ-axis: Points upward (out of the board)\n\n\nSetting Up the Hardware\nSince the XIAOML Kit comes pre-assembled with the expansion board, no additional hardware connections are required. The LSM6DS3TR-C IMU is already properly connected via I2C.\nWhat’s Already Connected:\n\nLSM6DS3TR-C IMU → I2C (SDA/SCL) → XIAO ESP32S3\nI2C Address: 0x6A\nPower: 3.3V from XIAO ESP32S3\n\nRequired Library: You should have the library installed during the Setup. If not, install the Seeed Arduino LSM6DS3 library following the steps:\n\nOpen Arduino IDE Library Manager\nSearch for “LSM6DS3”\nInstall “Seeed Arduino LSM6DS3” by Seeed Studio\nImportant: Do NOT install “Arduino_LSM6DS3 by Arduino” - that’s for different boards!\n\n\n\nTesting the IMU Sensor\nLet’s start with a simple test to verify the IMU is working correctly. Upload this code to test the sensor:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// Create IMU object using I2C interface\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\nfloat accelX, accelY, accelZ;\nfloat gyroX, gyroY, gyroZ;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit IMU Test\");\n  Serial.println(\"LSM6DS3TR-C 6-Axis IMU\");\n  Serial.println(\"====================\");\n\n  // Initialize the IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  } else {\n      Serial.println(\"✓ IMU initialized successfully\");\n      Serial.println(\"Data Format: AccelX,AccelY,AccelZ,\"\n                    \"GyroX,GyroY,GyroZ\");\n      Serial.println(\"Units: g-force, degrees/second\");\n      Serial.println();\n  }\n}\n\nvoid loop() {\n  // Read accelerometer data (in g-force)\n  accelX = myIMU.readFloatAccelX();\n  accelY = myIMU.readFloatAccelY();\n  accelZ = myIMU.readFloatAccelZ();\n\n  // Read gyroscope data (in degrees per second)\n  gyroX = myIMU.readFloatGyroX();\n  gyroY = myIMU.readFloatGyroY();\n  gyroZ = myIMU.readFloatGyroZ();\n\n  // Print readable format\n  Serial.print(\"Accel (g): X=\"); Serial.print(accelX, 3);\n  Serial.print(\" Y=\"); Serial.print(accelY, 3);\n  Serial.print(\" Z=\"); Serial.print(accelZ, 3);\n  Serial.print(\" | Gyro (°/s): X=\"); Serial.print(gyroX, 2);\n  Serial.print(\" Y=\"); Serial.print(gyroY, 2);\n  Serial.print(\" Z=\"); Serial.println(gyroZ, 2);\n\n  delay(100); // 10 Hz update rate\n}\nWhen the kit is resting flat on a table, you should see:\n\nZ-axis acceleration around +1.0g (gravity)\nX and Y acceleration near 0.0g\nAll gyroscope values near 0.0°/s\n\nMove the kit around to see the values change accordingly."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#the-tinyml-motion-classification-project",
    "title": "Motion Classification and Anomaly Detection",
    "section": "The TinyML Motion Classification Project",
    "text": "The TinyML Motion Classification Project\nWe will simulate container (or, more accurately, package) transportation through various scenarios to make this tutorial more relatable and practical.\n \nUsing the accelerometer of the XIAOML Kit, we’ll capture motion data by manually simulating the conditions of:\n\nMaritime (pallets on boats) - Movement in all axes with wave-like patterns\nTerrestrial (pallets on trucks/trains) - Primarily horizontal movement\nLift (pallets being moved by forklift) - Primarily vertical movement\nIdle (pallets in storage) - Minimal movement\n\nFrom the above image, we can define for our simulation that primarily horizontal movements (\\(x\\) or \\(y\\) axis) should be associated with the “Terrestrial class.” Vertical movements (\\(z\\)-axis) with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#data-collection",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#data-collection",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection",
    "text": "Data Collection\nFor data collection, we have several options available. In a real-world scenario, we can connect our device directly to one container and store the collected data in a file (e.g., CSV) on an SD card. Data can also be sent remotely to a nearby repository, such as a mobile phone, using Wi-Fi or Bluetooth (as demonstrated in this project: Sensor DataLogger). Once your dataset is collected and stored as a .CSV file, we can upload it to the Studio using the CSV Wizard tool.\n\nThis video shows alternative ways to send data to the Edge Impulse Studio.\n\n\nPreparing the Data Collection Code\nIn this lab, we will connect the Kit directly to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\nFor data collection, we should first connect the Kit to Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions here to install Node.js and Edge Impulse CLI on your computer.\n\nOnce the XIAOML Kit is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n \nWe’ll modify our test code to output data in a format suitable for Edge Impulse:\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\nstatic unsigned long last_interval_ms = 0;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) delay(10);\n\n  Serial.println(\"XIAOML Kit - Motion Data Collection\");\n  Serial.println(\"LSM6DS3TR-C IMU Sensor\");\n\n  // Initialize IMU\n  if (myIMU.begin() != 0) {\n      Serial.println(\"ERROR: IMU initialization failed!\");\n      while(1) delay(1000);\n  }\n\n  delay(2000);\n  Serial.println(\"Starting data collection in 3 seconds...\");\n  delay(3000);\n}\n\nvoid loop() {\n  if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n      last_interval_ms = millis();\n\n      // Read accelerometer data\n      float ax = myIMU.readFloatAccelX();\n      float ay = myIMU.readFloatAccelY();\n      float az = myIMU.readFloatAccelZ();\n\n      // Convert to m/s² (multiply by 9.81)\n      float ax_ms2 = ax * 9.81;\n      float ay_ms2 = ay * 9.81;\n      float az_ms2 = az * 9.81;\n\n      // Output in Edge Impulse format\n      Serial.print(ax_ms2);\n      Serial.print(\"\\t\");\n      Serial.print(ay_ms2);\n      Serial.print(\"\\t\");\n      Serial.println(az_ms2);\n  }\n}\nUpload the code to the Arduino IDE. We should see the accelerometer values (converted to m/s²) at the Serial Monitor:\n \n\nKeep the code running, but turn off the Serial Monitor. The data generated by the Kit will be sent to the Edge Impulse Studio via Serial Connection.\n\n\n\nConnecting to Edge Impulse for Data Collection\nCreate an Edge Impulse Project - Go to Edge Impulse Studio and create a new project - Choose a descriptive name (keep under 63 characters for Arduino library compatibility)\n \nSet up CLI Data Forwarder - Install Edge Impulse CLI on your computer - Confirm that the XIAOML Kit is connected to the computer, the code is running and the Serial Monitor is OFF, otherwise we can get an error. - On the Computer Terminal, run: edge-impulse-data-forwarder --clean - Enter your Edge Impulse credentials - Select your project and configure device settings\n \n\nGo to the Edge Impulse Studio Project. On the Device section is possible to verify if the kit is correctly connected (the dot should be green)."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#data-collection-at-the-studio",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#data-collection-at-the-studio",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Collection at the Studio",
    "text": "Data Collection at the Studio\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer (In this case, our XIAOML Kit). Now imagine your container is on a boat, facing an angry ocean:\n \nOr in a Truck, travelling on a road, or being moved with a forklift, etc.\n\nMovement Simulation\nMaritime Class:\n\nHold the kit and simulate boat movement\nMove in all three axes with wave-like, undulating motions\nInclude gentle rolling and pitching movements\n\nTerrestrial Class:\n\nMove the kit horizontally in straight lines (left to right and vice versa)\nSimulate truck/train vibrations with small horizontal shakes\nOccasional gentle bumps and turns\n\nLift Class:\n\nMove the kit primarily in vertical directions (up and down)\nSimulate forklift operations: up, pause, down\nInclude some short horizontal positioning movements\n\nIdle Class:\n\nPlace the kit on a stable surface\nMinimal to no movement\nCapture environmental vibrations and sensor noise\n\n\n\nData Acquisition\nOn the Data Acquisition section, you should see that your board [xiaoml-kit] is connected. The sensor is available: [sensor with 3 axes (accX, accY, accZ)] with a sampling frequency of [50 Hz]. The Studio suggests a sample length of [10000] ms (10 s). The last thing left is defining the sample label. Let’s start, for example, with[terrestrial].\nPress [Start Sample]and move your kit horizontally (left to right), keeping it in one direction. After 10 seconds, our data will be uploaded to the Studio.\nBelow is one sample (raw data) of 10 seconds of collected data. It is notable that the ondulatory movement predominantly occurs along the Y-axis (left-right). The other axes are almost stationary (the X-axis is centered around zero, and the Z-axis is centered around 9.8 ms² due to gravity).\n \nYou should capture, for example, around 2 minutes (ten to twelve samples of 10 seconds each) for each of the four classes. Using the 3 dots after each sample, select two and move them to the Test set. Alternatively, you can use the Automatic Train/Test Split tool on the Danger Zone of the Dashboard tab. Below, it is possible to see the result datasets:"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#data-pre-processing",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#data-pre-processing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n \nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50 Hz. A 2-second window will capture 300 data points (3 axes \\(\\times\\) 2 seconds \\(\\times\\) 50 samples). We will slide this window every 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, given Nyquist’s theorem, which states that a periodic signal must be sampled at least twice the highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nIn the Studio, this dataset will be the input to a Spectral Analysis block, which is well-suited to analyzing repetitive motion, such as accelerometer data. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\n\n\n\n\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\n \nFor example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will serve as the input tensor for a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP Spectral Features"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#model-design",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#model-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Model Design",
    "text": "Model Design\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#impulse-design",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#impulse-design",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Impulse Design",
    "text": "Impulse Design\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block (Dense model) to classify new data.\nWe also utilize a second model, the K-means, which can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that cannot fit into one of these clusters could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean or being upside down on the floor).\n \n\nImagine our XIAOML Kit rolling or moving upside-down, on a movement complement different from the one trained on.\n\n \nBelow the final Impulse design:"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#generating-features",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#generating-features",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Generating features",
    "text": "Generating features\nAt this point in our project, we have defined the preprocessing method and designed the model. Now, it is time to have the job done. First, let’s convert the raw data (time-series type) into tabular data. Go to the Spectral Features tab and select [Save Parameters]. Alternatively, instead of using the default values, we can select the [Autotune parameters] button. In this case, the Studio will define new hyperparameters, such as the filter design and FFT length, based on the raw data.\n \nAt the top menu, select the Generate features tab, and there, select the options, Calculate feature importance, Normalize features, and press the [Generate features] button. Each 2-second window of data (300 data points) will be converted into a single tabular data point with 63 features.\n\nThe Feature Explorer will display this data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimensionality-reduction technique used for visualization, similar to t-SNE, but also for general nonlinear dimensionality reduction.\n\nThe visualization shows that the classes are well separated, indicating that the classifier should perform well.\n \nOptionally, you can analyze the relative importance of each feature for one class compared with other classes."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#training",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#training",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Training",
    "text": "Training\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n \nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of the data for validation for 30 epochs. After training, we can see that the accuracy is 100%.\n \nFor anomaly detection, we should select the suggested features that are most important for feature extraction. The number of clusters will be 32, as suggested by the Studio. After training, we can select some data for testing, such as maritime data. The resulting Anomally score was min: -0.1642, max: 0.0738, avg: -0.0867.\nWhen changing the data, it is possible to realize that small or negative Anomaly Scores indicate that the data are normal."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#testing",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#testing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Testing",
    "text": "Testing\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was very good (8%).\n \nYou should also use your kit (which is still connected to the Studio) and perform some Live Classification. For example, let’s test some “terrestrial” movement:\n \n\nBe aware that here, you will capture real data with your device and upload it to the Studio, where an inference will be made using the trained model (note that the model is not on your device)."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#deploy",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#deploy",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Deploy",
    "text": "Deploy\nNow it is time for magic! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the Arduino Library option, and then, at the bottom, choose Quantized (Int8) and click [Build]. A ZIP file will be created and downloaded to your computer.\n \nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#inference",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#inference",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Inference",
    "text": "Inference\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and in examples, select nano_ble_sense_accelerometer:\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes -------------------------------------------- */\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the “includes” portion with the code related to the IMU:\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\nChange the Constant Defines\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\nOn the setup function, initiate the IMU:\n   // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. In the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\n// Read IMU data\nfloat x = myIMU.readFloatAccelX();\nfloat y = myIMU.readFloatAccelY();\nfloat z = myIMU.readFloatAccelZ();\nYou should reorder the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (m/s2)”, followed by the test regarding the maximum acceptance range (that here is in m/s2, but on Arduino, was in Gs):\n  // Convert to m/s²\n  buffer[i + 0] = x * CONVERT_G_TO_MS2;\n  buffer[i + 1] = y * CONVERT_G_TO_MS2;\n  buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n  // Apply range limiting\n  for (int j = 0; j &lt; 3; j++) {\n      if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n          buffer[i + j] = copysign(MAX_ACCEPTED_RANGE, buffer[i + j]);\n      }\n  }\nAnd this is enough. We can also adjust how the inference is displayed in the Serial Monitor. You can now upload the complete code below to your device and proceed with the inferences.\n// Motion Classification with LSM6DS3TR-C IMU\n#include &lt;XIAOML_Kit_Motion_Class_-_AD_inferencing.h&gt;\n#include &lt;LSM6DS3.h&gt;\n#include &lt;Wire.h&gt;\n\n// IMU setup\nLSM6DS3 myIMU(I2C_MODE, 0x6A);\n\n// Inference settings\n#define CONVERT_G_TO_MS2    9.81f\n#define MAX_ACCEPTED_RANGE  2.0f * CONVERT_G_TO_MS2\n\nstatic bool debug_nn = false;\nstatic float buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE] = { 0 };\nstatic float inference_buffer[EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE];\n\nvoid setup() {\n    Serial.begin(115200);\n    while (!Serial) delay(10);\n\n    Serial.println(\"XIAOML Kit - Motion Classification\");\n    Serial.println(\"LSM6DS3TR-C IMU Inference\");\n\n    // Initialize IMU\n    if (myIMU.begin() != 0) {\n        Serial.println(\"ERROR: IMU initialization failed!\");\n        return;\n    }\n\n    Serial.println(\"✓ IMU initialized\");\n\n    if (EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME != 3) {\n        Serial.println(\"ERROR: EI_CLASSIFIER_RAW_SAMPLES_PER_FRAME\"\n                       \"should be 3\");\n        return;\n    }\n\n    Serial.println(\"✓ Model loaded\");\n    Serial.println(\"Starting motion classification...\");\n}\n\nvoid loop() {\n    ei_printf(\"\\nStarting inferencing in 2 seconds...\\n\");\n    delay(2000);\n\n    ei_printf(\"Sampling...\\n\");\n\n    // Clear buffer\n    for (size_t i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        buffer[i] = 0.0f;\n    }\n\n    // Collect accelerometer data\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i += 3) {\n        uint64_t next_tick = micros() +\n          (EI_CLASSIFIER_INTERVAL_MS * 1000);\n\n        // Read IMU data\n        float x = myIMU.readFloatAccelX();\n        float y = myIMU.readFloatAccelY();\n        float z = myIMU.readFloatAccelZ();\n\n        // Convert to m/s²\n        buffer[i + 0] = x * CONVERT_G_TO_MS2;\n        buffer[i + 1] = y * CONVERT_G_TO_MS2;\n        buffer[i + 2] = z * CONVERT_G_TO_MS2;\n\n        // Apply range limiting\n        for (int j = 0; j &lt; 3; j++) {\n            if (fabs(buffer[i + j]) &gt; MAX_ACCEPTED_RANGE) {\n                buffer[i + j] = copysign(MAX_ACCEPTED_RANGE,\n                                         buffer[i + j]);\n            }\n        }\n\n        delayMicroseconds(next_tick - micros());\n    }\n\n    // Copy to inference buffer\n    for (int i = 0; i &lt; EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE; i++) {\n        inference_buffer[i] = buffer[i];\n    }\n\n    // Create signal from buffer\n    signal_t signal;\n    int err = numpy::signal_from_buffer(inference_buffer,\n              EI_CLASSIFIER_DSP_INPUT_FRAME_SIZE, &signal);\n    if (err != 0) {\n        ei_printf(\"ERROR: Failed to create signal from buffer (%d)\\n\",\n                  err);\n        return;\n    }\n\n    // Run the classifier\n    ei_impulse_result_t result = { 0 };\n    err = run_classifier(&signal, &result, debug_nn);\n    if (err != EI_IMPULSE_OK) {\n        ei_printf(\"ERROR: Failed to run classifier (%d)\\n\", err);\n        return;\n    }\n\n    // Print predictions\n    ei_printf(\"Predictions (DSP: %d ms, Classification: %d ms, \"\n              \"Anomaly: %d ms):\\n\",\n        result.timing.dsp, result.timing.classification, result.timing.anomaly);\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        ei_printf(\"    %s: %.5f\\n\", result.classification[ix].label,\n                  result.classification[ix].value);\n    }\n\n    // Print anomaly score\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    ei_printf(\"Anomaly score: %.3f\\n\", result.anomaly);\n#endif\n\n    // Determine prediction\n    float max_confidence = 0.0;\n    String predicted_class = \"unknown\";\n\n    for (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n        if (result.classification[ix].value &gt; max_confidence) {\n            max_confidence = result.classification[ix].value;\n            predicted_class = String(result.classification[ix].label);\n        }\n    }\n\n    // Display result with confidence threshold\n    if (max_confidence &gt; 0.6) {\n        ei_printf(\"\\n🎯 PREDICTION: %s (%.1f%% confidence)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    } else {\n        ei_printf(\"\\n❓ UNCERTAIN: Highest confidence is %s (%.1f%%)\\n\",\n                 predicted_class.c_str(), max_confidence * 100);\n    }\n\n    // Check for anomaly\n#if EI_CLASSIFIER_HAS_ANOMALY == 1\n    if (result.anomaly &gt; 0.5) {\n        ei_printf(\"⚠️ ANOMALY DETECTED! Score: %.3f\\n\", result.anomaly);\n    }\n#endif\n\n    delay(1000);\n}\n\nvoid ei_printf(const char *format, ...) {\n    static char print_buf[1024] = { 0 };\n    va_list args;\n    va_start(args, format);\n    int r = vsnprintf(print_buf, sizeof(print_buf), format, args);\n    va_end(args);\n    if (r &gt; 0) {\n        Serial.write(print_buf);\n    }\n}\n\nThe complete code is available on the Lab’s GitHub.\n\nNow you should try your movements, seeing the result of the inference of each class on the images:\n \n \n \n \nAnd, of course, some “anomaly”, for example, putting the XIAO upside-down. The anomaly score will be over 0.5:"
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#post-prossessing",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#post-prossessing",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Post-Prossessing",
    "text": "Post-Prossessing\nNow that we know the model is working, we suggest modifying the code to see the result with the Kit completely offline (disconnected from the PC and powered by a battery, a power bank, or an independent 5V power supply).\nThe idea is that if a specific movement is detected, a corresponding message will appear on the OLED display.\n\nThe modified inference code to have the OLED display is available on the Lab’s GitHub."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#summary",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#summary",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Summary",
    "text": "Summary\nThis lab demonstrated how to build a complete motion classification system using the XIAOML Kit’s built-in LSM6DS3TR-C IMU sensor. Key achievements include:\nTechnical Implementation:\n\nUtilized the integrated 6-axis IMU for motion sensing\nCollected labeled training data for four transportation scenarios\nImplemented spectral feature extraction for time-series analysis\nDeployed a neural network classifier optimized for microcontroller inference\nAdded anomaly detection for identifying unusual movements\n\nMachine Learning Pipeline:\n\nData collection directly from embedded sensors\nFeature engineering using frequency domain analysis\nModel training and optimization in Edge Impulse\nReal-time inference on resource-constrained hardware\nPerformance monitoring and validation\n\nPractical Applications: The techniques learned apply directly to real-world scenarios, including:\n\nAsset tracking and logistics monitoring\nPredictive maintenance for machinery\nHuman activity recognition\nVehicle and equipment monitoring\nIoT sensor networks for smart cities\n\nKey Learnings:\n\nWorking with IMU coordinate systems and sensor fusion\nBalancing model accuracy with inference speed on edge devices\nImplementing robust data collection and preprocessing pipelines\nDeploying machine learning models to embedded systems\nIntegrating multiple sensors (IMU + display) for complete solutions\n\nThe integration of motion classification with the XIAOML Kit demonstrates how modern embedded systems can perform sophisticated AI tasks locally, enabling real-time decision-making without reliance on the cloud. This approach is fundamental to the future of edge AI in industrial IoT, autonomous systems, and smart device applications."
  },
  {
    "objectID": "content/xiaoml_kit/motion_classification/motion_classification.html#resources",
    "href": "content/xiaoml_kit/motion_classification/motion_classification.html#resources",
    "title": "Motion Classification and Anomaly Detection",
    "section": "Resources",
    "text": "Resources\n\nXIAOML KIT Code\nDSP Spectral Features\nEdge Impulse Project\nEdge Impulse Spectral Features Block Colab Notebook\nEdge Impulse Documentation\nEdge Impulse Spectral Features\nSeeed Studio LSM6DS3 Library"
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-overview-23c9",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-overview-23c9",
    "title": "DSP Spectral Features",
    "section": "Overview",
    "text": "Overview\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it."
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-extracting-features-review-f16c",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-extracting-features-review-f16c",
    "title": "DSP Spectral Features",
    "section": "Extracting Features Review",
    "text": "Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons."
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-tinyml-motion-classification-project-95fd",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-tinyml-motion-classification-project-95fd",
    "title": "DSP Spectral Features",
    "section": "A TinyML Motion Classification project",
    "text": "A TinyML Motion Classification project\n \nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n \nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50 Hz:\n \n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5 Hz instead of 50 Hz."
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-data-preprocessing-f6a3",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-data-preprocessing-f6a3",
    "title": "DSP Spectral Features",
    "section": "Data Pre-Processing",
    "text": "Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis \\(\\times\\) 2 seconds \\(\\times\\) 62.5 samples). The window is slid every 80 ms, creating a larger dataset where each instance has 375 “raw features.”\n \nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n \nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\nEdge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n \nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n \nPaste the data points to a new variable data:\ndata = [\n    -5.6330,  0.2376,  9.8701,\n    -5.9442,  0.4830,  9.8701,\n    -5.4217, ...\n]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where \\(N= 125\\) (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n \nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [\n    2.7322, -0.0978, -0.3813,\n    2.3980, 3.8924, 24.6841,\n    9.6303, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ]\nplot_data(sensors, axis, 'Raw Features')\n \nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [\n    (sum(x) / len(x))\n    for x in sensors\n]\n\n[\n    print('mean_' + x + ' =', round(y, 4))\n    for x, y in zip(axis, dtmean)\n][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')"
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-time-domain-statistical-features-1239",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-time-domain-statistical-features-1239",
    "title": "DSP Spectral Features",
    "section": "Time Domain Statistical features",
    "text": "Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of \\(n\\) values \\({𝑥_1, 𝑥_2, \\ldots, 𝑥_𝑛}\\), the RMS is: \\[\nx_{\\mathrm{RMS}} = \\sqrt{\\frac{1}{n} \\left( x_1^2 + x_2^2 + \\cdots + x_n^2 \\right)}\n\\]\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standardized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n \nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n \n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n \n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4))\n  for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]"
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-spectral-features-b39a",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-spectral-features-b39a",
    "title": "DSP Spectral Features",
    "section": "Spectral features",
    "text": "Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len,\n        # and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n \nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) # 13: 3+N_feat_axis;\n                           # 26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166"
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-timefrequency-domain-a386",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-timefrequency-domain-a386",
    "title": "DSP Spectral Features",
    "section": "Time-frequency domain",
    "text": "Time-frequency domain\n\nWavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n \nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n \nAs we did before, let’s copy and past the Processed Features:\n \nfeatures = [\n    3.6251, 0.0615, 0.0615,\n    -7.3517, -2.7641, 2.8462,\n    5.0924, ...\n]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline \\((y = 0)\\) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be \\(14\\times 2\\) (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\nWavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n \n\n\nFeature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n\nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\n        \"{:.2f}\".format(\n          (((my_array[:-1] * my_array[1:]) &lt; 0).sum()) / len(arr)\n        )\n    )\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\n    \"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\",\n    \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\",\n    \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"\n]\n\nL0_features_names = [\n    \"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\",\n    \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\",\n    \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"\n]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = (\n        stat_feat_l0[i]\n        + [skew_l0[i]]\n        + [kurtosis_l0[i]]\n        + [cross_l0[0][i]]\n        + [cross_l0[1][i]]\n        + [entropy_l0[i]]\n    )\n    [print(axis[i] + ' +x+= ', round(y, 4))\n       for x, y in zip(LO_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\n\nall_feat_l0 = [\n    item\n    for sublist in all_feat_l0\n    for item in sublist\n]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\n    feat_l1 = (\n        stat_feat_l1[i]\n        + [skew_l1[i]]\n        + [kurtosis_l1[i]]\n        + [cross_l1[0][i]]\n        + [cross_l1[1][i]]\n        + [entropy_l1[i]]\n    )\n    [print(axis[i]+' '+x+'= ', round(y, 4))\n       for x,y in zip(L1_features_names, feat_l1)][0]\n    all_feat_l1.append(feat_l1)\n\nall_feat_l1 = [\n    item\n    for sublist in all_feat_l1\n    for item in sublist\n]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")"
  },
  {
    "objectID": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-summary-c34e",
    "href": "content/shared/dsp_spectral_features_block/dsp_spectral_features_block.html#sec-dsp-spectral-features-summary-c34e",
    "title": "DSP Spectral Features",
    "section": "Summary",
    "text": "Summary\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge."
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-c86f",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-c86f",
    "title": "KWS Feature Engineering",
    "section": "Overview",
    "text": "Overview\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks"
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-kws-8c20",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-kws-8c20",
    "title": "KWS Feature Engineering",
    "section": "The KWS",
    "text": "The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition transcribes all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n \n\nApplications of KWS\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\nDifferences from General Speech Recognition\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately."
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-audio-signals-de7c",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-audio-signals-de7c",
    "title": "KWS Feature Engineering",
    "section": "Overview to Audio Signals",
    "text": "Overview to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16 kHz. Although music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n \n\nWhy Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning."
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-mfccs-9fcb",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-overview-mfccs-9fcb",
    "title": "KWS Feature Engineering",
    "section": "Overview to MFCCs",
    "text": "Overview to MFCCs\n\nWhat are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n \n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\nWhy are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\nComputing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: \\(y(t)=x(t)-\\alpha x(t-1)\\), where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n \n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f)=|X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n \n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n \n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector."
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-handson-using-python-dcd8",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-handson-using-python-dcd8",
    "title": "KWS Feature Engineering",
    "section": "Hands-On using Python",
    "text": "Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]"
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-summary-e7ea",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-summary-e7ea",
    "title": "KWS Feature Engineering",
    "section": "Summary",
    "text": "Summary\nWhat Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\nMFCCs are particularly strong for\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\nSpectrograms or MFEs are often more suitable for\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts."
  },
  {
    "objectID": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-resources-060a",
    "href": "content/shared/kws_feature_eng/kws_feature_eng.html#sec-kws-feature-engineering-resources-060a",
    "title": "KWS Feature Engineering",
    "section": "Resources",
    "text": "Resources\n\nAudio_Data_Analysis Colab Notebook"
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#introduction",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#introduction",
    "title": "Setup and No-Code Applications",
    "section": "Introduction",
    "text": "Introduction\n\nGrove Vision AI Module (V2) Overview\n\n\n\n\n\nThe Grove Vision AI (V2) is an MCU-based vision AI module that utilizes a Himax WiseEye2 HX6538 processor featuring a dual-core Arm Cortex-M55 and an integrated ARM Ethos-U55 neural network unit. The Arm Ethos-U55 is a machine learning (ML) processor class, specifically designed as a microNPU, to accelerate ML inference in area-constrained embedded and IoT devices. The Ethos-U55, combined with the AI-capable Cortex-M55 processor, provides a 480x uplift in ML performance over existing Cortex-M-based systems. Its clock frequency is 400 MHz, and its internal system memory (SRAM) is configurable, with a maximum capacity of 2.4 MB.\n\n\n\n\n\n\nNote: Based on Seeed Studio documentation, besides the Himax internal memory of 2.5MB (2.4MB SRAM + 64KB ROM), the Grove Vision AI (V2) is also equipped with a 16MB/133 MHz external flash.\n\n\n\n\n\n\nBelow is a block Diagram of the Grove Vision AI (V2) system, including a camera and a master controller.\n\n\n\n\n\nWith interfaces like IIC, UART, SPI, and Type-C, the Grove Vision AI (V2) can be easily connected to devices such as XIAO, Raspberry Pi, BeagleBoard, and ESP-based products for further development. For instance, integrating Grove Vision AI V2 with one of the devices from the XIAO family makes it easy to access the data resulting from inference on the device through the Arduino IDE or MicroPython, and conveniently connect to the cloud or dedicated servers, such as Home Assistance.\n\nUsing the I2C Grove connector, the Grove Vision AI V2 can be easily connected with any Master Device.\n\n\n\n\n\n\nBesides performance, another area to comment on is Power Consumption. For example, in a comparative test against the XIAO ESP32S3 Sense, running Swift-YOLO Tiny 96x96, despite achieving higher performance (30 FPS vs. 5.5 FPS), the Grove Vision AI V2 exhibited lower power consumption (0.35 W vs. 0.45 W) when compared with the XIAO ESP32S3 Sense.\n\n\n\n\n\n\nThe above comparison (and with other devices) can be found in the article 2024 MCU AI Vision Boards: Performance Comparison, which confirms the power of Grove Vision AI (V2).\n\n\n\nCamera Installation\nHaving the Grove Vision AI (V2) and camera ready, you can connect, for example, a Raspberry Pi OV5647 Camera Module via the CSI cable.\n\nWhen connecting, please pay attention to the direction of the row of pins and ensure they are plugged in correctly, not in the opposite direction."
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#the-sensecraft-ai-studio",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#the-sensecraft-ai-studio",
    "title": "Setup and No-Code Applications",
    "section": "The SenseCraft AI Studio",
    "text": "The SenseCraft AI Studio\nThe SenseCraft AI Studio is a robust platform that offers a wide range of AI models compatible with various devices, including the XIAO ESP32S3 Sense and the Grove Vision AI V2. In this lab, we will walk through the process of using an AI model with the Grove Vision AI V2 and preview the model’s output. We will also explore some key concepts, settings, and how to optimize the model’s performance.\n\n\n\n\n\nModels can also be deployed using the SenseCraft Web Toolkit, a simplified version of the SenseCraft AI Studio.\n\nWe can start using the SenseCraft Web Toolkit for simplicity, or go directly to the SenseCraft AI Studio, which has more resources.\n\n\nThe SenseCraft Web-Toolkit\nThe SenseCraft Web Toolkit is a visual model deployment tool included in the SSCMA(Seeed SenseCraft Model Assistant). This tool enables us to deploy models to various platforms with ease through simple operations. The tool offers a user-friendly interface and does not require any coding.\nThe SenseCraft Web Toolkit is based on the Himax AI Web Toolkit, which can (optionally) be downloaded from here. Once downloaded and unzipped to the local PC, double-click index.html to run it locally.\n\n\n\n\n\nBut in our case, let’s follow the steps below to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website on a web browser as Chrome.\nConnect Grove Vision AI (V2) to your computer using a Type-C cable.\nHaving the XIAO connected, select it as below:\n\n\n\n\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\n\n\n\n\nNote: The WebUSB tool may not function correctly in certain browsers, such as Safari. Use Chrome instead.\n\nWe can try several Basic Computer Vision models previously uploaded by Seeed Studio. Passing the cursor over the AI models, we can have some information about them, such as name, description, category (Image Classification, Object Detection, or Pose/Keypoint Detection), the algorithm (like YOLO V5 or V8, FOMO, MobileNet V2, etc.) and metrics (Accuracy or mAP).\n\n\n\n\n\nWe can choose one of those ready-to-use AI models by clicking on it and pressing the [Send] button, or upload our model.\nFor the SenseCraft AI platform, follow the instructions here."
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#exploring-cv-ai-models",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#exploring-cv-ai-models",
    "title": "Setup and No-Code Applications",
    "section": "Exploring CV AI models",
    "text": "Exploring CV AI models\n\nObject Detection\nObject detection is a pivotal technology in computer vision that focuses on identifying and locating objects within digital images or video frames. Unlike image classification, which categorizes an entire image into a single label, object detection recognizes multiple objects within the image and determines their precise locations, typically represented by bounding boxes. This capability is crucial for a wide range of applications, including autonomous vehicles, security, surveillance systems, and augmented reality, where understanding the context and content of the visual environment is essential.\nCommon architectures that have set the benchmark in object detection include the YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), FOMO (Faster Objects, More Objects), and Faster R-CNN (Region-based Convolutional Neural Networks) models.\nLet’s choose one of the ready-to-use AI models, such as Person Detection, which was trained using the Swift-YOLO algorithm.\n\n\n\n\n\nOnce the model is uploaded successfully, you can see the live feed from the Grove Vision AI (V2) camera in the Preview area on the right. Also, the inference details can be shown on the Serial Monitor by clicking on the [Device Log] button at the top.\n\n\n\n\n\n\nIn the SenseCraft AI Studio, the Device Logger is always on the screen.\n\nPointing the camera at me, only one person was detected, so that the model output will be a single “box”. Looking in detail, the module sends continuously two lines of information:\n\n\n\n\n\nperf (Performance), displays latency in milliseconds.\n\nPreprocess time (image capture and Crop): 7ms;\nInference time (model latency): 76ms (13 fps)\nPostprocess time (display of the image and inclusion of data): less than 0ms.\n\nboxes: Show the objects detected in the image. In this case, only one.\n\nThe box has the x, y, w, and h coordinates of (245, 292,449,392), and the object (person, label 0) was captured with a value of .89.\n\nIf we point the camera at an image with several people, we will get one box for each person (object):\n\n\n\n\n\n\nOn the SenseCraft AI Studio, the inference latency (48ms) is lower than on the SenseCraft ToolKit (76ms), due to a distinct deployment implementation.\n\n\n\n\n\n\nPower Consumption\nThe peak power consumption running this Swift-YOLO model was 410 milliwatts.\nPreview Settings\nWe can see that in the Settings, two settings options can be adjusted to optimize the model’s recognition accuracy.\n\nConfidence: Refers to the level of certainty or probability assigned to its predictions by a model. This value determines the minimum confidence level required for the model to consider a detection as valid. A higher confidence threshold will result in fewer detections but with higher certainty, while a lower threshold will allow more detections but may include some false positives.\nIoU: Used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes. IoU is a metric that measures the overlap between the predicted bounding box and the ground truth bounding box. It is used to determine the accuracy of the object detection. The IoU threshold sets the minimum IoU value required for a detection to be considered a true positive. Adjusting this threshold can help in fine-tuning the model’s precision and recall.\n\n\n\n\n\n\n\nExperiment with different values for the Confidence Threshold and IoU Threshold to find the optimal balance between detecting persons accurately and minimizing false positives. The best settings may vary depending on our specific application and the characteristics of the images or video feed.\n\n\n\nPose/Keypoint Detection\nPose or keypoint detection is a sophisticated area within computer vision that focuses on identifying specific points of interest within an image or video frame, often related to human bodies, faces, or other objects of interest. This technology can detect and map out the various keypoints of a subject, such as the joints on a human body or the features of a face, enabling the analysis of postures, movements, and gestures. This has profound implications for various applications, including augmented reality, human-computer interaction, sports analytics, and healthcare monitoring, where understanding human motion and activity is crucial.\nUnlike general object detection, which identifies and locates objects, pose detection drills down to a finer level of detail, capturing the nuanced positions and orientations of specific parts. Leading architectures in this field include OpenPose, AlphaPose, and PoseNet, each designed to tackle the challenges of pose estimation with varying degrees of complexity and precision. Through advancements in deep learning and neural networks, pose detection has become increasingly accurate and efficient, offering real-time insights into the intricate dynamics of subjects captured in visual data.\nSo, let’s explore this popular CV application, Pose/Keypoint Detection.\n\n\n\n\n\nStop the current model inference by pressing [Stop] in the Preview area. Select the model and press [Send]. Once the model is uploaded successfully, you can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (accessible by clicking the [Device Log] button at the top).\n\n\n\n\n\nThe YOLOV8 Pose model was trained using the COCO-Pose Dataset, which contains 200K images labeled with 17 keypoints for pose estimation tasks.\nLet’s look at a single screenshot of the inference (to simplify, let’s analyse an image with a single person in it). We can note that we have two lines, one with the inference performance in milliseconds (121 ms) and a second line with the keypoints as below:\n\n1 box of info, the same as we got with the object detection example (box coordinates (113, 119, 67, 208), inference result (90), label (0).\n17 groups of 4 numbers represent the 17 “joints” of the body, where ‘0’ is the nose, ‘1’ and ‘2’ are the eyes, ‘15’ and’ 16’ are the feet, and so on.\n\n\n\n\n\n\n\nTo understand a pose estimation project more deeply, please refer to the tutorial: Exploring AI at the Edge! - Pose Estimation.\n\n\n\nImage Classification\nImage classification is a foundational task within computer vision aimed at categorizing entire images into one of several predefined classes. This process involves analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the predominant object or scene it contains.\nImage classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as ImageNet, by learning hierarchical representations of visual data.\nAs the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let’s also explore this computer vision application.\n\n\n\n\n\n\nThis example is available on the SenseCraft ToolKit, but not in the SenseCraft AI Studio. In the last one, it is possible to find other examples of Image Classification.\n\nAfter the model is uploaded successfully, we can view the live feed from the Grove Vision AI (V2) camera in the Preview area on the right, along with the inference details displayed in the Serial Monitor (by clicking the [Device Log] button at the top).\n\n\n\n\n\nAs a result, we will receive a score and the class as output.\n\n\n\n\n\nFor example, [99, 1] means class: 1 (Person) with a score of 0.99. Once this model is a binary classification, class 0 will be “No Person” (or Background). The Inference latency is 15ms or around 70fps.\n\nPower Consumption\nTo run the Mobilenet V2 0.35, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\nRuning the same model on XIAO ESP32S3 Sense, the power consumption was 523mW with a latency of 291ms.\n\n\n\n\n\n\n\n\nExploring Other Models on SenseCraft AI Studio\nSeveral public AI models can also be downloaded from the SenseCraft AI WebPage. For example, you can run a Swift-YOLO model, detecting traffic lights as shown here:\n\n\n\n\n\nThe latency of this model is approximately 86 ms, with an average power consumption of 420 mW."
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#an-image-classification-project",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#an-image-classification-project",
    "title": "Setup and No-Code Applications",
    "section": "An Image Classification Project",
    "text": "An Image Classification Project\nLet’s create a complete Image Classification project, using the SenseCraft AI Studio.\n\n\n\n\n\nOn SenseCraft AI Studio: Let’s open the tab Training:\n\n\n\n\n\nThe default is to train a Classification model with a WebCam if it is available. Let’s select the Grove Vision AI V2 instead. Pressing the green button[Connect], a Pop-Up window will appear. Select the corresponding Port and press the blue button [Connect].\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\nThe Goal\nThe first step is always to define a goal. Let’s classify, for example, two simple objects—for instance, a toy box and a toy wheel. We should also include a 3rd class of images, background, where no object is in the scene.\n\n\n\n\n\n\n\nData Collection\nLet’s create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: box\nClass 3: wheel\n\n\n\n\n\n\nSelect one of the classes and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class and go to Training Step:\n\n\nTraining\nConfirm if the correct device is selected (Grove Vision AI V2) and press [Start Training]\n\n\n\n\n\n\n\nTest\nAfter training, the inference result can be previewed.\n\nNote that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a live preview using the training model, which is running in the Studio.\n\n\n\n\n\n\nNow is time to really deploy the model in the device:\n\n\nDeployment\nSelect the trained model on [Deploy to device], select the Grove Vision AI V2:\n\n\n\n\n\nThe Studio will redirect us to the Vision Workplace tab. Confirm the deployment, select the appropriate Port, and connect it:\n\n\n\n\n\nThe model will be flashed into the device. After an automatic reset, the model will start runing on the device. On the Device Logger, we can see that the inference has a latency of approximately 8 ms, corresponding to a frame rate of 125 frames per second (FPS).\nAlso, note that it is possible to adjust the model’s confidence.\n\n\n\n\n\n\nTo run the Image Classification Model, the Grove Vision AI V2 had a peak current of 80mA at 5.24V, resulting in a power consumption of 420mW.\n\n\n\nSaving the Model\nIt is possible to save the model in the SenseCraft AI Studio. The Studio will keep all our models, which can be deployed later. For that, return to the Training tab and select the button [Save to SenseCraft]:"
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#conclusion",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#conclusion",
    "title": "Setup and No-Code Applications",
    "section": "Conclusion",
    "text": "Conclusion\nIn this lab, we explored several computer vision (CV) applications using the Seeed Studio Grove Vision AI Module V2, demonstrating its exceptional capabilities as a powerful yet compact device specifically designed for embedded machine learning applications.\nPerformance Excellence: The Grove Vision AI V2 demonstrated remarkable performance across multiple computer vision tasks. With its Himax WiseEye2 chip featuring a dual-core Arm Cortex-M55 and integrated ARM Ethos-U55 neural network unit, the device delivered:\n\nImage Classification: 15 ms inference time (67 FPS)\nObject Detection (Person): 48 ms to 76 ms inference time (21 FPS to 13 FPS)\nPose Detection: 121 ms real-time keypoint detection with 17-joint tracking (8 FPS)\n\nPower Efficiency Leadership: One of the most compelling advantages of the Grove Vision AI V2 is its superior power efficiency. Comparative testing revealed significant improvements over traditional embedded platforms:\n\nGrove Vision AI V2: 80 mA (410 mW) peak consumption (60+ FPS)\nXIAO ESP32S3: Performing similar CV tasks (Image Classification) 523 mW (3+ FPS)\n\nPractical Implementation: The device’s versatility was demonstrated through a comprehensive end-to-end project, encompassing dataset creation, model training, deployment, and offline inference.\nDeveloper-Friendly Ecosystem: The SenseCraft AI Studio, with its no-code deployment and integration capabilities for custom applications, makes the Grove Vision AI V2 accessible to both beginners and advanced developers. The extensive library of pre-trained models and support for custom model deployment provide flexibility for diverse applications.\nThe Grove Vision AI V2 represents a significant advancement in edge AI hardware, offering professional-grade computer vision capabilities in a compact, energy-efficient package that democratizes AI deployment for embedded applications across industrial, IoT, and educational domains.\nKey Takeaways\nThis Lab demonstrates that sophisticated computer vision applications are not limited to cloud-based solutions or power-hungry hardware, as the Raspberry Pi or Jetson Nanos – they can now be deployed effectively at the edge with remarkable efficiency and performance.\nOptionally, we can have the XIAO Vision AI Camera. This innovative vision solution seamlessly combines the Grove Vision AI V2 module, XIAO ESP32-C3 controller, and an OV5647 camera, all housed in a custom 3D-printed enclosure:"
  },
  {
    "objectID": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#resourses",
    "href": "content/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html#resourses",
    "title": "Setup and No-Code Applications",
    "section": "Resourses",
    "text": "Resourses\nSenseCraft AI Studio Instructions.\nSenseCraft-Web-Toolkit website.\nSenseCraft AI Studio\nHimax AI Web Toolkit\nHimax examples"
  },
  {
    "objectID": "content/grove_vision_ai_v2/image_classification/image_classification.html#introduction",
    "href": "content/grove_vision_ai_v2/image_classification/image_classification.html#introduction",
    "title": "Image Classification",
    "section": "Introduction",
    "text": "Introduction\nSo far, we have explored several computer vision models previously uploaded by Seeed Studio or used the SenseCraft AI Studio for Image Classification, without choosing a specific model. Let’s now develop our Image Classification project from scratch, where we will select our data and model.\nBelow, we can see the project’s main steps and where we will work with them:\n\n\n\n\n\n\nProject Goal\nThe first step in any machine learning (ML) project is defining the goal. In this case, the goal is to detect and classify two specific objects present in a single image. For this project, we will use two small toys: a robot and a small Brazilian parrot (named Periquito). Also, we will collect images of a background where those two objects are absent.\n\n\n\n\n\n\n\nData Collection\nWith the Machine Learning project goal defined, dataset collection is the next and most crucial step. Suppose your project utilizes images that are publicly available on datasets, for example, to be used on a Person Detection project. In that case, you can download the Wake Vision dataset for use in the project.\nBut, in our case, we define a project where the images do not exist publicly, so we need to generate them. We can use a phone, computer camera, or other devices to capture the photos, offline or connected to the Edge Impulse Studio.\nIf you want to use the Grove Vision AI V2 to capture your dataset, you can use the SenseCraft AI Studio as we did in the previous Lab, or the camera_web_server sketch as we will describe later in the Postprocessing / Getting the Video Stream section of this Lab.\n\n\n\n\n\nIn this Lab, we will use the SenseCraft AI Studio to collect the dataset.\n\n\nCollecting Data with the SenseCraft AI Studio\nOn SenseCraft AI Studio: Let’s open the tab Training.\nThe default is to train a Classification model with a WebCam if it is available. Let’s select the Grove Vision AI V2 instead. Pressing the green button[Connect] (1), a Pop-Up window will appear. Select the corresponding Port (2) and press the blue button [Connect] (3).\n\n\n\n\n\nThe image streamed from the Grove Vision AI V2 will be displayed.\n\nImage Collection\nLet’s create the classes, following, for example, an alphabetical order:\n\nClass1: background\nClass 2: periquito\nClass 3: robot\n\n\n\n\n\n\nSelect one of the classes (note that a green line will be around the window) and keep pressing the green button under the preview area. The collected images will appear on the Image Samples Screen.\n\n\n\n\n\nAfter collecting the images, review them and, if necessary, delete any incorrect ones.\n\n\n\n\n\nCollect around 50 images from each class. After you collect the three classes, open the menu on each of them and select Export Data.\n\n\n\n\n\nIn the Download area of the Computer, we will get three zip files, each one with its corresponding class name. Each Zip file contains a folder with the images.\n\n\n\nUploading the dataset to the Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. Edge Impulseis a leading development platform for machine learning on edge devices.\n\nEnter your account credentials (or create a free account) at Edge Impulse.\nNext, create a new project:\n\n\n\n\n\n\n\nThe dataset comprises approximately 50 images per label, with 40 for training and 10 for testing.\n\n\n\nImpulse Design and Pre-Processing\nImpulse Design\nAn impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.\nClassifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach” or “model” each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as “Transfer Learning” (TL). With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.\n\n\n\n\n\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\n\n\n\n\nFor comparison, we will keep the image size as 96 x 96. However, keep in mind that with the Grove Vision AI Module V2 and its internal SRAM of 2.4 MB, larger images can be utilized (for example, 160 x 160).\n\nAlso select the Target device (Himax WiseEye2 (M55 400 MHz + U55)) on the up-right corner.\n\n\nPre-processing (Feature generation)\nBesides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let’s select [RGB] in the Image section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing [Save Parameters] will open a new tab, Generate Features. Press the button [Generate Features]to generate the features.\n\n\nModel Design, Training, and Test\nIn 2007, Google introduced MobileNetV1. In 2018, MobileNetV2: Inverted Residuals and Linear Bottlenecks, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, α (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\n\nFor comparison, we will use the MobileNet V2 0.1 as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 8 neurons with a 10% dropout rate for preventing overfitting.\n\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nSet the Hyperparameters:\n\nEpochs: 20,\nBach Size: 32\nLearning Rate: 0.0005\nValidation size: 20%\n\nTraining result:\n\n\n\n\n\nThe model profile predicts 146 KB of RAM and 187 KB of Flash, indicating no problem with the Grove AI Vision (V2), which has almost 2.5 MB of internal SRAM. Additionally, the Studio indicates a latency of around 4 ms.\n\nDespite this, with a 100% accuracy on the Validation set when using the spare data for testing, we confirmed an Accuracy of 81%, using the Quantized (Int8) trained model. However, it is sufficient for our purposes in this lab.\n\n\n\nModel Deployment\nOn the Deployment tab, we should select: Seeed Grove Vision AI Module V2 (Himax WiseEye2) and press [Build]. A ZIP file will be downloaded to our computer.\nThe Zip file contains the model_vela.tflite, which is a TensorFlow Lite (TFLite) model optimized for neural processing units (NPUs) using the Vela compiler, a tool developed by Arm to adapt TFLite models for Ethos-U NPUs.\n\n\n\n\n\nWe can flash the model following the instructions in the README.txt or use the SenseCraft AI Studio. We will use the latter.\n\n\nDeploy the model on the SenseCraft AI Studio\nOn SenseCraft AI Studio, go to the Vision Workspace tab, and connect the device:\n\n\n\n\n\nYou should see the last model that was uploaded to the device. Select the green button [Upload Model]. A pop-up window will ask for the model name, the model file, and to enter the class names (objects). We should use labels following alphabetical order: 0: background, 1: periquito, and 2: robot, and then press [Send].\n\n\n\n\n\nAfter a few seconds, the model will be uploaded (“flashed”) to our device, and the camera image will appear in real-time on the Preview Sector. The Classification result will be displayed under the image preview. It is also possible to select the Confidence Threshold of your inference using the cursor on Settings.\nOn the Device Logger, we can view the Serial Monitor, where we can observe the latency, which is approximately 1 to 2 ms for pre-processing and 4 to 5 ms for inference, aligning with the estimates made in Edge Impulse Studio.\n\n\n\n\n\nHere are other screenshots:\n\n\n\n\n\nThe power consumption of this model is approximately 70 mA, equivalent to 0.4 W.\n\n\nImage Classification (non-official) Benchmark\nSeveral development boards can be used for embedded machine learning (tinyML), and the most common ones (so far) for Computer Vision applications (with low energy) are the ESP32 CAM, the Seeed XIAO ESP32S3 Sense, and the Arduino Nicla Vision.\nTaking advantage of this opportunity, a similarly trained model, MobilenetV2 96x96, with an alpha of 0.1, was also deployed on the ESP-CAM, the XIAO, and a Raspberry Pi Zero W2. Here is the result:\n\n\n\n\n\n\nThe Grove Vision AI V2 with an ARM Ethus-U55 was approximately 14 times faster than devices with an ARM-M7, and more than 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi, with a much more powerful CPU, the U55 reduces latency by almost half. Additionally, the power consumption is lower than that of other devices (see the full article here for power consumption comparison).\n\n\n\nPostprocessing\nNow that we have the model uploaded to the board and working correctly, classifying our images, let’s connect a Master Device to export the inference result to it and see the result completely offline (disconnected from the PC and, for example, powered by a battery).\n\nNote that we can use any microcontroller as a Master Controller, such as the XIAO, Arduino, or Raspberry Pi.\n\n\nGetting the Video Stream\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO (Master Controller) via IIC. For that, we will use the Arduino SSMA library. This library’s primary purpose is to process Grove Vision AI’s data stream, which does not involve model inference.\n\nThe Grove Vision AI (V2) communicates (Inference result) with the XIAO via the IIC; the device’s IIC address is 0x62. Image information transfer is via the USB serial port.\n\nStep 1: Download the Arduino SSMA library as a zip file from its GitHub:\n\n\n\n\n\nStep 2: Install it in the Arduino IDE (sketch &gt; Include Library &gt; Add .Zip Library).\nStep 3: Install the ArduinoJSON library.\n\n\n\n\n\nStep 4: Install the Eigen Library\n\n\n\n\n\nStep 3: Now, connect the XIAO and Grove Vision AI (V2) via the socket (a row of pins) located at the back of the device.\n\n\n\n\n\n\nCAUTION: Please note the direction of the connection, Grove Vision AI’s Type-C connector should be in the same direction as XIAO’s Type-C connector.\n\nStep 5: Connect the XIAO USB-C port to your computer\n\n\n\n\n\nStep 6: In the Arduino IDE, select the Xiao board and the corresponding USB port.\nOnce we want to stream the video to a webpage, we will use the XIAO ESP32S3, which has wifi and enough memory to handle images. Select XIAO_ESP32S3 and the appropriate USB Port:\n\n\n\n\n\nBy default, the PSRAM is disabled. Open the Tools menu and on PSRAM: \"OPI PSRAM\"select OPI PSRAM.\n\n\n\n\n\nStep 7: Open the example in Arduino IDE:\nFile -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; camera_web_server.\nAnd edit the ssid and password in the camera_web_server.ino sketch to match the Wi-Fi network.\nStep 8: Upload the sketch to the board and open the Serial Monitor. When connected to the Wi-Fi network, the board’s IP address will be displayed.\n\n\n\n\n\nOpen the address using a web browser. A Video App will be available. To see only the video stream from the Grove Vision AI V2, press [Sample Only] and [Start Stream].\n\n\n\n\n\nIf you want to create an image dataset, you can use this app, saving frames of the video generated by the device. Pressing [Save Frame], the image will be saved in the download area of our desktop.\n\n\n\n\n\nOpening the App without selecting [Sample Only], the inference result should appear on the video screen, but this does not happen for Image Classification. For Object Detection or Pose Estimation, the result is embedded with the video stream.\nFor example, if the model is a Person Detection using YoloV8:\n\n\n\n\n\n\n\nGetting the Inference Result\n\nGo to File -&gt; Examples -&gt; Seeed_Arduino_SSCMA -&gt; inference_class.\nUpload the sketch to the board, and open the Serial Monitor.\nPointing the camera at one of our objects, we can see the inference result on the Serial Terminal.\n\n\n\n\n\n\n\nThe inference running on the Arduino IDE had an average consumption of 160 mA or 800 mW and a peak of 330 mA 1.65 W when transmitting the image to the App.\n\n\n\nPostprocessing with LED\nThe idea behind our postprocessing is that whenever a specific image is detected (for example, the Periquito - Label:1), the User LED is turned on. If the Robot or a background is detected, the LED will be off.\nCopy the below code and past it to your IDE:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\nvoid setup()\n{\n    AI.begin();\n    \n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n    // Pins for the built-in LED\n    pinMode(LED_BUILTIN, OUTPUT);\n    // Ensure the LED is OFF by default.\n    // Note: The LED is ON when the pin is LOW, OFF when HIGH.\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_led(pred_index);\n    }\n}\n\n/**\n* @brief      turn_off_led function - turn-off the User LED\n*/\nvoid turn_off_led(){\n    digitalWrite(LED_BUILTIN, HIGH);\n}\n\n/**\n* @brief      turn_on_led function used to turn on the User LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; ALL OFF\n*             label 1: [1] ==&gt; LED ON\n*             label 2: [2] ==&gt; ALL OFF\n*             label 3: [3] ==&gt; ALL OFF\n*/\nvoid turn_on_led(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_led();\n            break;\n        case 1:\n            turn_off_led();\n            digitalWrite(LED_BUILTIN, LOW);\n            break;\n        case 2:\n            turn_off_led();\n            break;\n        case 3:\n            turn_off_led();\n            break;\n    }\n}\nThis sketch uses the Seeed_Arduino_SSCMA.h library to interface with the Grove Vision AI Module V2. The AI module and the LED are initialized in the setup() function, and serial communication is started.\nThe loop() function repeatedly calls the invoke() method to perform inference using the built-in algorithms of the Grove Vision AI Module V2. Upon a successful inference, the sketch prints out performance metrics to the serial monitor, including preprocessing, inference, and postprocessing times.\nThe sketch processes and prints out detailed information about the results of the inference:\n\n(AI.classes()[0]) that identifies the class of image (.target) and its confidence score (.score).\nThe inference result (class) is stored in the integer variable pred_index, which will be used as an input to the function turn_on_led(). As a result, the LED will turn ON, depending on the classification result.\n\nHere is the result:\nIf the Periquito is detected (Label:1), the LED is ON:\n\n\n\n\n\nIf the Robot is detected (Label:2) the LED is OFF (Same for Background (Label:0):\n\n\n\n\n\nTherefore, we can now power the Grove Viaon AI V2 + Xiao ESP32S3 with an external battery, and the inference result will be displayed by the LED completely offline. The consumption is approximately 165 mA or 825 mW.\n\nIt is also possible to send the result using Wifi, BLE, or other communication protocols available on the used Master Device.\n\n\n\n\nOptional: Post-processing on external devices\nOf course, one of the significant advantages of working with EdgeAI is that devices can run entirely disconnected from the cloud, allowing for seamless interactions with the real world. We did it in the last section, but using the internal Xiao LED. Now, we will connect external LEDs (which could be any actuator).\n\n\n\n\n\n\nThe LEDS should be connected to the XIAO ground via a 220-ohm resistor.\n\n\n\n\n\n\nThe idea is to modify the previous sketch to handle the three external LEDs.\nGOAL: Whenever the image of a Periquito is detected, the LED Green will be ON; if it is a Robot, the LED Yellow will be ON; if it is a Background, the LED Red will be ON.\nThe image processing and model inference are processed locally in Grove Vision AI (V2), and we want the result to be output to the XIAO via IIC. For that, we will use the Arduino SSMA library again.\nHere the scketch to be used:\n#include &lt;Seeed_Arduino_SSCMA.h&gt;\nSSCMA AI;\n\n// Define the LED pin according to the pin diagram\n// The LEDS negative lead should be connected to the XIAO ground \n// via a 220-ohm resistor. \nint LEDR = D1; # XIAO ESP32S3 Pin 1\nint LEDY = D2; # XIAO ESP32S3 Pin 2\nint LEDG = D3; # XIAO ESP32S3 Pin 3\n\n  void setup()\n{\n    AI.begin();\n    \n    Serial.begin(115200);\n    while (!Serial);\n    Serial.println(\"Inferencing - Grove AI V2 / XIAO ESP32S3\");\n\n// Initialize the external LEDs\n    pinMode(LEDR, OUTPUT);\n    pinMode(LEDY, OUTPUT);\n    pinMode(LEDG, OUTPUT);\n    // Ensure the LEDs are OFF by default.\n    // Note: The LEDs are ON when the pin is HIGH, OFF when LOW.\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\nvoid loop()\n{\n    if (!AI.invoke()){\n        Serial.println(\"\\nInvoke Success\");\n        Serial.print(\"Latency [ms]: prepocess=\");\n        Serial.print(AI.perf().prepocess);\n        Serial.print(\", inference=\");\n        Serial.print(AI.perf().inference);\n        Serial.print(\", postpocess=\");\n        Serial.println(AI.perf().postprocess);\n        int pred_index = AI.classes()[0].target;\n        Serial.print(\"Result= Label: \");\n        Serial.print(pred_index);\n        Serial.print(\", score=\");\n        Serial.println(AI.classes()[0].score);\n        turn_on_leds(pred_index);\n    }\n}\n\n\n/**\n* @brief turn_off_leds function - turn-off all LEDs\n*/\nvoid turn_off_leds(){\n    digitalWrite(LEDR, LOW);\n    digitalWrite(LEDY, LOW);\n    digitalWrite(LEDG, LOW);\n}\n\n/**\n* @brief turn_on_leds function used to turn on an specif LED\n* @param[in]  pred_index\n*             label 0: [0] ==&gt; Red ON\n*             label 1: [1] ==&gt; Green ON\n*             label 2: [2] ==&gt; Yellow ON\n*/\nvoid turn_on_leds(int pred_index) {\n    switch (pred_index)\n    {\n        case 0:\n            turn_off_leds();\n            digitalWrite(LEDR, HIGH);\n            break;\n        case 1:\n            turn_off_leds();\n            digitalWrite(LEDG, HIGH);\n            break;\n        case 2:\n            turn_off_leds();\n            digitalWrite(LEDY, HIGH);\n            break;\n        case 3:\n            turn_off_leds();\n            break;\n    }\n}\nWe should connect the Grove Vision AI V2 with the XIAO using its I2C Grove connector. For the XIAO, we will use an Expansion Board for the facility (although it is possible to connect the I2C directly to the XIAO’s pins). We will power the boards using the USB-C connector, but a battery can also be used.\n\n\n\n\n\nHere is the result:\n\n\n\n\n\n\nThe power consumption reached a peak of 240 mA (Green LED), equivalent to 1.2 W. Driving the Yellow and Red LEDs consumes 14 mA, equivalent to 0.7 W. Sending information to the terminal via serial has no impact on power consumption."
  },
  {
    "objectID": "content/grove_vision_ai_v2/image_classification/image_classification.html#conclusion",
    "href": "content/grove_vision_ai_v2/image_classification/image_classification.html#conclusion",
    "title": "Image Classification",
    "section": "Conclusion",
    "text": "Conclusion\nIn this lab, we’ve explored the complete process of developing an image classification system using the Seeed Studio Grove Vision AI Module V2 powered by the Himax WiseEye2 chip. We’ve walked through every stage of the machine learning workflow, from defining our project goals to deploying a working model with real-world interactions.\nThe Grove Vision AI V2 has demonstrated impressive performance, with inference times of just 4-5ms, dramatically outperforming other common tinyML platforms. Our benchmark comparison showed it to be approximately 14 times faster than ARM-M7 devices and over 100 times faster than an Xtensa LX6 (ESP-CAM). Even when compared to a Raspberry Pi Zero W2, the Edge TPU architecture delivered nearly twice the speed while consuming less power.\nThrough this project, we’ve seen how transfer learning enables us to achieve good classification results with a relatively small dataset of custom images. The MobileNetV2 model with an alpha of 0.1 provided an excellent balance of accuracy and efficiency for our three-class problem, requiring only 146 KB of RAM and 187 KB of Flash memory, well within the capabilities of the Grove Vision AI Module V2’s 2.4 MB internal SRAM.\nWe also explored several deployment options, from viewing inference results through the SenseCraft AI Studio to creating a standalone system with visual feedback using LEDs. The ability to stream video to a web browser and process inference results locally demonstrates the versatility of edge AI systems for real-world applications.\nThe power consumption of our final system remained impressively low, ranging from approximately 70mA (0.4W) for basic inference to 240mA (1.2W) when driving external components. This efficiency makes the Grove Vision AI Module V2 an excellent choice for battery-powered applications where power consumption is critical.\nThis lab has demonstrated that sophisticated computer vision tasks can now be performed entirely at the edge, without reliance on cloud services or powerful computers. With tools like Edge Impulse Studio and SenseCraft AI Studio, the development process has become accessible even to those without extensive machine learning expertise.\nAs edge AI technology continues to evolve, we can expect even more powerful capabilities from compact, energy-efficient devices like the Grove Vision AI Module V2, opening up new possibilities for smart sensors, IoT applications, and embedded intelligence in everyday objects."
  },
  {
    "objectID": "content/grove_vision_ai_v2/image_classification/image_classification.html#resources",
    "href": "content/grove_vision_ai_v2/image_classification/image_classification.html#resources",
    "title": "Image Classification",
    "section": "Resources",
    "text": "Resources\nCollecting Images with SenseCraft AI Studio.\nEdge Impulse Studio Project\nSenseCraft AI Studio - Vision Workplace (Deploy Models)\nOther Himax examples\nArduino Sketches"
  },
  {
    "objectID": "content/grove_vision_ai_v2/object_detection/object_detection.html",
    "href": "content/grove_vision_ai_v2/object_detection/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "This Lab is under Development"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis” by Wes McKinney\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden and Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake and Jenny Plunkett\n“XIAO: Big Power, Small Board” by Lei Feng and Marcelo Rovai\n“MACHINE LEARNING SYSTEMS for TinyML” by a collaborative effort\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  },
  {
    "objectID": "about_the_author.html",
    "href": "about_the_author.html",
    "title": "About the author",
    "section": "",
    "text": "Marcelo Rovai, a Brazilian living in Chile, is a recognized figure in engineering and technology education. He holds the title of Professor Honoris Causa from the Federal University of Itajubá (UNIFEI), Brazil. His educational background includes an Engineering degree from UNIFEI and a specialization from the Polytechnic School of São Paulo University (USP). Further enhancing his expertise, he earned an MBA from IBMEC (INSPER) and a Master’s in Data Science from the Universidad del Desarrollo (UDD) in Chile.\nWith a career spanning several high-profile technology companies such as AVIBRAS Airspace, ATT, NCR, and IGT, where he served as Vice President for Latin America, he brings a wealth of industry experience to his academic endeavors. He is a prolific writer on electronics-related topics and shares his knowledge through open platforms like Hackster.io.\nIn addition to his professional pursuits, he is dedicated to educational outreach, serving as a volunteer professor at UNIFEI and engaging with the TinyML4D group as a Co-Chair, promoting TinyML education in developing countries. His work underscores a commitment to leveraging technology for societal advancement.\nLinkedIn profile: https://www.linkedin.com/in/marcelo-jose-rovai-brazil-chile/\nLectures, books, papers, and tutorials: https://github.com/Mjrovai/TinyML4D"
  }
]