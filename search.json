[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TinyML Made Easy",
    "section": "",
    "text": "Preface\nFinding the right info used to be the big issue for people involved in technical projects. Nowadays, there is a deluge of information, but it is not easy to determine what can and what cannot be trusted. A good pointer is to look for the credentials and the affiliation of the author of a document or that of the associated institutions. Since 1992 EsLaRed has been providing training in different aspects of Information Technologies in Latin America and the Caribbean, always striving to provide accurate and timely training materials, which have evolved accordingly to shifts in the technology focus. IoT and Machine Learning are poised to play a pivotal role, so the work of Professor Marcelo Rovai addressing Tiny Machine Learning is both timely and authoritative, in this rapidly changing field.\nTinyML Made Easy series is exactly what the title means. Written in a clear and concise style, with emphasis on practical applications and examples, drawing from many years of experience and overwhelming enthusiasm in sharing his knowledge, there is no doubt that Marcelo’s work is a very significant contribution to this very interesting field. The knowledge acquired from the exercises will enable the readers to undertake other projects that might interest them.\nErmanno Pietrosemoli, President Fundación Escuela Latinoamericana de Redes\nNovember 2023."
  },
  {
    "objectID": "Acknowledgements.html",
    "href": "Acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "We extend our deepest gratitude to the entire TinyML4D Academic Network, comprised of distinguished professors, researchers, and professionals. Notable contributions from Marco Zennaro, Brian Plancher, José Alberto Ferreira, Jesus Lopez, Diego Mendez, Shawn Hymel, Dan Situnayake, Pete Warden, and Laurence Moroney have been instrumental in advancing our understanding of Embedded Machine Learning (TinyML).\nSpecial commendation is reserved for Professor Vijay Janapa Reddi of Harvard University. His steadfast belief in the transformative potential of open-source communities, coupled with his invaluable guidance and teachings, has served as a beacon for our efforts from the very beginning.\nWe also thank Ermanno Petrosemoli for his meticulous review of the content in this material.\nAcknowledging these individuals, we pay tribute to the collective wisdom and dedication that have enriched this field and our work.\n\nIllustrative images on the e-book and chapter’s covers generated by OpenAI’s DALL-E via ChatGPT"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Microcontrollers (MCUs) are not just cheap electronic components, but they are the backbone of our modern world. With just a few kilobytes of RAM, they are designed to consume small amounts of power. Today, MCUs can be found embedded in all residential, medical, automotive, and industrial devices. Over 40 billion microcontrollers are estimated to be marketed annually, and hundreds of billions are currently in service. Yet, these devices often go unnoticed, as they are used to replace functionalities in older electromechanical systems in cars, washing machines, or remote controls. Understanding the potential of these microcontrollers is key to unlocking the future of technology.\nMore recently, with the era of IoT (Internet of Things), a significant part of these MCUs is generating “quintillions” of data, which, in their majority, are not used due to the high cost and complexity of their data transmission (bandwidth and latency).\nOn the other hand, in the last decades, we have witnessed the development of Machine Learning models (sub-area of Artificial Intelligence) trained with “tons” of data and powerful mainframes. But now, it suddenly becomes possible for “noisy” and complex signals, such as images, audio, or accelerometers, to extract meaning from the same through neural networks. More importantly, we can execute these models of neural networks in microcontrollers and sensors using very little energy and extract much more meaning from the data generated by these sensors, which we are currently ignoring.\n\nTinyML, a new area of applied AI, allows for extracting “machine intelligence” from the physical world (where the data is generated).\n\nTinyML Made Ease is a foundational text designed to facilitate the understanding and application of Embedded Machine Learning, or TinyML. In an era where embedded devices boast ultra-low power consumption—measured in mere milliwatts—and machine learning frameworks such as TensorFlow Lite for Microcontrollers (TF Lite Micro) become increasingly tailored for embedded applications, the intersection of AI and IoT is rapidly expanding. This book demystifies the integration of AI capabilities into these devices, making it accessible and empowering for all, paving the way for a wide-reaching adoption of AI-enabled IoT, or “AioT.”"
  },
  {
    "objectID": "about_book.html",
    "href": "about_book.html",
    "title": "About this Book",
    "section": "",
    "text": "This book is part of the open book Machine Learning Systems, which we invite you to read.\nTinyML Made Easy, an open-access eBook and part of the open book Machine Learning Systems, is an accessible resource for enthusiasts, professionals, and engineering students embarking on the transformative path of embedded machine learning (TinyML). This book offers a systematic introduction to integrating ML algorithms with microcontrollers, focusing on practicality and hands-on experiences.\nStudents are introduced to core concepts of TinyML through the setup and programming of the Seeed Studio XIAO ESP32-S3, a state-of-the-art microcontroller designed for ML applications. The book breaks down complex ideas into understandable segments, ensuring foundational knowledge is built from the ground up.\nTrue to the engineering ethos of ‘learning by doing,’ each chapter focuses on a project that challenges students to apply their knowledge in real-world scenarios. The projects are designed to reinforce learning and stimulate innovation, covering:\n\nMicrocontroller setup and sensor tests,\nImage classification,\nDataset labeling and Object detection,\nAudio processing and keyword spotting,\nTime-series data and Digital Signal Processing,\nMotion classification and Anomaly detection.\n\nTinyML Made Easy provides detailed walkthroughs of industry-standard tools such as Arduino IDE, Seeed SenseCraft, and Edge Impulse Studio. Students will gain hands-on experience with data collection, pre-processing, model training, and deployment, equipping them with the technical skills sought in the embedded systems industry.\nAs TinyML is poised to be a driving force in the evolution of IoT and smart devices, students will emerge from this book with an introductory theoretical understanding and the practical skills necessary to contribute to and shape the future of technology.\nWith its straightforward language, clarity, and focus on experiential learning, TinyML Made Easy is more than just a book—it’s a comprehensive toolkit for anyone ready to delve into the world of embedded machine learning. It empowers them to move beyond the classroom and into the lab or field, and they are well-prepared to tackle the challenges and opportunities TinyML presents.\n\nSupplementary Online Resources\nChapter 1 - Setup XIAO ESP32S3 Sense\n\nArduino Codes\n\nChapter 2 - Image Classification\n\nArduino Codes\nDataset\nEdge Impulse Project\n\nChapter 3 - Object Detection\n\nEdge Impulse Project\n\nChapter 4 - Audio Feature Engineering\n\nAudio_Data_Analysis Colab Notebook\n\nChapter 5 - Keyword Spotting (KWS)\n\nArduino Codes\nSubset of Google Speech Commands Dataset\nKWS_MFCC_Analysis Colab Notebook\nKWS_CNN_training Colab Notebook\nArduino post-processing code\nEdge Impulse Project\n\nChapter 6 - DSP Spectral Features\n\nDSP - Spectral Features Colab Notebook\n\nChapter 7 - Motion Classification and Anomaly Detection\n\nArduino Codes\nEdge_Impulse_Spectral_Features_Block Colab Notebook\nEdge Impulse Project"
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#introduction",
    "href": "content/xiao_esp32s3_setup.html#introduction",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThe XIAO ESP32S3 Sense is Seeed Studio’s affordable development board, which integrates a camera sensor, digital microphone, and SD card support. Combining embedded ML computing power and photography capability, this development board is a great tool to start with TinyML (intelligent voice and vision AI).\n\n\n\n\n\nXIAO ESP32S3 Sense Main Features\n\nPowerful MCU Board: Incorporate the ESP32S3 32-bit, dual-core, Xtensa processor chip operating up to 240 MHz, mounted multiple development ports, Arduino / MicroPython supported\nAdvanced Functionality: Detachable OV2640 camera sensor for 1600 * 1200 resolution, compatible with OV5640 camera sensor, integrating an additional digital microphone\nElaborate Power Design: Lithium battery charge management capability offers four power consumption models, which allows for deep sleep mode with power consumption as low as 14μA\nGreat Memory for more Possibilities: Offer 8MB PSRAM and 8MB FLASH, supporting SD card slot for external 32GB FAT memory\nOutstanding RF performance: Support 2.4GHz Wi-Fi and BLE dual wireless communication, support 100m+ remote communication when connected with U.FL antenna\nThumb-sized Compact Design: 21 x 17.5mm, adopting the classic form factor of XIAO, suitable for space-limited projects like wearable devices\n\n\n\n\n\n\nBelow is the general board pinout:\n\n\n\n\n\n\nFor more details, please refer to the Seeed Studio WiKi page:  https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/"
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "href": "content/xiao_esp32s3_setup.html#installing-the-xiao-esp32s3-sense-on-arduino-ide",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.2 Installing the XIAO ESP32S3 Sense on Arduino IDE",
    "text": "1.2 Installing the XIAO ESP32S3 Sense on Arduino IDE\nOn Arduino IDE, navigate to File &gt; Preferences, and fill in the URL:\nhttps://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_dev_index.json\non the field ==&gt; Additional Boards Manager URLs\n\nNext, open boards manager. Go to Tools &gt; Board &gt; Boards Manager… and enter with esp32. Select and install the most updated and stable package (avoid alpha versions) :\n\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nOn Tools, select the Board (XIAO ESP32S3):\n\nLast but not least, choose the Port where the ESP32S3 is connected.\nThat is it! The device should be OK. Let’s do some tests."
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#testing-the-board-with-blink",
    "href": "content/xiao_esp32s3_setup.html#testing-the-board-with-blink",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.3 Testing the board with BLINK",
    "text": "1.3 Testing the board with BLINK\nThe XIAO ESP32S3 Sense has a built-in LED that is connected to GPIO21. So, you can run the blink sketch as it is (using the LED_BUILTIN Arduino constant) or by changing the Blink sketch accordingly:\n#define LED_BUILT_IN 21 \n\nvoid setup() {\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n}\n\n// Remember that the pin work with inverted logic\n// LOW to Turn on and HIGH to turn off\nvoid loop() {\n  digitalWrite(LED_BUILT_IN, LOW); //Turn on\n  delay (1000); //Wait 1 sec\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n  delay (1000); //Wait 1 sec\n}\n\nNote that the pins work with inverted logic: LOW to Turn on and HIGH to turn off."
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#connecting-sense-module-expansion-board",
    "href": "content/xiao_esp32s3_setup.html#connecting-sense-module-expansion-board",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.4 Connecting Sense module (Expansion Board)",
    "text": "1.4 Connecting Sense module (Expansion Board)\nWhen purchased, the expansion board is separated from the main board, but installing the expansion board is very simple. You need to align the connector on the expansion board with the B2B connector on the XIAO ESP32S3, press it hard, and when you hear a “click,” the installation is complete.\nAs commented in the introduction, the expansion board, or the “sense” part of the device, has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone."
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#microphone-test",
    "href": "content/xiao_esp32s3_setup.html#microphone-test",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.5 Microphone Test",
    "text": "1.5 Microphone Test\nLet’s start with sound detection. Go to the GitHub project and download the sketch: XIAOEsp2s3_Mic_Test and run it on the Arduino IDE:\n\nWhen producing sound, you can verify it on the Serial Plotter.\nSave recorded sound (.wav audio files) to a microSD card.\nNow, the onboard SD Card reader can save .wav audio files. To do that, we need to habilitate the XIAO PSRAM.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. This can be insufficient for some purposes, so up to 16 MB of external PSRAM (pseudo-static RAM) can be connected with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\nDownload the sketch Wav_Record, which you can find on GitHub.\nTo execute the code (Wav Record), it is necessary to use the PSRAM function of the ESP-32 chip, so turn it on before uploading.: Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nRun the code Wav_Record.ino\nThis program is executed only once after the user **turns on the serial monitor. It records for 20 seconds and saves the recording file to a microSD card as “arduino_rec.wav.”\nWhen the “.” is output every 1 second in the serial monitor, the program execution is finished, and you can play the recorded sound file with the help of a card reader.\n\n\nThe sound quality is excellent!\n\nThe explanation of how the code works is beyond the scope of this tutorial, but you can find an excellent description on the wiki page."
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#testing-the-camera",
    "href": "content/xiao_esp32s3_setup.html#testing-the-camera",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.6 Testing the Camera",
    "text": "1.6 Testing the Camera\nTo test the camera, you should download the folder take_photos_command from GitHub. The folder contains the sketch (.ino) and two .h files with camera details.\n\nRun the code: take_photos_command.ino. Open the Serial Monitor and send the command capture to capture and save the image on the SD Card:\n\n\nVerify that [Both NL & CR] are selected on Serial Monitor.\n\n\nHere is an example of a taken photo:"
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#testing-wifi",
    "href": "content/xiao_esp32s3_setup.html#testing-wifi",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.7 Testing WiFi",
    "text": "1.7 Testing WiFi\nOne of the XIAO ESP32S3’s differentiators is its WiFi capability. So, let’s test its radio by scanning the Wi-Fi networks around it. You can do this by running one of the code examples on the board.\nGo to Arduino IDE Examples and look for WiFI ==&gt; WiFIScan\nYou should see the Wi-Fi networks (SSIDs and RSSIs) within your device’s range on the serial monitor. Here is what I got in the lab:\n\nSimple WiFi Server (Turning LED ON/OFF)\nLet’s test the device’s capability to behave as a WiFi Server. We will host a simple page on the device that sends commands to turn the XIAO built-in LED ON and OFF.\nLike before, go to GitHub to download the folder using the sketch SimpleWiFiServer.\nBefore running the sketch, you should enter your network credentials:\nconst char* ssid     = \"Your credentials here\";\nconst char* password = \"Your credentials here\";\nYou can monitor how your server is working with the Serial Monitor.\n\nTake the IP address and enter it on your browser:\n\nYou will see a page with links that can turn the built-in LED of your XIAO ON and OFF.\nStreaming video to Web\nNow that you know that you can send commands from the webpage to your device, let’s do the reverse. Let’s take the image captured by the camera and stream it to a webpage:\nDownload from GitHub the folder that contains the code: XIAO-ESP32S3-Streeming_Video.ino.\n\nRemember that the folder contains the.ino file and a couple of .h files necessary to handle the camera.\n\nEnter your credentials and run the sketch. On the Serial monitor, you can find the page address to enter in your browser:\n\nOpen the page on your browser (wait a few seconds to start the streaming). That’s it.\n\nStreamlining what your camera is “seen” can be important when you position it to capture a dataset for an ML project (for example, using the code “take_phots_commands.ino”.\nOf course, we can do both things simultaneously: show what the camera sees on the page and send a command to capture and save the image on the SD card. For that, you can use the code Camera_HTTP_Server_STA, which can be downloaded from GitHub.\n\nThe program will do the following tasks:\n\nSet the camera to JPEG output mode.\nCreate a web page (for example ==&gt; http://192.168.4.119//). The correct address will be displayed on the Serial Monitor.\nIf server.on (“/capture”, HTTP_GET, serverCapture), the program takes a photo and sends it to the Web.\nIt is possible to rotate the image on webPage using the button [ROTATE]\nThe command [CAPTURE] only will preview the image on the webpage, showing its size on the Serial Monitor\nThe [SAVE] command will save an image on the SD Card and show the image on the browser.\nSaved images will follow a sequential naming (image1.jpg, image2.jpg.\n\n\n\nThis program can capture an image dataset with an image classification project.\n\nInspect the code; it will be easier to understand how the camera works. This code was developed based on the great Rui Santos Tutorial ESP32-CAM Take Photo and Display in Web Server, which I invite all of you to visit.\nUsing the CameraWebServer\nIn the Arduino IDE, go to File &gt; Examples &gt; ESP32 &gt; Camera, and select CameraWebServer\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nDo not forget the Tools to enable the PSRAM.\nEnter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. Using the [Save] button, you can save an image to your computer download area.\n\nThat’s it! You can save the images directly on your computer for use on projects."
  },
  {
    "objectID": "content/xiao_esp32s3_setup.html#conclusion",
    "href": "content/xiao_esp32s3_setup.html#conclusion",
    "title": "1  XIAO ESP32S3 Sense Setup",
    "section": "1.8 Conclusion",
    "text": "1.8 Conclusion\nThe XIAO ESP32S3 Sense is flexible, inexpensive, and easy to program. With 8 MB of RAM, memory is not an issue, and the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codes on the GitHub repository: XIAO-ESP32S3-Sense."
  },
  {
    "objectID": "content/xiao_image_classification.html#introduction",
    "href": "content/xiao_image_classification.html#introduction",
    "title": "2  Image Classification",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nMore and more, we are facing an artificial intelligence (AI) revolution where, as stated by Gartner, Edge AI has a very high impact potential, and it is for now!\n\nAt the forefront of the Emerging Technologies Radar is the universal language of Edge Computer Vision. When we delve into Machine Learning (ML) applied to vision, the first concept that greets us is Image Classification, a kind of ML’ Hello World ’ that is both simple and profound!\nThe Seeed Studio XIAO ESP32S3 Sense is a powerful tool that combines camera and SD card support. With its embedded ML computing power and photography capability, it is an excellent starting point for exploring TinyML vision AI."
  },
  {
    "objectID": "content/xiao_image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "href": "content/xiao_image_classification.html#a-tinyml-image-classification-project---fruits-versus-veggies",
    "title": "2  Image Classification",
    "section": "2.2 A TinyML Image Classification Project - Fruits versus Veggies",
    "text": "2.2 A TinyML Image Classification Project - Fruits versus Veggies\n\nThe whole idea of our project will be to train a model and proceed with inference on the XIAO ESP32S3 Sense. For training, we should find some data (in fact, tons of data!).\nBut first of all, we need a goal! What do we want to classify?\nWith TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We will differentiate apples from bananas and potatoes (you can try other categories).\nSo, let’s find a specific dataset that includes images from those categories. Kaggle is a good start:\nhttps://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition\nThis dataset contains images of the following food items:\n\nFruits - banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango.\nVegetables - cucumber, carrot, capsicum, onion, potato, lemon, tomato, radish, beetroot, cabbage, lettuce, spinach, soybean, cauliflower, bell pepper, chili pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant.\n\nEach category is split into the train (100 images), test (10 images), and validation (10 images).\n\nDownload the dataset from the Kaggle website and put it on your computer.\n\n\nOptionally, you can add some fresh photos of bananas, apples, and potatoes from your home kitchen, using, for example, the codes discussed in the setup lab."
  },
  {
    "objectID": "content/xiao_image_classification.html#training-the-model-with-edge-impulse-studio",
    "href": "content/xiao_image_classification.html#training-the-model-with-edge-impulse-studio",
    "title": "2  Image Classification",
    "section": "2.3 Training the model with Edge Impulse Studio",
    "text": "2.3 Training the model with Edge Impulse Studio\nWe will use the Edge Impulse Studio to train our model. As you may know, Edge Impulse is a leading development platform for machine learning on edge devices.\nEnter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:\n\n\n2.3.1 Data Acquisition\nNext, on the UPLOAD DATA section, upload from your computer the files from chosen categories:\n\nIt would be best if you now had your training dataset split into three classes of data:\n\n\nYou can upload extra data for further model testing or split the training data. I will leave it as it is to use the most data possible.\n\n\n\n2.3.2 Impulse Design\n\nAn impulse takes raw data (in this case, images), extracts features (resize pictures), and then uses a learning block to classify new data.\n\nClassifying images is the most common use of deep learning, but a lot of data should be used to accomplish this task. We have around 90 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach or model” to differentiate an apple from a banana. But, we can solve this issue by re-training a previously trained model with thousands of images. We call this technique “Transfer Learning” (TL).\n\nWith TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).\nSo, starting from the raw images, we will resize them (96x96) pixels and feed them to our Transfer Learning block:\n\n\n2.3.2.1 Pre-processing (Feature Generation)\nBesides resizing the images, we can change them to Grayscale or keep the actual RGB color depth. Let’s start selecting Grayscale. Doing that, each one of our data samples will have dimension 9, 216 features (96x96x1). Keeping RGB, this dimension would be three times bigger. Working with Grayscale helps to reduce the amount of final memory needed for inference.\n\nRemember to [Save parameters]. This will generate the features to be used in training.\n\n\n2.3.2.2 Model Design\nTransfer Learning\nIn 2007, Google introduced MobileNetV1, a family of general-purpose computer vision neural networks designed with mobile devices in mind to support classification, detection, and more. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.\nAlthough the base MobileNet architecture is already tiny and has low latency, many times, a specific use case or application may require the model to be smaller and faster. MobileNet introduces a straightforward parameter α (alpha) called width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.\nEdge Impulse Studio has MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images) available, with several different α values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency.\nThe smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).\nFor this first pass, we will use MobileNet V1 and α=0.10.\n\n\n\n2.3.3 Training\nData Augmentation\nAnother necessary technique to use with deep learning is data augmentation. Data augmentation is a method that can help improve the accuracy of machine learning models, creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).\nUnder the rood, here you can see how Edge Impulse implements a data Augmentation policy on your data:\n# Implements the data augmentation policy\ndef augment_image(image, label):\n    # Flips the image randomly\n    image = tf.image.random_flip_left_right(image)\n\n    # Increase the image size, then randomly crop it down to\n    # the original dimensions\n    resize_factor = random.uniform(1, 1.2)\n    new_height = math.floor(resize_factor * INPUT_SHAPE[0])\n    new_width = math.floor(resize_factor * INPUT_SHAPE[1])\n    image = tf.image.resize_with_crop_or_pad(image, new_height, new_width)\n    image = tf.image.random_crop(image, size=INPUT_SHAPE)\n\n    # Vary the brightness of the image\n    image = tf.image.random_brightness(image, max_delta=0.2)\n\n    return image, label\nExposure to these variations during training can help prevent your model from taking shortcuts by “memorizing” superficial clues in your training data, meaning it may better reflect the deep underlying patterns in your dataset.\nThe final layer of our model will have 16 neurons with a 10% dropout for overfitting prevention. Here is the Training output:\n\nThe result could be better. The model reached around 77% accuracy, but the amount of RAM expected to be used during the inference is relatively tiny (about 60 KBytes), which is very good.\n\n\n2.3.4 Deployment\nThe trained model will be deployed as a .zip Arduino library:\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Please select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code under your project name.\n\nOpen the Static Buffer example:\n\nYou can see that the first line of code is exactly the calling of a library with all the necessary stuff for running inference on your device.\n#include &lt;XIAO-ESP32S3-CAM-Fruits-vs-Veggies_inferencing.h&gt;\nOf course, this is a generic code (a “template”) that only gets one sample of raw data (stored on the variable: features = {} and runs the classifier, doing the inference. The result is shown on the Serial Monitor.\nWe should get the sample (image) from the camera and pre-process it (resizing to 96x96, converting to grayscale, and flatting it). This will be the input tensor of our model. The output tensor will be a vector with three values (labels), showing the probabilities of each one of the classes.\n\nReturning to your project (Tab Image), copy one of the Raw Data Sample:\n\n9, 216 features will be copied to the clipboard. This is the input tensor (a flattened image of 96x96x1), in this case, bananas. Past this Input tensor onfeatures[] = {0xb2d77b, 0xb5d687, 0xd8e8c0, 0xeaecba, 0xc2cf67, ...}\n\nEdge Impulse included the library ESP NN in its SDK, which contains optimized NN (Neural Network) functions for various Espressif chips, including the ESP32S3 (running at Arduino IDE).\nWhen running the inference, you should get the highest score for “banana.”\n\nGreat news! Our device handles an inference, discovering that the input image is a banana. Also, note that the inference time was around 317ms, resulting in a maximum of 3 fps if you tried to classify images from a video.\nNow, we should incorporate the camera and classify images in real time.\nGo to the Arduino IDE Examples and download from your project the sketch esp32_camera:\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nThe modified sketch can be downloaded from GitHub: xiao_esp32s3_camera.\n\nNote that you can optionally keep the pins as a .h file as we did in the Setup Lab.\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start classifying your fruits and vegetables! You can check the result on Serial Monitor."
  },
  {
    "objectID": "content/xiao_image_classification.html#testing-the-model-inference",
    "href": "content/xiao_image_classification.html#testing-the-model-inference",
    "title": "2  Image Classification",
    "section": "2.4 Testing the Model (Inference)",
    "text": "2.4 Testing the Model (Inference)\n\n\n\n\n\nGetting a photo with the camera, the classification result will appear on the Serial Monitor:\n\nOther tests:"
  },
  {
    "objectID": "content/xiao_image_classification.html#testing-with-a-bigger-model",
    "href": "content/xiao_image_classification.html#testing-with-a-bigger-model",
    "title": "2  Image Classification",
    "section": "2.5 Testing with a Bigger Model",
    "text": "2.5 Testing with a Bigger Model\nNow, let’s go to the other side of the model size. Let’s select a MobilinetV2 96x96 0.35, having as input RGB images.\n\nEven with a bigger model, the accuracy could be better, and the amount of memory necessary to run the model increases five times, with latency increasing seven times.\n\nNote that the performance here is estimated with a smaller device, the ESP-EYE. The actual inference with the ESP32S3 should be better.\n\nTo improve our model, we will need to train more images.\nEven though our model did not improve accuracy, let’s test whether the XIAO can handle such a bigger model. We will do a simple inference test with the Static Buffer sketch.\nLet’s redeploy the model. If the EON Compiler is enabled when you generate the library, the total memory needed for inference should be reduced, but it does not influence accuracy.\n\n⚠️ Attention - The Xiao ESP32S3 with PSRAM enable has enought memory to run the inference, even in such bigger model. Keep the EON Compiler NOT ENABLED.\n\n\nDoing an inference with MobilinetV2 96x96 0.35, having as input RGB images, the latency was 219ms, which is great for such a bigger model.\n\nFor the test, we can train the model again, using the smallest version of MobileNet V2, with an alpha of 0.05. Interesting that the result in accuracy was higher.\n\n\nNote that the estimated latency for an Arduino Portenta (ou Nicla), running with a clock of 480MHz is 45ms.\n\nDeploying the model, we got an inference of only 135ms, remembering that the XIAO runs with half of the clock used by the Portenta/Nicla (240MHz):"
  },
  {
    "objectID": "content/xiao_image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "href": "content/xiao_image_classification.html#running-inference-on-the-sensecraft-web-toolkit",
    "title": "2  Image Classification",
    "section": "2.6 Running inference on the SenseCraft-Web-Toolkit",
    "text": "2.6 Running inference on the SenseCraft-Web-Toolkit\nOne significant limitation of viewing inference on Arduino IDE is that we can not see what the camera focuses on. A good alternative is the SenseCraft-Web-Toolkit, a visual model deployment tool provided by SSCMA(Seeed SenseCraft Model Assistant). This tool allows you to deploy models to various platforms easily through simple operations. The tool offers a user-friendly interface and does not require any coding.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn the Dashboard, download the model (“block output”): Transfer learning model - TensorFlow Lite (int8 quantized).\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: apple, banana, potato).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe Classification result will be at the top of the image. You can also select the Confidence of your inference cursor Confidence.\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, the same that we have done with the Arduino IDE:\n\nOn Device Log, you will get information as:\n\n\nPreprocess time (image capture and Crop): 4ms;\nInference time (model latency): 106ms,\nPostprocess time (display of the image and inclusion of data): 0ms.\nOutput tensor (classes), for example: [[89,0]]; where 0 is Apple (and 1is banana and 2 is potato)\n\nHere are other screenshots:"
  },
  {
    "objectID": "content/xiao_image_classification.html#conclusion",
    "href": "content/xiao_image_classification.html#conclusion",
    "title": "2  Image Classification",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\nThe XIAO ESP32S3 Sense is very flexible, inexpensive, and easy to program. The project proves the potential of TinyML. Memory is not an issue; the device can handle many post-processing tasks, including communication.\nYou will find the last version of the codes on the GitHub repository: XIAO-ESP32S3-Sense."
  },
  {
    "objectID": "content/xiao_object_detection.html#introduction",
    "href": "content/xiao_object_detection.html#introduction",
    "title": "3  Object Detection",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn the last section regarding Computer Vision (CV) and the XIAO ESP32S3, Image Classification, we learned how to set up and classify images with this remarkable development board. Continuing our CV journey, we will explore Object Detection on microcontrollers.\n\n3.1.1 Object Detection versus Image Classification\nThe main task with Image Classification models is to identify the most probable object category present on an image, for example, to classify between a cat or a dog, dominant “objects” in an image:\n\nBut what happens if there is no dominant category in the image?\n\nAn image classification model identifies the above image utterly wrong as an “ashcan,” possibly due to the color tonalities.\n\nThe model used in the previous images is MobileNet, which is trained with a large dataset, ImageNet, running on a Raspberry Pi.\n\nTo solve this issue, we need another type of model, where not only multiple categories (or labels) can be found but also where the objects are located on a given image.\nAs we can imagine, such models are much more complicated and bigger, for example, the MobileNetV2 SSD FPN-Lite 320x320, trained with the COCO dataset. This pre-trained object detection model is designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The below image is the result of such a model running on a Raspberry Pi:\n\nThose models used for object detection (such as the MobileNet SSD or YOLO) usually have several MB in size, which is OK for use with Raspberry Pi but unsuitable for use with embedded devices, where the RAM usually has, at most, a few MB as in the case of the XIAO ESP32S3.\n\n\n3.1.2 An Innovative Solution for Object Detection: FOMO\nEdge Impulse launched in 2022, FOMO (Faster Objects, More Objects), a novel solution to perform object detection on embedded devices, such as the Nicla Vision and Portenta (Cortex M7), on Cortex M4F CPUs (Arduino Nano33 and OpenMV M4 series) as well the Espressif ESP32 devices (ESP-CAM, ESP-EYE and XIAO ESP32S3 Sense).\nIn this Hands-On project, we will explore Object Detection using FOMO.\n\nTo understand more about FOMO, you can go into the official FOMO announcement by Edge Impulse, where Louis Moreau and Mat Kelcey explain in detail how it works."
  },
  {
    "objectID": "content/xiao_object_detection.html#the-object-detection-project-goal",
    "href": "content/xiao_object_detection.html#the-object-detection-project-goal",
    "title": "3  Object Detection",
    "section": "3.2 The Object Detection Project Goal",
    "text": "3.2 The Object Detection Project Goal\nAll Machine Learning projects need to start with a detailed goal. Let’s assume we are in an industrial or rural facility and must sort and count oranges (fruits) and particular frogs (bugs).\n\nIn other words, we should perform a multi-label classification, where each image can have three classes:\n\nBackground (No objects)\nFruit\nBug\n\nHere are some not labeled image samples that we should use to detect the objects (fruits and bugs):\n\nWe are interested in which object is in the image, its location (centroid), and how many we can find on it. The object’s size is not detected with FOMO, as with MobileNet SSD or YOLO, where the Bounding Box is one of the model outputs.\nWe will develop the project using the XIAO ESP32S3 for image capture and model inference. The ML project will be developed using the Edge Impulse Studio. But before starting the object detection project in the Studio, let’s create a raw dataset (not labeled) with images that contain the objects to be detected."
  },
  {
    "objectID": "content/xiao_object_detection.html#data-collection",
    "href": "content/xiao_object_detection.html#data-collection",
    "title": "3  Object Detection",
    "section": "3.3 Data Collection",
    "text": "3.3 Data Collection\nYou can capture images using the XIAO, your phone, or other devices. Here, we will use the XIAO with code from the Arduino IDE ESP32 library.\n\n3.3.1 Collecting Dataset with the XIAO ESP32S3\nOpen the Arduino IDE and select the XIAO_ESP32S3 board (and the port where it is connected). On File &gt; Examples &gt; ESP32 &gt; Camera, select CameraWebServer.\nOn the BOARDS MANAGER panel, confirm that you have installed the latest “stable” package.\n\n⚠️ Attention\nAlpha versions (for example, 3.x-alpha) do not work correctly with the XIAO and Edge Impulse. Use the last stable version (for example, 2.0.11) instead.\n\nYou also should comment on all cameras’ models, except the XIAO model pins:\n#define CAMERA_MODEL_XIAO_ESP32S3 // Has PSRAM\nAnd on Tools, enable the PSRAM. Enter your wifi credentials and upload the code to the device:\n\nIf the code is executed correctly, you should see the address on the Serial Monitor:\n\nCopy the address on your browser and wait for the page to be uploaded. Select the camera resolution (for example, QVGA) and select [START STREAM]. Wait for a few seconds/minutes, depending on your connection. You can save an image on your computer download area using the [Save] button.\n\nEdge impulse suggests that the objects should be similar in size and not overlapping for better performance. This is OK in an industrial facility, where the camera should be fixed, keeping the same distance from the objects to be detected. Despite that, we will also try using mixed sizes and positions to see the result.\n\nWe do not need to create separate folders for our images because each contains multiple labels.\n\nWe suggest using around 50 images to mix the objects and vary the number of each appearing on the scene. Try to capture different angles, backgrounds, and light conditions.\n\nThe stored images use a QVGA frame size of 320x240 and RGB565 (color pixel format).\n\nAfter capturing your dataset, [Stop Stream] and move your images to a folder."
  },
  {
    "objectID": "content/xiao_object_detection.html#edge-impulse-studio",
    "href": "content/xiao_object_detection.html#edge-impulse-studio",
    "title": "3  Object Detection",
    "section": "3.4 Edge Impulse Studio",
    "text": "3.4 Edge Impulse Studio\n\n3.4.1 Setup the project\nGo to Edge Impulse Studio, enter your credentials at Login (or create an account), and start a new project.\n\n\nHere, you can clone the project developed for this hands-on: XIAO-ESP32S3-Sense-Object_Detection\n\nOn your Project Dashboard, go down and on Project info and select Bounding boxes (object detection) and Espressif ESP-EYE (most similar to our board) as your Target Device:\n\n\n\n3.4.2 Uploading the unlabeled data\nOn Studio, go to the Data acquisition tab, and on the UPLOAD DATA section, upload files captured as a folder from your computer.\n\n\nYou can leave for the Studio to split your data automatically between Train and Test or do it manually. We will upload all of them as training.\n\n\nAll the not-labeled images (47) were uploaded but must be labeled appropriately before being used as a project dataset. The Studio has a tool for that purpose, which you can find in the link Labeling queue (47).\nThere are two ways you can use to perform AI-assisted labeling on the Edge Impulse Studio (free version):\n\nUsing yolov5\nTracking objects between frames\n\n\nEdge Impulse launched an auto-labeling feature for Enterprise customers, easing labeling tasks in object detection projects.\n\nOrdinary objects can quickly be identified and labeled using an existing library of pre-trained object detection models from YOLOv5 (trained with the COCO dataset). But since, in our case, the objects are not part of COCO datasets, we should select the option of tracking objects. With this option, once you draw bounding boxes and label the images in one frame, the objects will be tracked automatically from frame to frame, partially labeling the new ones (not all are correctly labeled).\n\nYou can use the EI uploader to import your data if you already have a labeled dataset containing bounding boxes.\n\n\n\n3.4.3 Labeling the Dataset\nStarting with the first image of your unlabeled data, use your mouse to drag a box around an object to add a label. Then click Save labels to advance to the next item.\n\nContinue with this process until the queue is empty. At the end, all images should have the objects labeled as those samples below:\n\nNext, review the labeled samples on the Data acquisition tab. If one of the labels is wrong, you can edit it using the three dots menu after the sample name:\n\nYou will be guided to replace the wrong label and correct the dataset.\n\n\n\n3.4.4 Balancing the dataset and split Train/Test\nAfter labeling all data, it was realized that the class fruit had many more samples than the bug. So, 11 new and additional bug images were collected (ending with 58 images). After labeling them, it is time to select some images and move them to the test dataset. You can do it using the three-dot menu after the image name. I selected six images, representing 13% of the total dataset."
  },
  {
    "objectID": "content/xiao_object_detection.html#the-impulse-design",
    "href": "content/xiao_object_detection.html#the-impulse-design",
    "title": "3  Object Detection",
    "section": "3.5 The Impulse Design",
    "text": "3.5 The Impulse Design\nIn this phase, you should define how to:\n\nPre-processing consists of resizing the individual images from 320 x 240 to 96 x 96 and squashing them (squared form, without cropping). Afterward, the images are converted from RGB to Grayscale.\nDesign a Model, in this case, “Object Detection.”\n\n\n\n3.5.1 Preprocessing all dataset\nIn this section, select Color depth as Grayscale, suitable for use with FOMO models and Save parameters.\n\nThe Studio moves automatically to the next section, Generate features, where all samples will be pre-processed, resulting in a dataset with individual 96x96x1 images or 9,216 features.\n\nThe feature explorer shows that all samples evidence a good separation after the feature generation.\n\nSome samples seem to be in the wrong space, but clicking on them confirms the correct labeling."
  },
  {
    "objectID": "content/xiao_object_detection.html#model-design-training-and-test",
    "href": "content/xiao_object_detection.html#model-design-training-and-test",
    "title": "3  Object Detection",
    "section": "3.6 Model Design, Training, and Test",
    "text": "3.6 Model Design, Training, and Test\nWe will use FOMO, an object detection model based on MobileNetV2 (alpha 0.35) designed to coarsely segment an image into a grid of background vs objects of interest (here, boxes and wheels).\nFOMO is an innovative machine learning model for object detection, which can use up to 30 times less energy and memory than traditional models like Mobilenet SSD and YOLOv5. FOMO can operate on microcontrollers with less than 200 KB of RAM. The main reason this is possible is that while other models calculate the object’s size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.\nHow FOMO works?\nFOMO takes the image in grayscale and divides it into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as background). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.\n\nFor training, we should select a pre-trained model. Let’s use the FOMO (Faster Objects, More Objects) MobileNetV2 0.35. This model uses around 250KB of RAM and 80KB of ROM (Flash), which suits well with our board.\n\nRegarding the training hyper-parameters, the model will be trained with:\n\nEpochs: 60\nBatch size: 32\nLearning Rate: 0.001.\n\nFor validation during training, 20% of the dataset (validation_dataset) will be spared. For the remaining 80% (train_dataset), we will apply Data Augmentation, which will randomly flip, change the size and brightness of the image, and crop them, artificially increasing the number of samples on the dataset for training.\nAs a result, the model ends with an overall F1 score of 85%, similar to the result when using the test data (83%).\n\nNote that FOMO automatically added a 3rd label background to the two previously defined (box and wheel).\n\n\n\nIn object detection tasks, accuracy is generally not the primary evaluation metric. Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing. Because of that, we will use the F1 score.\n\n\n3.6.1 Test model with “Live Classification”\nOnce our model is trained, we can test it using the Live Classification tool. On the correspondent section, click on Connect a development board icon (a small MCU) and scan the QR code with your phone.\n\nOnce connected, you can use the smartphone to capture actual images to be tested by the trained model on Edge Impulse Studio.\n\nOne thing to be noted is that the model can produce false positives and negatives. This can be minimized by defining a proper Confidence Threshold (use the Three dots menu for the setup). Try with 0.8 or more."
  },
  {
    "objectID": "content/xiao_object_detection.html#deploying-the-model-arduino-ide",
    "href": "content/xiao_object_detection.html#deploying-the-model-arduino-ide",
    "title": "3  Object Detection",
    "section": "3.7 Deploying the Model (Arduino IDE)",
    "text": "3.7 Deploying the Model (Arduino IDE)\nSelect the Arduino Library and Quantized (int8) model, enable the EON Compiler on the Deploy Tab, and press [Build].\n\nOpen your Arduino IDE, and under Sketch, go to Include Library and add.ZIP Library. Select the file you download from Edge Impulse Studio, and that’s it!\n\nUnder the Examples tab on Arduino IDE, you should find a sketch code (esp32 &gt; esp32_camera) under your project name.\n\nYou should change lines 32 to 75, which define the camera model and pins, using the data related to our model. Copy and paste the below lines, replacing the lines 32-75:\n#define PWDN_GPIO_NUM     -1 \n#define RESET_GPIO_NUM    -1 \n#define XCLK_GPIO_NUM     10 \n#define SIOD_GPIO_NUM     40 \n#define SIOC_GPIO_NUM     39\n#define Y9_GPIO_NUM       48 \n#define Y8_GPIO_NUM       11 \n#define Y7_GPIO_NUM       12 \n#define Y6_GPIO_NUM       14 \n#define Y5_GPIO_NUM       16 \n#define Y4_GPIO_NUM       18 \n#define Y3_GPIO_NUM       17 \n#define Y2_GPIO_NUM       15 \n#define VSYNC_GPIO_NUM    38 \n#define HREF_GPIO_NUM     47 \n#define PCLK_GPIO_NUM     13\nHere you can see the resulting code:\n\nUpload the code to your XIAO ESP32S3 Sense, and you should be OK to start detecting fruits and bugs. You can check the result on Serial Monitor.\nBackground\n\nFruits\n\nBugs\n\nNote that the model latency is 143ms, and the frame rate per second is around 7 fps (similar to what we got with the Image Classification project). This happens because FOMO is cleverly built over a CNN model, not with an object detection model like the SSD MobileNet. For example, when running a MobileNetV2 SSD FPN-Lite 320x320 model on a Raspberry Pi 4, the latency is around five times higher (around 1.5 fps)."
  },
  {
    "objectID": "content/xiao_object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "href": "content/xiao_object_detection.html#deploying-the-model-sensecraft-web-toolkit",
    "title": "3  Object Detection",
    "section": "3.8 Deploying the Model (SenseCraft-Web-Toolkit)",
    "text": "3.8 Deploying the Model (SenseCraft-Web-Toolkit)\nAs discussed in the Image Classification chapter, verifying inference with Image models on Arduino IDE is very challenging because we can not see what the camera focuses on. Again, let’s use the SenseCraft-Web Toolkit.\nFollow the following steps to start the SenseCraft-Web-Toolkit:\n\nOpen the SenseCraft-Web-Toolkit website.\nConnect the XIAO to your computer:\n\n\nHaving the XIAO connected, select it as below:\n\n\n\nSelect the device/Port and press [Connect]:\n\n\n\nYou can try several Computer Vision models previously uploaded by Seeed Studio. Try them and have fun!\n\nIn our case, we will use the blue button at the bottom of the page: [Upload Custom AI Model].\nBut first, we must download from Edge Impulse Studio our quantized .tflite model.\n\nGo to your project at Edge Impulse Studio, or clone this one:\n\n\nXIAO-ESP32S3-CAM-Fruits-vs-Veggies-v1-ESP-NN\n\n\nOn Dashboard, download the model (“block output”): Object Detection model - TensorFlow Lite (int8 quantized)\n\n\n\nOn SenseCraft-Web-Toolkit, use the blue button at the bottom of the page: [Upload Custom AI Model]. A window will pop up. Enter the Model file that you downloaded to your computer from Edge Impulse Studio, choose a Model Name, and enter with labels (ID: Object):\n\n\n\nNote that you should use the labels trained on EI Studio, entering them in alphabetic order (in our case: background, bug, fruit).\n\nAfter a few seconds (or minutes), the model will be uploaded to your device, and the camera image will appear in real-time on the Preview Sector:\n\nThe detected objects will be marked (the centroid). You can select the Confidence of your inference cursor Confidence. and IoU, which is used to assess the accuracy of predicted bounding boxes compared to truth bounding boxes\nClicking on the top button (Device Log), you can open a Serial Monitor to follow the inference, as we did with the Arduino IDE.\n\nOn Device Log, you will get information as:\n\nPreprocess time (image capture and Crop): 3 ms;\nInference time (model latency): 115 ms,\nPostprocess time (display of the image and marking objects): 1 ms.\nOutput tensor (boxes), for example, one of the boxes: [[30,150, 20, 20,97, 2]]; where 30,150, 20, 20 are the coordinates of the box (around the centroid); 97 is the inference result, and 2 is the class (in this case 2: fruit)\n\n\nNote that in the above example, we got 5 boxes because none of the fruits got 3 centroids. One solution will be post-processing, where we can aggregate close centroids in one.\n\nHere are other screen shots:"
  },
  {
    "objectID": "content/xiao_object_detection.html#conclusion",
    "href": "content/xiao_object_detection.html#conclusion",
    "title": "3  Object Detection",
    "section": "3.9 Conclusion",
    "text": "3.9 Conclusion\nFOMO is a significant leap in the image processing space, as Louis Moreau and Mat Kelcey put it during its launch in 2022:\n\nFOMO is a ground-breaking algorithm that brings real-time object detection, tracking, and counting to microcontrollers for the first time.\n\nMultiple possibilities exist for exploring object detection (and, more precisely, counting them) on embedded devices."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#introduction",
    "href": "content/kws_feature_eng/kws_feature_eng.html#introduction",
    "title": "4  Audio Feature Engineering",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this hands-on tutorial, the emphasis is on the critical role that feature engineering plays in optimizing the performance of machine learning models applied to audio classification tasks, such as speech recognition. It is essential to be aware that the performance of any machine learning model relies heavily on the quality of features used, and we will deal with “under-the-hood” mechanics of feature extraction, mainly focusing on Mel-frequency Cepstral Coefficients (MFCCs), a cornerstone in the field of audio signal processing.\nMachine learning models, especially traditional algorithms, don’t understand audio waves. They understand numbers arranged in some meaningful way, i.e., features. These features encapsulate the characteristics of the audio signal, making it easier for models to distinguish between different sounds.\n\nThis tutorial will deal with generating features specifically for audio classification. This can be particularly interesting for applying machine learning to a variety of audio data, whether for speech recognition, music categorization, insect classification based on wingbeat sounds, or other sound analysis tasks"
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#the-kws",
    "href": "content/kws_feature_eng/kws_feature_eng.html#the-kws",
    "title": "4  Audio Feature Engineering",
    "section": "4.2 The KWS",
    "text": "4.2 The KWS\nThe most common TinyML application is Keyword Spotting (KWS), a subset of the broader field of speech recognition. While general speech recognition aims to transcribe all spoken words into text, Keyword Spotting focuses on detecting specific “keywords” or “wake words” in a continuous audio stream. The system is trained to recognize these keywords as predefined phrases or words, such as yes or no. In short, KWS is a specialized form of speech recognition with its own set of challenges and requirements.\nHere a typical KWS Process using MFCC Feature Converter:\n\n\n\n\n\n\n4.2.0.1 Applications of KWS:\n\nVoice Assistants: In devices like Amazon’s Alexa or Google Home, KWS is used to detect the wake word (“Alexa” or “Hey Google”) to activate the device.\nVoice-Activated Controls: In automotive or industrial settings, KWS can be used to initiate specific commands like “Start engine” or “Turn off lights.”\nSecurity Systems: Voice-activated security systems may use KWS to authenticate users based on a spoken passphrase.\nTelecommunication Services: Customer service lines may use KWS to route calls based on spoken keywords.\n\n\n\n4.2.0.2 Differences from General Speech Recognition:\n\nComputational Efficiency: KWS is usually designed to be less computationally intensive than full speech recognition, as it only needs to recognize a small set of phrases.\nReal-time Processing: KWS often operates in real-time and is optimized for low-latency detection of keywords.\nResource Constraints: KWS models are often designed to be lightweight, so they can run on devices with limited computational resources, like microcontrollers or mobile phones.\nFocused Task: While general speech recognition models are trained to handle a broad range of vocabulary and accents, KWS models are fine-tuned to recognize specific keywords, often in noisy environments accurately."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "href": "content/kws_feature_eng/kws_feature_eng.html#introduction-to-audio-signals",
    "title": "4  Audio Feature Engineering",
    "section": "4.3 Introduction to Audio Signals",
    "text": "4.3 Introduction to Audio Signals\nUnderstanding the basic properties of audio signals is crucial for effective feature extraction and, ultimately, for successfully applying machine learning algorithms in audio classification tasks. Audio signals are complex waveforms that capture fluctuations in air pressure over time. These signals can be characterized by several fundamental attributes: sampling rate, frequency, and amplitude.\n\nFrequency and Amplitude: Frequency refers to the number of oscillations a waveform undergoes per unit time and is also measured in Hz. In the context of audio signals, different frequencies correspond to different pitches. Amplitude, on the other hand, measures the magnitude of the oscillations and correlates with the loudness of the sound. Both frequency and amplitude are essential features that capture audio signals’ tonal and rhythmic qualities.\nSampling Rate: The sampling rate, often denoted in Hertz (Hz), defines the number of samples taken per second when digitizing an analog signal. A higher sampling rate allows for a more accurate digital representation of the signal but also demands more computational resources for processing. Typical sampling rates include 44.1 kHz for CD-quality audio and 16 kHz or 8 kHz for speech recognition tasks. Understanding the trade-offs in selecting an appropriate sampling rate is essential for balancing accuracy and computational efficiency. In general, with TinyML projects, we work with 16KHz. Altough music tones can be heard at frequencies up to 20 kHz, voice maxes out at 8 kHz. Traditional telephone systems use an 8 kHz sampling frequency.\n\n\nFor an accurate representation of the signal, the sampling rate must be at least twice the highest frequency present in the signal.\n\n\nTime Domain vs. Frequency Domain: Audio signals can be analyzed in the time and frequency domains. In the time domain, a signal is represented as a waveform where the amplitude is plotted against time. This representation helps to observe temporal features like onset and duration but the signal’s tonal characteristics are not well evidenced. Conversely, a frequency domain representation provides a view of the signal’s constituent frequencies and their respective amplitudes, typically obtained via a Fourier Transform. This is invaluable for tasks that require understanding the signal’s spectral content, such as identifying musical notes or speech phonemes (our case).\n\nThe image below shows the words YES and NO with typical representations in the Time (Raw Audio) and Frequency domains:\n\n\n\n\n\n\n4.3.1 Why Not Raw Audio?\nWhile using raw audio data directly for machine learning tasks may seem tempting, this approach presents several challenges that make it less suitable for building robust and efficient models.\nUsing raw audio data for Keyword Spotting (KWS), for example, on TinyML devices poses challenges due to its high dimensionality (using a 16 kHz sampling rate), computational complexity for capturing temporal features, susceptibility to noise, and lack of semantically meaningful features, making feature extraction techniques like MFCCs a more practical choice for resource-constrained applications.\nHere are some additional details of the critical issues associated with using raw audio:\n\nHigh Dimensionality: Audio signals, especially those sampled at high rates, result in large amounts of data. For example, a 1-second audio clip sampled at 16 kHz will have 16,000 individual data points. High-dimensional data increases computational complexity, leading to longer training times and higher computational costs, making it impractical for resource-constrained environments. Furthermore, the wide dynamic range of audio signals requires a significant amount of bits per sample, while conveying little useful information.\nTemporal Dependencies: Raw audio signals have temporal structures that simple machine learning models may find hard to capture. While recurrent neural networks like LSTMs can model such dependencies, they are computationally intensive and tricky to train on tiny devices.\nNoise and Variability: Raw audio signals often contain background noise and other non-essential elements affecting model performance. Additionally, the same sound can have different characteristics based on various factors such as distance from the microphone, the orientation of the sound source, and acoustic properties of the environment, adding to the complexity of the data.\nLack of Semantic Meaning: Raw audio doesn’t inherently contain semantically meaningful features for classification tasks. Features like pitch, tempo, and spectral characteristics, which can be crucial for speech recognition, are not directly accessible from raw waveform data.\nSignal Redundancy: Audio signals often contain redundant information, with certain portions of the signal contributing little to no value to the task at hand. This redundancy can make learning inefficient and potentially lead to overfitting.\n\nFor these reasons, feature extraction techniques such as Mel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), and simple Spectograms are commonly used to transform raw audio data into a more manageable and informative format. These features capture the essential characteristics of the audio signal while reducing dimensionality and noise, facilitating more effective machine learning."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "href": "content/kws_feature_eng/kws_feature_eng.html#introduction-to-mfccs",
    "title": "4  Audio Feature Engineering",
    "section": "4.4 Introduction to MFCCs",
    "text": "4.4 Introduction to MFCCs\n\n4.4.1 What are MFCCs?\nMel-frequency Cepstral Coefficients (MFCCs) are a set of features derived from the spectral content of an audio signal. They are based on human auditory perceptions and are commonly used to capture the phonetic characteristics of an audio signal. The MFCCs are computed through a multi-step process that includes pre-emphasis, framing, windowing, applying the Fast Fourier Transform (FFT) to convert the signal to the frequency domain, and finally, applying the Discrete Cosine Transform (DCT). The result is a compact representation of the original audio signal’s spectral characteristics.\nThe image below shows the words YES and NO in their MFCC representation:\n\n\n\n\n\n\nThis video explains the Mel Frequency Cepstral Coefficients (MFCC) and how to compute them.\n\n\n\n4.4.2 Why are MFCCs important?\nMFCCs are crucial for several reasons, particularly in the context of Keyword Spotting (KWS) and TinyML:\n\nDimensionality Reduction: MFCCs capture essential spectral characteristics of the audio signal while significantly reducing the dimensionality of the data, making it ideal for resource-constrained TinyML applications.\nRobustness: MFCCs are less susceptible to noise and variations in pitch and amplitude, providing a more stable and robust feature set for audio classification tasks.\nHuman Auditory System Modeling: The Mel scale in MFCCs approximates the human ear’s response to different frequencies, making them practical for speech recognition where human-like perception is desired.\nComputational Efficiency: The process of calculating MFCCs is computationally efficient, making it well-suited for real-time applications on hardware with limited computational resources.\n\nIn summary, MFCCs offer a balance of information richness and computational efficiency, making them popular for audio classification tasks, particularly in constrained environments like TinyML.\n\n\n4.4.3 Computing MFCCs\nThe computation of Mel-frequency Cepstral Coefficients (MFCCs) involves several key steps. Let’s walk through these, which are particularly important for Keyword Spotting (KWS) tasks on TinyML devices.\n\nPre-emphasis: The first step is pre-emphasis, which is applied to accentuate the high-frequency components of the audio signal and balance the frequency spectrum. This is achieved by applying a filter that amplifies the difference between consecutive samples. The formula for pre-emphasis is: y(t) = x(t) - \\(\\alpha\\) x(t-1) , where \\(\\alpha\\) is the pre-emphasis factor, typically around 0.97.\nFraming: Audio signals are divided into short frames (the frame length), usually 20 to 40 milliseconds. This is based on the assumption that frequencies in a signal are stationary over a short period. Framing helps in analyzing the signal in such small time slots. The frame stride (or step) will displace one frame and the adjacent. Those steps could be sequential or overlapped.\nWindowing: Each frame is then windowed to minimize the discontinuities at the frame boundaries. A commonly used window function is the Hamming window. Windowing prepares the signal for a Fourier transform by minimizing the edge effects. The image below shows three frames (10, 20, and 30) and the time samples after windowing (note that the frame length and frame stride are 20 ms):\n\n\n\n\n\n\n\nFast Fourier Transform (FFT) The Fast Fourier Transform (FFT) is applied to each windowed frame to convert it from the time domain to the frequency domain. The FFT gives us a complex-valued representation that includes both magnitude and phase information. However, for MFCCs, only the magnitude is used to calculate the Power Spectrum. The power spectrum is the square of the magnitude spectrum and measures the energy present at each frequency component.\n\n\nThe power spectrum \\(P(f)\\) of a signal \\(x(t)\\) is defined as \\(P(f) = |X(f)|^2\\), where \\(X(f)\\) is the Fourier Transform of \\(x(t)\\). By squaring the magnitude of the Fourier Transform, we emphasize stronger frequencies over weaker ones, thereby capturing more relevant spectral characteristics of the audio signal. This is important in applications like audio classification, speech recognition, and Keyword Spotting (KWS), where the focus is on identifying distinct frequency patterns that characterize different classes of audio or phonemes in speech.\n\n\n\n\n\n\n\nMel Filter Banks: The frequency domain is then mapped to the Mel scale, which approximates the human ear’s response to different frequencies. The idea is to extract more features (more filter banks) in the lower frequencies and less in the high frequencies. Thus, it performs well on sounds distinguished by the human ear. Typically, 20 to 40 triangular filters extract the Mel-frequency energies. These energies are then log-transformed to convert multiplicative factors into additive ones, making them more suitable for further processing.\n\n\n\n\n\n\n\nDiscrete Cosine Transform (DCT): The last step is to apply the Discrete Cosine Transform (DCT) to the log Mel energies. The DCT helps to decorrelate the energies, effectively compressing the data and retaining only the most discriminative features. Usually, the first 12-13 DCT coefficients are retained, forming the final MFCC feature vector."
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "href": "content/kws_feature_eng/kws_feature_eng.html#hands-on-using-python",
    "title": "4  Audio Feature Engineering",
    "section": "4.5 Hands-On using Python",
    "text": "4.5 Hands-On using Python\nLet’s apply what we discussed while working on an actual audio sample. Open the notebook on Google CoLab and extract the MLCC features on your audio samples: [Open In Colab]"
  },
  {
    "objectID": "content/kws_feature_eng/kws_feature_eng.html#conclusion",
    "href": "content/kws_feature_eng/kws_feature_eng.html#conclusion",
    "title": "4  Audio Feature Engineering",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\n\n4.6.1 What Feature Extraction technique should we use?\nMel-frequency Cepstral Coefficients (MFCCs), Mel-Frequency Energies (MFEs), or Spectrogram are techniques for representing audio data, which are often helpful in different contexts.\nIn general, MFCCs are more focused on capturing the envelope of the power spectrum, which makes them less sensitive to fine-grained spectral details but more robust to noise. This is often desirable for speech-related tasks. On the other hand, spectrograms or MFEs preserve more detailed frequency information, which can be advantageous in tasks that require discrimination based on fine-grained spectral content.\n\n4.6.1.1 MFCCs are particularly strong for:\n\nSpeech Recognition: MFCCs are excellent for identifying phonetic content in speech signals.\nSpeaker Identification: They can be used to distinguish between different speakers based on voice characteristics.\nEmotion Recognition: MFCCs can capture the nuanced variations in speech indicative of emotional states.\nKeyword Spotting: Especially in TinyML, where low computational complexity and small feature size are crucial.\n\n\n\n4.6.1.2 Spectrograms or MFEs are often more suitable for:\n\nMusic Analysis: Spectrograms can capture harmonic and timbral structures in music, which is essential for tasks like genre classification, instrument recognition, or music transcription.\nEnvironmental Sound Classification: In recognizing non-speech, environmental sounds (e.g., rain, wind, traffic), the full spectrogram can provide more discriminative features.\nBirdsong Identification: The intricate details of bird calls are often better captured using spectrograms.\nBioacoustic Signal Processing: In applications like dolphin or bat call analysis, the fine-grained frequency information in a spectrogram can be essential.\nAudio Quality Assurance: Spectrograms are often used in professional audio analysis to identify unwanted noises, clicks, or other artifacts."
  },
  {
    "objectID": "content/xiao_kws.html#introduction",
    "href": "content/xiao_kws.html#introduction",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nKeyword Spotting (KWS) is integral to many voice recognition systems, enabling devices to respond to specific words or phrases. While this technology underpins popular devices like Google Assistant or Amazon Alexa, it’s equally applicable and achievable on smaller, low-power devices. This lab will guide you through implementing a KWS system using TinyML on the XIAO ESP32S3 microcontroller board.\nThe XIAO ESP32S3, equipped with Espressif’s ESP32-S3 chip, is a compact and potent microcontroller offering a dual-core Xtensa LX7 processor, integrated Wi-Fi, and Bluetooth. Its balance of computational power, energy efficiency, and versatile connectivity make it a fantastic platform for TinyML applications. Also, with its expansion board, we will have access to the “sense” part of the device, which has a 1600x1200 OV2640 camera, an SD card slot, and a digital microphone. The integrated microphone and the SD card will be essential in this project.\nWe will utilize the Edge Impulse Studio, a powerful, user-friendly platform that simplifies creating and deploying machine learning models onto edge devices. We’ll train a KWS model step-by-step, optimizing and deploying it onto the XIAO ESP32S3 Sense.\nOur model will be designed to recognize keywords that can trigger device wake-up or specific actions (in the case of “YES”), bringing your projects to life with voice-activated commands.\nLeveraging our experience with TensorFlow Lite for Microcontrollers (the engine “under the hood” on the EI Studio), we’ll create a KWS system capable of real-time machine learning on the device.\nAs we progress through the lab, we’ll break down each process stage - from data collection and preparation to model training and deployment - to provide a comprehensive understanding of implementing a KWS system on a microcontroller.\n\n5.1.1 How does a voice assistant work?\nKeyword Spotting (KWS) is critical to many voice assistants, enabling devices to respond to specific words or phrases. To start, it is essential to realize that Voice Assistants on the market, like Google Home or Amazon Echo-Dot, only react to humans when they are “waked up” by particular keywords such as “ Hey Google” on the first one and “Alexa” on the second.\n\n\n\nimg\n\n\nIn other words, recognizing voice commands is based on a multi-stage model or Cascade Detection.\n\n\n\nimg\n\n\nStage 1: A smaller microprocessor inside the Echo Dot or Google Home continuously listens to the sound, waiting for the keyword to be spotted. For such detection, a TinyML model at the edge is used (KWS application).\nStage 2: Only when triggered by the KWS application on Stage 1 is the data sent to the cloud and processed on a larger model.\nThe video below shows an example where I emulate a Google Assistant on a Raspberry Pi (Stage 2), having an Arduino Nano 33 BLE as the tinyML device (Stage 1).\n\n\n\nIf you want to go deeper on the full project, please see my tutorial: Building an Intelligent Voice Assistant From Scratch.\n\nIn this lab, we will focus on Stage 1 (KWS or Keyword Spotting), where we will use the XIAO ESP2S3 Sense, which has a digital microphone for spotting the keyword.\n\n\n5.1.2 The KWS Project\nThe below diagram will give an idea of how the final KWS application should work (during inference):\n\n\n\nimage.png\n\n\nOur KWS application will recognize four classes of sound:\n\nYES (Keyword 1)\nNO (Keyword 2)\nNOISE (no keywords spoken, only background noise is present)\nUNKNOW (a mix of different words than YES and NO)\n\n\nOptionally for real-world projects, it is always advised to include different words than keywords, such as “Noise” (or Background) and “Unknow.”\n\n\n\n5.1.3 The Machine Learning workflow\nThe main component of the KWS application is its model. So, we must train such a model with our specific keywords, noise, and other words (the “unknown”):\n\n\n\nimg"
  },
  {
    "objectID": "content/xiao_kws.html#dataset",
    "href": "content/xiao_kws.html#dataset",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.2 Dataset",
    "text": "5.2 Dataset\nThe critical component of Machine Learning Workflow is the dataset. Once we have decided on specific keywords (YES and NO), we can take advantage of the dataset developed by Pete Warden, “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” This dataset has 35 keywords (with +1,000 samples each), such as yes, no, stop, and go. In other words, we can get 1,500 samples of yes and no.\nYou can download a small portion of the dataset from Edge Studio (Keyword spotting pre-built dataset), which includes samples from the four classes we will use in this project: yes, no, noise, and background. For this, follow the steps below:\n\nDownload the keywords dataset.\nUnzip the file in a location of your choice.\n\nAlthough we have a lot of data from Pete’s dataset, collecting some words spoken by us is advised. When working with accelerometers, creating a dataset with data captured by the same type of sensor was essential. In the case of sound, it is different because what we will classify is, in reality, audio data.\n\nThe key difference between sound and audio is their form of energy. Sound is mechanical wave energy (longitudinal sound waves) that propagate through a medium causing variations in pressure within the medium. Audio is made of electrical energy (analog or digital signals) that represent sound electrically.\n\nThe sound waves should be converted to audio data when we speak a keyword. The conversion should be done by sampling the signal generated by the microphone in 16KHz with a 16-bit depth.\nSo, any device that can generate audio data with this basic specification (16Khz/16bits) will work fine. As a device, we can use the proper XIAO ESP32S3 Sense, a computer, or even your mobile phone.\n\n\n\nsound-audio.png\n\n\nCapturing online Audio Data with Edge Impulse and a smartphone\nIn the lab Motion Classification and Anomaly Detection, we connect our device directly to Edge Impulse Studio for data capturing (having a sampling frequency of 50Hz to 100Hz). For such low frequency, we could use the EI CLI function Data Forwarder, but according to Jan Jongboom, Edge Impulse CTO, audio (16KHz) goes too fast for the data forwarder to be captured. So, once we have the digital data captured by the microphone, we can turn it into a WAV file to be sent to the Studio via Data Uploader (same as we will do with Pete’s dataset).\n\nIf we want to collect audio data directly on the Studio, we can use any smartphone connected online with it. We will not explore this option here, but you can easily follow EI documentation.\n\n\n5.2.1 Capturing (offline) Audio Data with the XIAO ESP32S3 Sense\nThe built-in microphone is the MSM261D3526H1CPM, a PDM digital output MEMS microphone with Multi-modes. Internally, it is connected to the ESP32S3 via an I2S bus using pins IO41 (Clock) and IO41 (Data).\n\n\n\nPasted Graphic 62.png\n\n\nWhat is I2S?\nI2S, or Inter-IC Sound, is a standard protocol for transmitting digital audio from one device to another. It was initially developed by Philips Semiconductor (now NXP Semiconductors). It is commonly used in audio devices such as digital signal processors, digital audio processors, and, more recently, microcontrollers with digital audio capabilities (our case here).\nThe I2S protocol consists of at least three lines:\n\n\n\nimage.png\n\n\n1. Bit (or Serial) clock line (BCLK or CLK): This line toggles to indicate the start of a new bit of data (pin IO42).\n2. Word select line (WS): This line toggles to indicate the start of a new word (left channel or right channel). The Word select clock (WS) frequency defines the sample rate. In our case, L/R on the microphone is set to ground, meaning that we will use only the left channel (mono).\n3. Data line (SD): This line carries the audio data (pin IO41)\nIn an I2S data stream, the data is sent as a sequence of frames, each containing a left-channel word and a right-channel word. This makes I2S particularly suited for transmitting stereo audio data. However, it can also be used for mono or multichannel audio with additional data lines.\nLet’s start understanding how to capture raw data using the microphone. Go to the GitHub projectand download the sketch: XIAOEsp2s3_Mic_Test:\n/*\n  XIAO ESP32S3 Simple Mic Test\n*/\n\n#include &lt;I2S.h&gt;\n\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial) {\n  }\n\n  // start I2S at 16 kHz with 16-bits per sample\n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, 16000, 16)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1); // do nothing\n  }\n}\n\nvoid loop() {\n  // read a sample\n  int sample = I2S.read();\n\n  if (sample && sample != -1 && sample != 1) {\n    Serial.println(sample);\n  }\n}\nThis code is a simple microphone test for the XIAO ESP32S3 using the I2S (Inter-IC Sound) interface. It sets up the I2S interface to capture audio data at a sample rate of 16 kHz with 16 bits per sample and then continuously reads samples from the microphone and prints them to the serial monitor.\nLet’s dig into the code’s main parts:\n\nInclude the I2S library: This library provides functions to configure and use the I2S interface, which is a standard for connecting digital audio devices.\nI2S.setAllPins(-1, 42, 41, -1, -1): This sets up the I2S pins. The parameters are (-1, 42, 41, -1, -1), where the second parameter (42) is the PIN for the I2S clock (CLK), and the third parameter (41) is the PIN for the I2S data (DATA) line. The other parameters are set to -1, meaning those pins are not used.\nI2S.begin(PDM_MONO_MODE, 16000, 16): This initializes the I2S interface in Pulse Density Modulation (PDM) mono mode, with a sample rate of 16 kHz and 16 bits per sample. If the initialization fails, an error message is printed, and the program halts.\nint sample = I2S.read(): This reads an audio sample from the I2S interface.\n\nIf the sample is valid, it is printed on the Serial Monitor and Plotter.\nBelow is a test “whispering” in two different tones.\n\n\n\nplotter.png\n\n\n\n\n5.2.2 Save recorded sound samples (dataset) as .wav audio files to a microSD card.\nLet’s use the onboard SD Card reader to save .wav audio files; we must habilitate the XIAO PSRAM first.\n\nESP32-S3 has only a few hundred kilobytes of internal RAM on the MCU chip. It can be insufficient for some purposes so that ESP32-S3 can use up to 16 MB of external PSRAM (Psuedostatic RAM) connected in parallel with the SPI flash chip. The external memory is incorporated in the memory map and, with certain restrictions, is usable in the same way as internal data RAM.\n\nFor a start, Insert the SD Card on the XIAO as shown in the photo below (the SD Card should be formatted to FAT32).\n\n\n\nimage.png\n\n\nTurn the PSRAM function of the ESP-32 chip on (Arduino IDE): Tools&gt;PSRAM: “OPI PSRAM”&gt;OPI PSRAM\n\n\n\nimage.png\n\n\n\nDownload the sketch Wav_Record_dataset,which you can find on the project’s GitHub.\n\nThis code records audio using the I2S interface of the Seeed XIAO ESP32S3 Sense board, saves the recording as a.wav file on an SD card, and allows for control of the recording process through commands sent from the serial monitor. The name of the audio file is customizable (it should be the class labels to be used with the training), and multiple recordings can be made, each saved in a new file. The code also includes functionality to increase the volume of the recordings.\nLet’s break down the most essential parts of it:\n#include &lt;I2S.h&gt;\n#include \"FS.h\"\n#include \"SD.h\"\n#include \"SPI.h\"\nThose are the necessary libraries for the program. I2S.h allows for audio input, FS.h provides file system handling capabilities, SD.h enables the program to interact with an SD card, and SPI.h handles the SPI communication with the SD card.\n#define RECORD_TIME   10  \n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\n#define WAV_HEADER_SIZE 44\n#define VOLUME_GAIN 2\nHere, various constants are defined for the program.\n\nRECORD_TIME specifies the length of the audio recording in seconds.\nSAMPLE_RATE and SAMPLE_BITS define the audio quality of the recording.\nWAV_HEADER_SIZE specifies the size of the .wav file header.\nVOLUME_GAIN is used to increase the volume of the recording.\n\nint fileNumber = 1;\nString baseFileName;\nbool isRecording = false;\nThese variables keep track of the current file number (to create unique file names), the base file name, and whether the system is currently recording.\nvoid setup() {\n  Serial.begin(115200);\n  while (!Serial);\n  \n  I2S.setAllPins(-1, 42, 41, -1, -1);\n  if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n    Serial.println(\"Failed to initialize I2S!\");\n    while (1);\n  }\n  \n  if(!SD.begin(21)){\n    Serial.println(\"Failed to mount SD Card!\");\n    while (1);\n  }\n  Serial.printf(\"Enter with the label name\\n\");\n}\nThe setup function initializes the serial communication, I2S interface for audio input, and SD card interface. If the I2S did not initialize or the SD card fails to mount, it will print an error message and halt execution.\nvoid loop() {\n  if (Serial.available() &gt; 0) {\n    String command = Serial.readStringUntil('\\n');\n    command.trim();\n    if (command == \"rec\") {\n      isRecording = true;\n    } else {\n      baseFileName = command;\n      fileNumber = 1; //reset file number each time a new basefile name is set\n      Serial.printf(\"Send rec for starting recording label \\n\");\n    }\n  }\n  if (isRecording && baseFileName != \"\") {\n    String fileName = \"/\" + baseFileName + \".\" + String(fileNumber) + \".wav\";\n    fileNumber++;\n    record_wav(fileName);\n    delay(1000); // delay to avoid recording multiple files at once\n    isRecording = false;\n  }\n}\nIn the main loop, the program waits for a command from the serial monitor. If the command is rec, the program starts recording. Otherwise, the command is assumed to be the base name for the .wav files. If it’s currently recording and a base file name is set, it records the audio and saves it as a.wav file. The file names are generated by appending the file number to the base file name.\nvoid record_wav(String fileName)\n{\n  ...\n  \n  File file = SD.open(fileName.c_str(), FILE_WRITE);\n  ...\n  rec_buffer = (uint8_t *)ps_malloc(record_size);\n  ...\n\n  esp_i2s::i2s_read(esp_i2s::I2S_NUM_0, rec_buffer, record_size, &sample_size, portMAX_DELAY);\n  ...\n}\nThis function records audio and saves it as a.wav file with the given name. It starts by initializing the sample_size and record_size variables. record_size is calculated based on the sample rate, size, and desired recording time. Let’s dig into the essential sections;\nFile file = SD.open(fileName.c_str(), FILE_WRITE);\n// Write the header to the WAV file\nuint8_t wav_header[WAV_HEADER_SIZE];\ngenerate_wav_header(wav_header, record_size, SAMPLE_RATE);\nfile.write(wav_header, WAV_HEADER_SIZE);\nThis section of the code opens the file on the SD card for writing and then generates the .wav file header using the generate_wav_header function. It then writes the header to the file.\n// PSRAM malloc for recording\nrec_buffer = (uint8_t *)ps_malloc(record_size);\nif (rec_buffer == NULL) {\n  Serial.printf(\"malloc failed!\\n\");\n  while(1) ;\n}\nSerial.printf(\"Buffer: %d bytes\\n\", ESP.getPsramSize() - ESP.getFreePsram());\nThe ps_malloc function allocates memory in the PSRAM for the recording. If the allocation fails (i.e., rec_buffer is NULL), it prints an error message and halts execution.\n// Start recording\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, rec_buffer, record_size, &sample_size, portMAX_DELAY);\nif (sample_size == 0) {\n  Serial.printf(\"Record Failed!\\n\");\n} else {\n    Serial.printf(\"Record %d bytes\\n\", sample_size);\n  }\nThe i2s_read function reads audio data from the microphone into rec_buffer. It prints an error message if no data is read (sample_size is 0).\n// Increase volume\nfor (uint32_t i = 0; i &lt; sample_size; i += SAMPLE_BITS/8) {\n  (*(uint16_t *)(rec_buffer+i)) &lt;&lt;= VOLUME_GAIN;\n}\nThis section of the code increases the recording volume by shifting the sample values by VOLUME_GAIN.\n// Write data to the WAV file\nSerial.printf(\"Writing to the file ...\\n\");\nif (file.write(rec_buffer, record_size) != record_size)\n  Serial.printf(\"Write file Failed!\\n\");\n\nfree(rec_buffer);\nfile.close();\nSerial.printf(\"Recording complete: \\n\");\nSerial.printf(\"Send rec for a new sample or enter a new label\\n\\n\");\nFinally, the audio data is written to the .wav file. If the write operation fails, it prints an error message. After writing, the memory allocated for rec_buffer is freed, and the file is closed. The function finishes by printing a completion message and prompting the user to send a new command.\nvoid generate_wav_header(uint8_t *wav_header, uint32_t wav_size, uint32_t sample_rate)\n{\n  ...\n  memcpy(wav_header, set_wav_header, sizeof(set_wav_header));\n}\nThe generate_wav_header function creates a.wav file header based on the parameters (wav_size and sample_rate). It generates an array of bytes according to the .wav file format, which includes fields for the file size, audio format, number of channels, sample rate, byte rate, block alignment, bits per sample, and data size. The generated header is then copied into the wav_header array passed to the function.\nNow, upload the code to the XIAO and get samples from the keywords (yes and no). You can also capture noise and other words.\nThe Serial monitor will prompt you to receive the label to be recorded.\n\n\n\nPasted Graphic.png\n\n\nSend the label (for example, yes). The program will wait for another command: rec\n\n\n\nPasted Graphic 2.png\n\n\nAnd the program will start recording new samples every time a command rec is sent. The files will be saved as yes.1.wav, yes.2.wav, yes.3.wav, etc., until a new label (for example, no) is sent. In this case, you should send the command rec for each new sample, which will be saved as no.1.wav, no.2.wav, no.3.wav, etc.\n\n\n\nPasted Graphic 4.png\n\n\nUltimately, we will get the saved files on the SD card.\n\n\n\nimage.png\n\n\nThe files are ready to be uploaded to Edge Impulse Studio\n\n\n5.2.3 Capturing (offline) Audio Data Apps\nAlternatively, you can also use your PC or smartphone to capture audio data with a sampling frequency 16KHz and a bit depth of 16 Bits. A good app for that is Voice Recorder Pro (IOS). You should save your records as .wav files and send them to your computer.\n\n\n\nimage.png\n\n\n\nNote that any app, such as Audacity, can be used for audio recording or even your computer."
  },
  {
    "objectID": "content/xiao_kws.html#training-model-with-edge-impulse-studio",
    "href": "content/xiao_kws.html#training-model-with-edge-impulse-studio",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.3 Training model with Edge Impulse Studio",
    "text": "5.3 Training model with Edge Impulse Studio\n\n5.3.1 Uploading the Data\nWhen the raw dataset is defined and collected (Pete’s dataset + recorded keywords), we should initiate a new project at Edge Impulse Studio:\n\n\n\nPasted Graphic 44.png\n\n\nOnce the project is created, select the Upload Existing Data tool in the Data Acquisition section. Choose the files to be uploaded:\n\n\n\nPasted Graphic 48.png\n\n\nAnd upload them to the Studio (You can automatically split data in train/test). Repete to all classes and all raw data.\n\n\n\nPasted Graphic 46.png\n\n\nThe samples will now appear in the Data acquisition section.\n\n\n\nPasted Graphic 49.png\n\n\nAll data on Pete’s dataset have a 1s length, but the samples recorded in the previous section have 10s and must be split into 1s samples to be compatible.\nClick on three dots after the sample name and select Split sample.\n\n\n\nimage.png\n\n\nOnce inside the tool, split the data into 1-second records. If necessary, add or remove segments:\n\n\n\nimage.png\n\n\nThis procedure should be repeated for all samples.\n\nNote: For longer audio files (minutes), first, split into 10-second segments and after that, use the tool again to get the final 1-second splits.\n\nSuppose we do not split data automatically in train/test during upload. In that case, we can do it manually (using the three dots menu, moving samples individually) or using Perform Train / Test Split on Dashboard - Danger Zone.\n\nWe can optionally check all datasets using the tab Data Explorer.\n\n\n\n5.3.2 Creating Impulse (Pre-Process / Model definition)\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\n\n\n\nPasted Graphic 51.png\n\n\nFirst, we will take the data points with a 1-second window, augmenting the data, sliding that window each 500ms. Note that the option zero-pad data is set. It is essential to fill with zeros samples smaller than 1 second (in some cases, I reduced the 1000 ms window on the split tool to avoid noises and spikes).\nEach 1-second audio sample should be pre-processed and converted to an image (for example, 13 x 49 x 1). We will use MFCC, which extracts features from audio signals using Mel Frequency Cepstral Coefficients, which are great for the human voice.\n\n\n\nimage.png\n\n\nNext, we select KERAS for classification and build our model from scratch by doing Image Classification using Convolution Neural Network).\n\n\n5.3.3 Pre-Processing (MFCC)\nThe next step is to create the images to be trained in the next phase:\nWe can keep the default parameter values or take advantage of the DSP Autotuneparameters option, which we will do.\n\n\n\nimage.png\n\n\nThe result will not spend much memory to pre-process data (only 16KB). Still, the estimated processing time is high, 675 ms for an Espressif ESP-EYE (the closest reference available), with a 240KHz clock (same as our device), but with a smaller CPU ( XTensa LX6, versus the LX7 on the ESP32S). The real inference time should be smaller.\nSuppose we need to reduce the inference time later. In that case, we should return to the pre-processing stage and, for example, reduce the FFT length to 256, change the Number of coefficients, or another parameter.\nFor now, let’s keep the parameters defined by the Autotuning tool. Save parameters and generate the features.\n\n\n\nPasted Graphic 54.png\n\n\n\nIf you want to go further with converting temporal serial data into images using FFT, Spectrogram, etc., you can play with this CoLab: IESTI01_Audio_Raw_Data_Analisys.ipynb.\n\n\n\n5.3.4 Model Design and Training\nWe will use a Convolution Neural Network (CNN) model. The basic architecture is defined with two blocks of Conv1D + MaxPooling (with 8 and 16 neurons, respectively) and a 0.25 Dropout. And on the last layer, after Flattening four neurons, one for each class:\n\n\n\nimage.png\n\n\nAs hyper-parameters, we will have a Learning Rate of 0.005 and a model that will be trained by 100 epochs. We will also include data augmentation, as some noise. The result seems OK:\n\n\n\nimage.png\n\n\nIf you want to understand what is happening “under the hood,” you can download the dataset and run a Jupyter Notebook playing with the code. For example, you can analyze the accuracy by each epoch:\n\n\n\nimage.png\n\n\nThis CoLab Notebook can explain how you can go further: KWS Classifier Project - Looking “Under the hood.”"
  },
  {
    "objectID": "content/xiao_kws.html#testing",
    "href": "content/xiao_kws.html#testing",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.4 Testing",
    "text": "5.4 Testing\nTesting the model with the data put apart before training (Test Data), we got an accuracy of approximately 87%.\n\n\n\nPasted Graphic 58.png\n\n\nInspecting the F1 score, we can see that for YES. We got 0.95, an excellent result once we used this keyword to “trigger” our postprocessing stage (turn on the built-in LED). Even for NO, we got 0.90. The worst result is for unknown, what is OK.\nWe can proceed with the project, but it is possible to perform Live Classification using a smartphone before deployment on our device. Go to the Live Classification section and click on Connect a Development board:\n\n\n\nimage.png\n\n\nPoint your phone to the barcode and select the link.\n\n\n\nimage.png\n\n\nYour phone will be connected to the Studio. Select the option Classification on the app, and when it is running, start testing your keywords, confirming that the model is working with live and real data:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_kws.html#deploy-and-inference",
    "href": "content/xiao_kws.html#deploy-and-inference",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.5 Deploy and Inference",
    "text": "5.5 Deploy and Inference\nThe Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and press the button Build.\n\n\n\nPasted Graphic 59.png\n\n\nNow it is time for a real test. We will make inferences wholly disconnected from the Studio. Let’s change one of the ESP32 code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab look for your project, and select esp32/esp32_microphone:\n\n\n\nimage.png\n\n\nThis code was created for the ESP-EYE built-in microphone, which should be adapted for our device.\nStart changing the libraries to handle the I2S bus:\n\n\n\nimage.png\n\n\nBy:\n#include &lt;I2S.h&gt;\n#define SAMPLE_RATE 16000U\n#define SAMPLE_BITS 16\nInitialize the IS2 microphone at setup(), including the lines:\nvoid setup()\n{\n...\n    I2S.setAllPins(-1, 42, 41, -1, -1);\n    if (!I2S.begin(PDM_MONO_MODE, SAMPLE_RATE, SAMPLE_BITS)) {\n      Serial.println(\"Failed to initialize I2S!\");\n    while (1) ;\n...\n}\nOn the static void capture_samples(void* arg) function, replace the line 153 that reads data from I2S mic:\n\n\n\nimage.png\n\n\nBy:\n/* read data at once from i2s */\nesp_i2s::i2s_read(esp_i2s::I2S_NUM_0, (void*)sampleBuffer, i2s_bytes_to_read, &bytes_read, 100);\nOn function static bool microphone_inference_start(uint32_t n_samples), we should comment or delete lines 198 to 200, where the microphone initialization function is called. This is unnecessary because the I2S microphone was already initialized during the setup().\n\n\n\nimage.png\n\n\nFinally, on static void microphone_inference_end(void) function, replace line 243:\n\n\n\nimage.png\n\n\nBy:\nstatic void microphone_inference_end(void)\n{\n    free(sampleBuffer);\n    ei_free(inference.buffer);\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_kws.html#postprocessing",
    "href": "content/xiao_kws.html#postprocessing",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.6 Postprocessing",
    "text": "5.6 Postprocessing\nNow that we know the model is working by detecting our keywords, let’s modify the code to see the internal LED going on every time a YES is detected.\nYou should initialize the LED:\n#define LED_BUILT_IN 21\n...\nvoid setup()\n{\n...\n  pinMode(LED_BUILT_IN, OUTPUT); // Set the pin as output\n  digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n...\n}\nAnd change the // print the predictions portion of the previous code (on loop():\nint pred_index = 0;     // Initialize pred_index\nfloat pred_value = 0;   // Initialize pred_value\n\n// print the predictions\nei_printf(\"Predictions \");\nei_printf(\"(DSP: %d ms., Classification: %d ms., Anomaly: %d ms.)\",\n     result.timing.dsp, result.timing.classification, result.timing.anomaly);\nei_printf(\": \\n\");\nfor (size_t ix = 0; ix &lt; EI_CLASSIFIER_LABEL_COUNT; ix++) {\n      ei_printf(\"    %s: \", result.classification[ix].label);\n      ei_printf_float(result.classification[ix].value);\n      ei_printf(\"\\n\");\n\n      if (result.classification[ix].value &gt; pred_value){\n         pred_index = ix;\n         pred_value = result.classification[ix].value;\n      }\n}\n\n// show the inference result on LED\nif (pred_index == 3){\n    digitalWrite(LED_BUILT_IN, LOW); //Turn on\n}\nelse{\n   digitalWrite(LED_BUILT_IN, HIGH); //Turn off\n}\nYou can find the complete code on the project’s GitHub. Upload the sketch to your board and test some real inferences:\n\n\n\nimage.png\n\n\nThe idea is that the LED will be ON whenever the keyword YES is detected. In the same way, instead of turning on an LED, this could be a “trigger” for an external device, as we saw in the introduction."
  },
  {
    "objectID": "content/xiao_kws.html#conclusion",
    "href": "content/xiao_kws.html#conclusion",
    "title": "5  Keyword Spotting (KWS)",
    "section": "5.7 Conclusion",
    "text": "5.7 Conclusion\nThe Seeed XIAO ESP32S3 Sense is a giant tiny device! However, it is powerful, trustworthy, not expensive, low power, and has suitable sensors to be used on the most common embedded machine learning applications such as vision and sound. Even though Edge Impulse does not officially support XIAO ESP32S3 Sense (yet!), we realized that using the Studio for training and deployment is straightforward.\n\nOn my GitHub repository, you will find the last version all the codes used on this project and the previous ones of the XIAO ESP32S3 series.\n\nBefore we finish, consider that Sound Classification is more than just voice. For example, you can develop TinyML projects around sound in several areas, such as:\n\nSecurity (Broken Glass detection)\nIndustry (Anomaly Detection)\nMedical (Snore, Toss, Pulmonary diseases)\nNature (Beehive control, insect sound)"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#introduction",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#introduction",
    "title": "6  DSP - Spectral Features",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nTinyML projects related to motion (or vibration) involve data from IMUs (usually accelerometers and Gyroscopes). These time-series type datasets should be preprocessed before inputting them into a Machine Learning model training, which is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this complexity with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features Block for Inertial sensors.\nBut how does it work under the hood? Let’s dig into it."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#extracting-features-review",
    "title": "6  DSP - Spectral Features",
    "section": "6.2 Extracting Features Review",
    "text": "6.2 Extracting Features Review\nExtracting features from a dataset captured with inertial sensors, such as accelerometers, involves processing and analyzing the raw data. Accelerometers measure the acceleration of an object along one or more axes (typically three, denoted as X, Y, and Z). These measurements can be used to understand various aspects of the object’s motion, such as movement patterns and vibrations. Here’s a high-level overview of the process:\nData collection: First, we need to gather data from the accelerometers. Depending on the application, data may be collected at different sampling rates. It’s essential to ensure that the sampling rate is high enough to capture the relevant dynamics of the studied motion (the sampling rate should be at least double the maximum relevant frequency present in the signal).\nData preprocessing: Raw accelerometer data can be noisy and contain errors or irrelevant information. Preprocessing steps, such as filtering and normalization, can help clean and standardize the data, making it more suitable for feature extraction.\n\nThe Studio does not perform normalization or standardization, so sometimes, when working with Sensor Fusion, it could be necessary to perform this step before uploading data to the Studio. This is particularly crucial in sensor fusion projects, as seen in this tutorial, Sensor Data Fusion with Spresense and CommonSense.\n\nSegmentation: Depending on the nature of the data and the application, dividing the data into smaller segments or windows may be necessary. This can help focus on specific events or activities within the dataset, making feature extraction more manageable and meaningful. The window size and overlap (window span) choice depend on the application and the frequency of the events of interest. As a rule of thumb, we should try to capture a couple of “data cycles.”\nFeature extraction: Once the data is preprocessed and segmented, you can extract features that describe the motion’s characteristics. Some typical features extracted from accelerometer data include:\n\nTime-domain features describe the data’s statistical properties within each segment, such as mean, median, standard deviation, skewness, kurtosis, and zero-crossing rate.\nFrequency-domain features are obtained by transforming the data into the frequency domain using techniques like the Fast Fourier Transform (FFT). Some typical frequency-domain features include the power spectrum, spectral energy, dominant frequencies (amplitude and frequency), and spectral entropy.\nTime-frequency domain features combine the time and frequency domain information, such as the Short-Time Fourier Transform (STFT) or the Discrete Wavelet Transform (DWT). They can provide a more detailed understanding of how the signal’s frequency content changes over time.\n\nIn many cases, the number of extracted features can be large, which may lead to overfitting or increased computational complexity. Feature selection techniques, such as mutual information, correlation-based methods, or principal component analysis (PCA), can help identify the most relevant features for a given application and reduce the dimensionality of the dataset. The Studio can help with such feature-relevant calculations.\nLet’s explore in more detail a typical TinyML Motion Classification project covered in this series of Hands-Ons."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#a-tinyml-motion-classification-project",
    "title": "6  DSP - Spectral Features",
    "section": "6.3 A TinyML Motion Classification project",
    "text": "6.3 A TinyML Motion Classification project\n\n\n\n\n\nIn the hands-on project, Motion Classification and Anomaly Detection, we simulated mechanical stresses in transport, where our problem was to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (pallets in a Truck or Train)\nLift (pallets being handled by Fork-Lift)\nIdle (pallets in Storage houses)\n\nThe accelerometers provided the data on the pallet (or container).\n\n\n\n\n\nBelow is one sample (raw data) of 10 seconds, captured with a sampling frequency of 50Hz:\n\n\n\n\n\n\nThe result is similar when this analysis is done over another dataset with the same principle, using a different sampling frequency, 62.5Hz instead of 50Hz."
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#data-pre-processing",
    "title": "6  DSP - Spectral Features",
    "section": "6.4 Data Pre-Processing",
    "text": "6.4 Data Pre-Processing\nThe raw data captured by the accelerometer (a “time series” data) should be converted to “tabular data” using one of the typical Feature Extraction methods described in the last section.\nWe should segment the data using a sliding window over the sample data for feature extraction. The project captured accelerometer data every 10 seconds with a sample rate of 62.5 Hz. A 2-second window captures 375 data points (3 axis x 2 seconds x 62.5 samples). The window is slid every 80ms, creating a larger dataset where each instance has 375 “raw features.”\n\n\n\n\n\nOn the Studio, the previous version (V1) of the Spectral Analysis Block extracted as time-domain features only the RMS, and for the frequency-domain, the peaks and frequency (using FFT) and the power characteristics (PSD) of the signal over time resulting in a fixed tabular dataset of 33 features (11 per each axis),\n\n\n\n\n\nThose 33 features were the Input tensor of a Neural Network Classifier.\nIn 2022, Edge Impulse released version 2 of the Spectral Analysis block, which we will explore here.\n\n6.4.1 Edge Impulse - Spectral Analysis Block V.2 under the hood\nIn Version 2, Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness (in the next version)\nKurtosis (in the next version)\n\nIn this link, we can have more details about the feature extraction.\n\nClone the public project. You can also follow the explanation, playing with the code using my Google CoLab Notebook: Edge Impulse Spectral Analysis Block Notebook.\n\nStart importing the libraries:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy.stats import skew, kurtosis\nfrom scipy import signal\nfrom scipy.signal import welch\nfrom scipy.stats import entropy\nfrom sklearn import preprocessing\nimport pywt\n\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['lines.linewidth'] = 3\nFrom the studied project, let’s choose a data sample from accelerometers as below:\n\nWindow size of 2 seconds: [2,000] ms\nSample frequency: [62.5] Hz\nWe will choose the [None] filter (for simplicity) and a\nFFT length: [16].\n\nf =  62.5 # Hertz\nwind_sec = 2 # seconds\nFFT_Lenght = 16\naxis = ['accX', 'accY', 'accZ']\nn_sensors = len(axis)\n\n\n\n\n\nSelecting the Raw Features on the Studio Spectral Analysis tab, we can copy all 375 data points of a particular 2-second window to the clipboard.\n\n\n\n\n\nPaste the data points to a new variable data:\ndata=[-5.6330, 0.2376, 9.8701, -5.9442, 0.4830, 9.8701, -5.4217, ...]\nNo_raw_features = len(data)\nN = int(No_raw_features/n_sensors)\nThe total raw features are 375, but we will work with each axis individually, where N= 125 (number of samples per axis).\nWe aim to understand how Edge Impulse gets the processed features.\n\n\n\n\n\nSo, you should also past the processed features on a variable (to compare the calculated features in Python with the ones provided by the Studio) :\nfeatures = [2.7322, -0.0978, -0.3813, 2.3980, 3.8924, 24.6841, 9.6303, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nThe total number of processed features is 39, which means 13 features/axis.\nLooking at those 13 features closely, we will find 3 for the time domain (RMS, Skewness, and Kurtosis):\n\n[rms] [skew] [kurtosis]\n\nand 10 for the frequency domain (we will return to this later).\n\n[spectral skew][spectral kurtosis][Spectral Power 1] ... [Spectral Power 8]\n\nSplitting raw data per sensor\nThe data has samples from all axes; let’s split and plot them separately:\ndef plot_data(sensors, axis, title):\n    [plt.plot(x, label=y) for x,y in zip(sensors, axis)]\n    plt.legend(loc='lower right')\n    plt.title(title)\n    plt.xlabel('#Sample')\n    plt.ylabel('Value')\n    plt.box(False)\n    plt.grid()\n    plt.show()\n\naccX = data[0::3]\naccY = data[1::3]\naccZ = data[2::3]\nsensors = [accX, accY, accZ] \nplot_data(sensors, axis, 'Raw Features')\n\n\n\n\n\nSubtracting the mean\nNext, we should subtract the mean from the data. Subtracting the mean from a data set is a common data pre-processing step in statistics and machine learning. The purpose of subtracting the mean from the data is to center the data around zero. This is important because it can reveal patterns and relationships that might be hidden if the data is not centered.\nHere are some specific reasons why subtracting the mean can be helpful:\n\nIt simplifies analysis: By centering the data, the mean becomes zero, making some calculations simpler and easier to interpret.\nIt removes bias: If the data is biased, subtracting the mean can remove it and allow for a more accurate analysis.\nIt can reveal patterns: Centering the data can help uncover patterns that might be hidden if the data is not centered. For example, centering the data can help you identify trends over time if you analyze a time series dataset.\nIt can improve performance: In some machine learning algorithms, centering the data can improve performance by reducing the influence of outliers and making the data more easily comparable. Overall, subtracting the mean is a simple but powerful technique that can be used to improve the analysis and interpretation of data.\n\ndtmean = [(sum(x)/len(x)) for x in sensors]\n[print('mean_'+x+'= ', round(y, 4)) for x,y in zip(axis, dtmean)][0]\n\naccX = [(x - dtmean[0]) for x in accX]\naccY = [(x - dtmean[1]) for x in accY]\naccZ = [(x - dtmean[2]) for x in accZ]\nsensors = [accX, accY, accZ]\n\nplot_data(sensors, axis, 'Raw Features - Subctract the Mean')"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-domain-statistical-features",
    "title": "6  DSP - Spectral Features",
    "section": "6.5 Time Domain Statistical features",
    "text": "6.5 Time Domain Statistical features\nRMS Calculation\nThe RMS value of a set of values (or a continuous-time waveform) is the square root of the arithmetic mean of the squares of the values or the square of the function that defines the continuous waveform. In physics, the RMS value of an electrical current is defined as the “value of the direct current that dissipates the same power in a resistor.”\nIn the case of a set of n values {𝑥1, 𝑥2, …, 𝑥𝑛}, the RMS is:\n\n\n\n\n\n\nNOTE that the RMS value is different for the original raw data, and after subtracting the mean\n\n# Using numpy and standartized data (subtracting mean)\nrms = [np.sqrt(np.mean(np.square(x))) for x in sensors]\nWe can compare the calculated RMS values here with the ones presented by Edge Impulse:\n[print('rms_'+x+'= ', round(y, 4)) for x,y in zip(axis, rms)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nprint(features[0:N_feat:N_feat_axis])\nrms_accX=  2.7322\nrms_accY=  0.7833\nrms_accZ=  0.1383\nCompared with Edge Impulse result features:\n[2.7322, 0.7833, 0.1383]\nSkewness and kurtosis calculation\nIn statistics, skewness and kurtosis are two ways to measure the shape of a distribution.\nHere, we can see the sensor values distribution:\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(13, 4))\nsns.kdeplot(accX, fill=True, ax=axes[0])\nsns.kdeplot(accY, fill=True, ax=axes[1])\nsns.kdeplot(accZ, fill=True, ax=axes[2])\naxes[0].set_title('accX')\naxes[1].set_title('accY')\naxes[2].set_title('accZ')\nplt.suptitle('IMU Sensors distribution', fontsize=16, y=1.02)\nplt.show()\n\n\n\n\n\nSkewness is a measure of the asymmetry of a distribution. This value can be positive or negative.\n\n\n\n\n\n\nA negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.\nA positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.\nA zero value indicates no skewness in the distribution at all, meaning the distribution is perfectly symmetrical.\n\nskew = [skew(x, bias=False) for x in sensors]\n[print('skew_'+x+'= ', round(y, 4)) for x,y in zip(axis, skew)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[1:N_feat:N_feat_axis]\nskew_accX=  -0.099\nskew_accY=  0.1756\nskew_accZ=  6.9463\nCompared with Edge Impulse result features:\n[-0.0978, 0.1735, 6.8629]\nKurtosis is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a normal distribution.\n\n\n\n\n\n\nThe kurtosis of a normal distribution is zero.\nIf a given distribution has a negative kurtosis, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\nIf a given distribution has a positive kurtosis , it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.\n\nkurt = [kurtosis(x, bias=False) for x in sensors]\n[print('kurt_'+x+'= ', round(y, 4)) for x,y in zip(axis, kurt)][0]\nprint(\"\\nCompare with Edge Impulse result features\")\nfeatures[2:N_feat:N_feat_axis]\nkurt_accX=  -0.3475\nkurt_accY=  1.2673\nkurt_accZ=  68.1123\nCompared with Edge Impulse result features:\n[-0.3813, 1.1696, 65.3726]"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#spectral-features",
    "title": "6  DSP - Spectral Features",
    "section": "6.6 Spectral features",
    "text": "6.6 Spectral features\nThe filtered signal is passed to the Spectral power section, which computes the FFT to generate the spectral features.\nSince the sampled window is usually larger than the FFT size, the window will be broken into frames (or “sub-windows”), and the FFT is calculated over each frame.\nFFT length - The FFT size. This determines the number of FFT bins and the resolution of frequency peaks that can be separated. A low number means more signals will average together in the same FFT bin, but it also reduces the number of features and model size. A high number will separate more signals into separate bins, generating a larger model.\n\nThe total number of Spectral Power features will vary depending on how you set the filter and FFT parameters. With No filtering, the number of features is 1/2 of the FFT Length.\n\nSpectral Power - Welch’s method\nWe should use Welch’s method to split the signal on the frequency domain in bins and calculate the power spectrum for each bin. This method divides the signal into overlapping segments, applies a window function to each segment, computes the periodogram of each segment using DFT, and averages them to obtain a smoother estimate of the power spectrum.\n# Function used by Edge Impulse instead of scipy.signal.welch().\ndef welch_max_hold(fx, sampling_freq, nfft, n_overlap):\n    n_overlap = int(n_overlap)\n    spec_powers = [0 for _ in range(nfft//2+1)]\n    ix = 0\n    while ix &lt;= len(fx):\n        # Slicing truncates if end_idx &gt; len, and rfft will auto-zero pad\n        fft_out = np.abs(np.fft.rfft(fx[ix:ix+nfft], nfft))\n        spec_powers = np.maximum(spec_powers, fft_out**2/nfft)\n        ix = ix + (nfft-n_overlap)\n    return np.fft.rfftfreq(nfft, 1/sampling_freq), spec_powers\nApplying the above function to 3 signals:\nfax,Pax = welch_max_hold(accX, fs, FFT_Lenght, 0)\nfay,Pay = welch_max_hold(accY, fs, FFT_Lenght, 0)\nfaz,Paz = welch_max_hold(accZ, fs, FFT_Lenght, 0)\nspecs = [Pax, Pay, Paz ]\nWe can plot the Power Spectrum P(f):\nplt.plot(fax,Pax, label='accX')\nplt.plot(fay,Pay, label='accY')\nplt.plot(faz,Paz, label='accZ')\nplt.legend(loc='upper right')\nplt.xlabel('Frequency (Hz)')\n#plt.ylabel('PSD [V**2/Hz]')\nplt.ylabel('Power')\nplt.title('Power spectrum P(f) using Welch's method')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\n\n\nBesides the Power Spectrum, we can also include the skewness and kurtosis of the features in the frequency domain (should be available on a new version):\nspec_skew = [skew(x, bias=False) for x in specs]\nspec_kurtosis = [kurtosis(x, bias=False) for x in specs]\nLet’s now list all Spectral features per axis and compare them with EI:\nprint(\"EI Processed Spectral features (accX): \")\nprint(features[3:N_feat_axis][0:])\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[0],4))\nprint (round(spec_kurtosis[0],4))\n[print(round(x, 4)) for x in Pax[1:]][0]\nEI Processed Spectral features (accX):\n2.398, 3.8924, 24.6841, 9.6303, 8.4867, 7.7793, 2.9963, 5.6242, 3.4198, 4.2735\nCalculated features:\n2.9069 8.5569 24.6844 9.6304 8.4865 7.7794 2.9964 5.6242 3.4198 4.2736\nprint(\"EI Processed Spectral features (accY): \")\nprint(features[16:26][0:]) #13: 3+N_feat_axis;  26 = 2x N_feat_axis\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[1],4))\nprint (round(spec_kurtosis[1],4))\n[print(round(x, 4)) for x in Pay[1:]][0]\nEI Processed Spectral features (accY):\n0.9426, -0.8039, 5.429, 0.999, 1.0315, 0.9459, 1.8117, 0.9088, 1.3302, 3.112\nCalculated features:\n1.1426 -0.3886 5.4289 0.999 1.0315 0.9458 1.8116 0.9088 1.3301 3.1121\nprint(\"EI Processed Spectral features (accZ): \")\nprint(features[29:][0:]) #29: 3+(2*N_feat_axis);\nprint(\"\\nCalculated features:\")\nprint (round(spec_skew[2],4))\nprint (round(spec_kurtosis[2],4))\n[print(round(x, 4)) for x in Paz[1:]][0]\nEI Processed Spectral features (accZ):\n0.3117, -1.3812, 0.0606, 0.057, 0.0567, 0.0976, 0.194, 0.2574, 0.2083, 0.166\nCalculated features:\n0.3781 -1.4874 0.0606 0.057 0.0567 0.0976 0.194 0.2574 0.2083 0.166"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#time-frequency-domain",
    "title": "6  DSP - Spectral Features",
    "section": "6.7 Time-frequency domain",
    "text": "6.7 Time-frequency domain\n\n6.7.1 Wavelets\nWavelet is a powerful technique for analyzing signals with transient features or abrupt changes, such as spikes or edges, which are difficult to interpret with traditional Fourier-based methods.\nWavelet transforms work by breaking down a signal into different frequency components and analyzing them individually. The transformation is achieved by convolving the signal with a wavelet function, a small waveform centered at a specific time and frequency. This process effectively decomposes the signal into different frequency bands, each of which can be analyzed separately.\nOne of the critical benefits of wavelet transforms is that they allow for time-frequency analysis, which means that they can reveal the frequency content of a signal as it changes over time. This makes them particularly useful for analyzing non-stationary signals, which vary over time.\nWavelets have many practical applications, including signal and image compression, denoising, feature extraction, and image processing.\nLet’s select Wavelet on the Spectral Features block in the same project:\n\nType: Wavelet\nWavelet Decomposition Level: 1\nWavelet: bior1.3\n\n\n\n\n\n\nThe Wavelet Function\nwavelet_name='bior1.3'\nnum_layer = 1\n\nwavelet = pywt.Wavelet(wavelet_name)\n[phi_d,psi_d,phi_r,psi_r,x] = wavelet.wavefun(level=5)\nplt.plot(x, psi_d, color='red')\nplt.title('Wavelet Function')\nplt.ylabel('Value')\nplt.xlabel('Time')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\n\n\nAs we did before, let’s copy and past the Processed Features:\n\n\n\n\n\nfeatures = [3.6251, 0.0615, 0.0615, -7.3517, -2.7641, 2.8462, 5.0924, ...]\nN_feat = len(features)\nN_feat_axis = int(N_feat/n_sensors)\nEdge Impulse computes the Discrete Wavelet Transform (DWT) for each one of the Wavelet Decomposition levels selected. After that, the features will be extracted.\nIn the case of Wavelets, the extracted features are basic statistical values, crossing values, and entropy. There are, in total, 14 features per layer as below:\n\n[11] Statiscal Features: n5, n25, n75, n95, mean, median, standard deviation (std), variance (var) root mean square (rms), kurtosis, and skewness (skew).\n[2] Crossing Features: Zero crossing rate (zcross) and mean crossing rate (mcross) are the times that the signal passes through the baseline (y = 0) and the average level (y = u) per unit of time, respectively\n[1] Complexity Feature: Entropy is a characteristic measure of the complexity of the signal\n\nAll the above 14 values are calculated for each Layer (including L0, the original signal)\n\nThe total number of features varies depending on how you set the filter and the number of layers. For example, with [None] filtering and Level[1], the number of features per axis will be 14 x 2 (L0 and L1) = 28. For the three axes, we will have a total of 84 features.\n\n\n\n6.7.2 Wavelet Analysis\nWavelet analysis decomposes the signal (accX, accY, and accZ) into different frequency components using a set of filters, which separate these components into low-frequency (slowly varying parts of the signal containing long-term patterns), such as accX_l1, accY_l1, accZ_l1 and, high-frequency (rapidly varying parts of the signal containing short-term patterns) components, such as accX_d1, accY_d1, accZ_d1, permitting the extraction of features for further analysis or classification.\nOnly the low-frequency components (approximation coefficients, or cA) will be used. In this example, we assume only one level (Single-level Discrete Wavelet Transform), where the function will return a tuple. With a multilevel decomposition, the “Multilevel 1D Discrete Wavelet Transform”, the result will be a list (for detail, please see: Discrete Wavelet Transform (DWT) )\n(accX_l1, accX_d1) = pywt.dwt(accX, wavelet_name)\n(accY_l1, accY_d1) = pywt.dwt(accY, wavelet_name)\n(accZ_l1, accZ_d1) = pywt.dwt(accZ, wavelet_name)\nsensors_l1 = [accX_l1, accY_l1, accZ_l1]\n\n# Plot power spectrum versus frequency\nplt.plot(accX_l1, label='accX')\nplt.plot(accY_l1, label='accY')\nplt.plot(accZ_l1, label='accZ')\nplt.legend(loc='lower right')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Wavelet Approximation')\nplt.grid()\nplt.box(False)\nplt.show()\n\n\n\n\n\n\n\n6.7.3 Feature Extraction\nLet’s start with the basic statistical features. Note that we apply the function for both the original signals and the resultant cAs from the DWT:\ndef calculate_statistics(signal):\n    n5 = np.percentile(signal, 5)\n    n25 = np.percentile(signal, 25)\n    n75 = np.percentile(signal, 75)\n    n95 = np.percentile(signal, 95)\n    median = np.percentile(signal, 50)\n    mean = np.mean(signal)\n    std = np.std(signal)\n    var = np.var(signal)\n    rms = np.sqrt(np.mean(np.square(signal)))\n    return [n5, n25, n75, n95, median, mean, std, var, rms]\n \nstat_feat_l0 = [calculate_statistics(x) for x in sensors]\nstat_feat_l1 = [calculate_statistics(x) for x in sensors_l1]\nThe Skelness and Kurtosis:\nskew_l0 = [skew(x, bias=False) for x in sensors]\nskew_l1 = [skew(x, bias=False) for x in sensors_l1]\nkurtosis_l0 = [kurtosis(x, bias=False) for x in sensors]\nkurtosis_l1 = [kurtosis(x, bias=False) for x in sensors_l1]\nZero crossing (zcross) is the number of times the wavelet coefficient crosses the zero axis. It can be used to measure the signal’s frequency content since high-frequency signals tend to have more zero crossings than low-frequency signals.\nMean crossing (mcross), on the other hand, is the number of times the wavelet coefficient crosses the mean of the signal. It can be used to measure the amplitude since high-amplitude signals tend to have more mean crossings than low-amplitude signals.\ndef getZeroCrossingRate(arr):\n    my_array = np.array(arr)\n    zcross = float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) &lt; 0).su    m())/len(arr)))\n    return zcross\n\ndef getMeanCrossingRate(arr):\n    mcross = getZeroCrossingRate(np.array(arr) - np.mean(arr))\n    return mcross\n\ndef calculate_crossings(list):\n    zcross=[]\n    mcross=[]\n    for i in range(len(list)):\n        zcross_i = getZeroCrossingRate(list[i])\n        zcross.append(zcross_i)\n        mcross_i = getMeanCrossingRate(list[i])\n        mcross.append(mcross_i)\n    return zcross, mcross\n\ncross_l0 = calculate_crossings(sensors)\ncross_l1 = calculate_crossings(sensors_l1)\nIn wavelet analysis, entropy refers to the degree of disorder or randomness in the distribution of wavelet coefficients. Here, we used Shannon entropy, which measures a signal’s uncertainty or randomness. It is calculated as the negative sum of the probabilities of the different possible outcomes of the signal multiplied by their base 2 logarithm. In the context of wavelet analysis, Shannon entropy can be used to measure the complexity of the signal, with higher values indicating greater complexity.\ndef calculate_entropy(signal, base=None):\n    value, counts = np.unique(signal, return_counts=True)\n    return entropy(counts, base=base)\n\nentropy_l0 = [calculate_entropy(x) for x in sensors]\nentropy_l1 = [calculate_entropy(x) for x in sensors_l1]\nLet’s now list all the wavelet features and create a list by layers.\nL1_features_names = [\"L1-n5\", \"L1-n25\", \"L1-n75\", \"L1-n95\", \"L1-median\", \"L1-mean\", \"L1-std\", \"L1-var\", \"L1-rms\", \"L1-skew\", \"L1-Kurtosis\", \"L1-zcross\", \"L1-mcross\", \"L1-entropy\"]\n\nL0_features_names = [\"L0-n5\", \"L0-n25\", \"L0-n75\", \"L0-n95\", \"L0-median\", \"L0-mean\", \"L0-std\", \"L0-var\", \"L0-rms\", \"L0-skew\", \"L0-Kurtosis\", \"L0-zcross\", \"L0-mcross\", \"L0-entropy\"]\n\nall_feat_l0 = []\nfor i in range(len(axis)):\n    feat_l0 = stat_feat_l0[i]+[skew_l0[i]]+[kurtosis_l0[i]]+[cross_l0[0][i]]+[cross_l0[1][i]]+[entropy_l0[i]]\n    [print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L0_features_names, feat_l0)][0]\n    all_feat_l0.append(feat_l0)\nall_feat_l0 = [item for sublist in all_feat_l0 for item in sublist]\nprint(f\"\\nAll L0 Features = {len(all_feat_l0)}\")\n\nall_feat_l1 = []\nfor i in range(len(axis)):\nfeat_l1 = stat_feat_l1[i]+[skew_l1[i]]+[kurtosis_l1[i]]+[cross_l1[0][i]]+[cross_l1[1][i]]+[entropy_l1[i]]\n[print(axis[i]+' '+x+'= ', round(y, 4)) for x,y in zip(L1_features_names, feat_l1)][0]\nall_feat_l1.append(feat_l1)\nall_feat_l1 = [item for sublist in all_feat_l1 for item in sublist]\nprint(f\"\\nAll L1 Features = {len(all_feat_l1)}\")"
  },
  {
    "objectID": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "href": "content/dsp_spectral_features_block/dsp_spectral_features_block.html#conclusion",
    "title": "6  DSP - Spectral Features",
    "section": "6.8 Conclusion",
    "text": "6.8 Conclusion\nEdge Impulse Studio is a powerful online platform that can handle the pre-processing task for us. Still, given our engineering perspective, we want to understand what is happening under the hood. This knowledge will help us find the best options and hyper-parameters for tuning our projects.\nDaniel Situnayake wrote in his blog: “Raw sensor data is highly dimensional and noisy. Digital signal processing algorithms help us sift the signal from the noise. DSP is an essential part of embedded engineering, and many edge processors have on-board acceleration for DSP. As an ML engineer, learning basic DSP gives you superpowers for handling high-frequency time series data in your models.” I recommend you read Dan’s excellent post in its totality: nn to cpp: What you need to know about porting deep learning models to the edge."
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#introduction",
    "href": "content/xiao_motion_class_ad.html#introduction",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThe XIAO ESP32S3 Sense, with its built-in camera and mic, is a versatile device. But what if you need to add another type of sensor, such as an IMU? No problem! One of the standout features of the XIAO ESP32S3 is its multiple pins that can be used as an I2C bus (SDA/SCL pins), making it a suitable platform for sensor integration.\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#installing-the-imu",
    "href": "content/xiao_motion_class_ad.html#installing-the-imu",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.2 Installing the IMU",
    "text": "7.2 Installing the IMU\nWhen selecting your IMU, the market offers a wide range of devices, each with unique features and capabilities. You could choose, for example, the ADXL362 (3-axis), MAX21100 (6-axis), MPU6050 (6-axis), LIS3DHTR (3-axis), or the LCM20600Seeed Grove— (6-axis), which is part of the IMU 9DOF (lcm20600+AK09918). This variety allows you to tailor your choice to your project’s specific needs.\nFor this project, we will use an IMU, the MPU6050 (or 6500), a low-cost (less than 2.00 USD) 6-axis Accelerometer/Gyroscope unit.\n\nAt the end of the lab, we will also comment on using the LCM20600.\n\nThe MPU-6500 is a 6-axis Motion Tracking device that combines a 3-axis gyroscope, 3-axis accelerometer, and a Digital Motion ProcessorTM (DMP) in a small 3x3x0.9mm package. It also features a 4096-byte FIFO that can lower the traffic on the serial bus interface and reduce power consumption by allowing the system processor to burst read sensor data and then go into a low-power mode.\nWith its dedicated I2C sensor bus, the MPU-6500 directly accepts inputs from external I2C devices. MPU-6500, with its 6-axis integration, on-chip DMP, and run-time calibration firmware, enables manufacturers to eliminate the costly and complex selection, qualification, and system-level integration of discrete devices, guaranteeing optimal motion performance for consumers. MPU-6500 is also designed to interface with multiple non-inertial digital sensors, such as pressure sensors, on its auxiliary I2C port.\n\n\n\nimage.png\n\n\n\nUsually, the libraries available are for MPU6050, but they work for both devices.\n\nConnecting the HW\nConnect the IMU to the XIAO according to the below diagram:\n\nMPU6050 SCL –&gt; XIAO D5\nMPU6050 SDA –&gt; XIAO D4\nMPU6050 VCC –&gt; XIAO 3.3V\nMPU6050 GND –&gt; XIAO GND\n\n\n\n\nDrawing.png\n\n\nInstall the Library\nGo to Arduino Library Manager and type MPU6050. Install the latest version.\n\n\n\nPasted Graphic 16.png\n\n\nDownload the sketch MPU6050_Acc_Data_Acquisition.in:\n/*\n * Based on I2C device class (I2Cdev) Arduino sketch for MPU6050 class by Jeff Rowberg &lt;jeff@rowberg.net&gt;\n * and Edge Impulse Data Forwarder Exampe (Arduino) - https://docs.edgeimpulse.com/docs/cli-data-forwarder\n * \n * Developed by M.Rovai @11May23\n */\n\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\n\n#define FREQUENCY_HZ        50\n#define INTERVAL_MS         (1000 / (FREQUENCY_HZ + 1))\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n\n// convert factor g to m/s2 ==&gt; [-32768, +32767] ==&gt; [-2g, +2g]\n#define CONVERT_G_TO_MS2    (9.81/(16384.0/(1.+ACC_RANGE))) \n\nstatic unsigned long last_interval_ms = 0;\n\nMPU6050 imu;\nint16_t ax, ay, az;\n\nvoid setup() {\n  \n    Serial.begin(115200);\n\n    \n    // initialize device\n    Serial.println(\"Initializing I2C devices...\");\n    Wire.begin();\n    imu.initialize();\n    delay(10);\n    \n//    // verify connection\n//    if (imu.testConnection()) {\n//      Serial.println(\"IMU connected\");\n//    }\n//    else {\n//      Serial.println(\"IMU Error\");\n//    }\n    delay(300);\n    \n    //Set MCU 6050 OffSet Calibration \n    imu.setXAccelOffset(-4732);\n    imu.setYAccelOffset(4703);\n    imu.setZAccelOffset(8867);\n    imu.setXGyroOffset(61);\n    imu.setYGyroOffset(-73);\n    imu.setZGyroOffset(35);\n    \n    /* Set full-scale accelerometer range.\n     * 0 = +/- 2g\n     * 1 = +/- 4g\n     * 2 = +/- 8g\n     * 3 = +/- 16g\n     */\n    imu.setFullScaleAccelRange(ACC_RANGE);\n}\n\nvoid loop() {\n\n      if (millis() &gt; last_interval_ms + INTERVAL_MS) {\n        last_interval_ms = millis();\n        \n        // read raw accel/gyro measurements from device\n        imu.getAcceleration(&ax, &ay, &az);\n\n        // converting to m/s2\n        float ax_m_s2 = ax * CONVERT_G_TO_MS2;\n        float ay_m_s2 = ay * CONVERT_G_TO_MS2;\n        float az_m_s2 = az * CONVERT_G_TO_MS2;\n\n        Serial.print(ax_m_s2); \n        Serial.print(\"\\t\");\n        Serial.print(ay_m_s2); \n        Serial.print(\"\\t\");\n        Serial.println(az_m_s2); \n      }\n}\nSome comments about the code:\nNote that the values generated by the accelerometer and gyroscope have a range: [-32768, +32767], so for example, if the default accelerometer range is used, the range in Gs should be: [-2g, +2g]. So, “1G” means 16384.\nFor conversion to m/s2, for example, you can define the following:\n#define CONVERT_G_TO_MS2 (9.81/16384.0)\nIn the code, I left an option (ACC_RANGE) to be set to 0 (+/-2G) or 1 (+/- 4G). We will use +/-4G; that should be enough for us. In this case.\nWe will capture the accelerometer data on a frequency of 50Hz, and the acceleration data will be sent to the Serial Port as meters per squared second (m/s2).\nWhen you ran the code with the IMU resting over your table, the accelerometer data shown on the Serial Monitor should be around 0.00, 0.00, and 9.81. If the values are a lot different, you should calibrate the IMU.\nThe MCU6050 can be calibrated using the sketch: mcu6050-calibration.ino.\nRun the code. The following will be displayed on the Serial Monitor:\n\n\n\nPasted Graphic 19.png\n\n\nSend any character (in the above example, “x”), and the calibration should start.\n\nNote that A message MPU6050 connection failed. Ignore this message. For some reason, imu.testConnection() is not returning a correct result.\n\nIn the end, you will receive the offset values to be used on all your sketches:\n\n\n\nPasted Graphic 20.png\n\n\nTake the values and use them on the setup:\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\nNow, run the sketch MPU6050_Acc_Data_Acquisition.in:\nOnce you run the above sketch, open the Serial Monitor:\n\n\n\nPasted Graphic 21.png\n\n\nOr check the Plotter:\n\n\n\nPasted Graphic 23.png\n\n\nMove your device in the three axes. You should see the variation on Plotter:\n\n\n\nPasted Graphic 22.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#the-tinyml-motion-classification-project",
    "href": "content/xiao_motion_class_ad.html#the-tinyml-motion-classification-project",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.3 The TinyML Motion Classification Project",
    "text": "7.3 The TinyML Motion Classification Project\nFor our lab, we will simulate mechanical stresses in transport. Our problem will be to classify four classes of movement:\n\nMaritime (pallets in boats)\nTerrestrial (palettes in a Truck or Train)\nLift (Palettes being handled by Fork-Lift)\nIdle (Palettes in Storage houses)\n\nSo, to start, we should collect data. Then, accelerometers will provide the data on the palette (or container).\n\n\n\nimg\n\n\nFrom the above images, we can see that primarily horizontal movements should be associated with the “Terrestrial class,” Vertical movements with the “Lift Class,” no activity with the “Idle class,” and movement on all three axes to Maritime class."
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#connecting-the-device-to-edge-impulse",
    "href": "content/xiao_motion_class_ad.html#connecting-the-device-to-edge-impulse",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.4 Connecting the device to Edge Impulse",
    "text": "7.4 Connecting the device to Edge Impulse\nFor data collection, we should first connect our device to the Edge Impulse Studio, which will also be used for data pre-processing, model training, testing, and deployment.\n\nFollow the instructions hereto install the Node.jsand Edge Impulse CLI on your computer.\n\nOnce the XIAO ESP32S3 is not a fully supported development board by Edge Impulse, we should, for example, use the CLI Data Forwarder to capture data from our sensor and send it to the Studio, as shown in this diagram:\n\n\n\nimg\n\n\n\nYou can alternately capture your data “offline,” store them on an SD card or send them to your computer via Bluetooth or Wi-Fi. In this video, you can learn alternative ways to send data to the Edge Impulse Studio.\n\nConnect your device to the serial port and run the previous code to capture IMU (Accelerometer) data, “printing them” on the serial. This will allow the Edge Impulse Studio to “capture” them.\nGo to the Edge Impulse page and create a project.\n\n\n\nimage.png\n\n\n\nThe maximum length for an Arduino library name is 63 characters. Note that the Studio will name the final library using your project name and include “_inference” to it. The name I chose initially did not work when I tried to deploy the Arduino library because it resulted in 64 characters. So, I need to change it by taking out the “anomaly detection” part.\n\nStart the CLI Data Forwarderon your terminal, entering (if it is the first time) the following command:\n$ edge-impulse-data-forwarder –clean\n$ edge-impulse-data-forwarder --clean\nNext, enter your EI credentials and choose your project, variables, and device names:\n\n\n\nimage.png\n\n\nGo to your EI Project and verify if the device is connected (the dot should be green):\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#data-collection",
    "href": "content/xiao_motion_class_ad.html#data-collection",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.5 Data Collection",
    "text": "7.5 Data Collection\nAs discussed before, we should capture data from all four Transportation Classes. Imagine that you have a container with a built-in accelerometer:\n\n\n\nboat.jpg\n\n\nNow imagine your container is on a boat, facing an angry ocean, on a truck, etc.:\n\nMaritime (pallets in boats)\n\nMove the XIAO in all directions, simulating an undulatory boat movement.\n\nTerrestrial (palettes in a Truck or Train)\n\nMove the XIAO over a horizontal line.\n\nLift (Palettes being handled by\n\nMove the XIAO over a vertical line.\n\nIdle (Palettes in Storage houses)\n\nLeave the XIAO over the table.\n\n\n\n\n\nidle.jpg\n\n\nBelow is one sample (raw data) of 10 seconds:\n\n\n\nimg\n\n\nYou can capture, for example, 2 minutes (twelve samples of 10 seconds each) for the four classes. Using the “3 dots” after each one of the samples, select 2, moving them for the Test set (or use the automatic Train/Test Split tool on the Danger Zone of Dashboard tab). Below, you can see the result datasets:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#data-pre-processing",
    "href": "content/xiao_motion_class_ad.html#data-pre-processing",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.6 Data Pre-Processing",
    "text": "7.6 Data Pre-Processing\nThe raw data type captured by the accelerometer is a “time series” and should be converted to “tabular data”. We can do this conversion using a sliding window over the sample data. For example, in the below figure,\n\n\n\nimage.png\n\n\nWe can see 10 seconds of accelerometer data captured with a sample rate (SR) of 50Hz. A 2-second window will capture 300 data points (3 axis x 2 seconds x 50 samples). We will slide this window each 200ms, creating a larger dataset where each instance has 300 raw features.\n\nYou should use the best SR for your case, considering Nyquist’s theorem, which states that a periodic signal must be sampled at more than twice the signal’s highest frequency component.\n\nData preprocessing is a challenging area for embedded machine learning. Still, Edge Impulse helps overcome this with its digital signal processing (DSP) preprocessing step and, more specifically, the Spectral Features.\nOn the Studio, this dataset will be the input of a Spectral Analysis block, which is excellent for analyzing repetitive motion, such as data from accelerometers. This block will perform a DSP (Digital Signal Processing), extracting features such as “FFT” or “Wavelets”. In the most common case, FFT, the Time Domain Statistical features per axis/channel are:\n\nRMS\nSkewness\nKurtosis\n\nAnd the Frequency Domain Spectral features per axis/channel are:\n\nSpectral Power\nSkewness\nKurtosis\n\nSo, for example, for an FFT length of 32 points, the Spectral Analysis Block’s resulting output will be 21 features per axis (a total of 63 features).\nThose 63 features will be the Input Tensor of a Neural Network Classifier and the Anomaly Detection model (K-Means).\n\nYou can learn more by digging into the lab DSP - Spectral Features"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#model-design",
    "href": "content/xiao_motion_class_ad.html#model-design",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.7 Model Design",
    "text": "7.7 Model Design\nOur classifier will be a Dense Neural Network (DNN) that will have 63 neurons on its input layer, two hidden layers with 20 and 10 neurons, and an output layer with four neurons (one per each class), as shown here:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#impulse-design",
    "href": "content/xiao_motion_class_ad.html#impulse-design",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.8 Impulse Design",
    "text": "7.8 Impulse Design\nAn impulse takes raw data, uses signal processing to extract features, and then uses a learning block to classify new data.\nWe also take advantage of a second model, the K-means, that can be used for Anomaly Detection. If we imagine that we could have our known classes as clusters, any sample that could not fit on that could be an outlier, an anomaly (for example, a container rolling out of a ship on the ocean).\n\n\n\nimg\n\n\n\nImagine our XIAO rolling or moving upside-down, on a movement complement different from the one trained\n\n\n\n\nimg\n\n\nBelow is our final Impulse design:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#generating-features",
    "href": "content/xiao_motion_class_ad.html#generating-features",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.9 Generating features",
    "text": "7.9 Generating features\nAt this point in our project, we have defined the pre-processing method and the model designed. Now, it is time to have the job done. First, let’s take the raw data (time-series type) and convert it to tabular data. Go to the Spectral Features tab and select Save Parameters:\n\n\n\nimage.png\n\n\nAt the top menu, select the Generate Features option and the Generate Features button. Each 2-second window data will be converted into one data point of 63 features.\n\nThe Feature Explorer will show those data in 2D using UMAP. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualization similarly to t-SNE but also for general non-linear dimension reduction.\n\nThe visualization allows one to verify that the classes present an excellent separation, which indicates that the classifier should work well.\n\n\n\nimage.png\n\n\nOptionally, you can analyze the relative importance of each feature for one class compared with other classes."
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#training",
    "href": "content/xiao_motion_class_ad.html#training",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.10 Training",
    "text": "7.10 Training\nOur model has four layers, as shown below:\n\n\n\nimage.png\n\n\nAs hyperparameters, we will use a Learning Rate of 0.005 and 20% of data for validation for 30 epochs. After training, we can see that the accuracy is 97%.\n\n\n\nimage.png\n\n\nFor anomaly detection, we should choose the suggested features that are precisely the most important in feature extraction. The number of clusters will be 32, as suggested by the Studio:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#testing",
    "href": "content/xiao_motion_class_ad.html#testing",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.11 Testing",
    "text": "7.11 Testing\nUsing 20% of the data left behind during the data capture phase, we can verify how our model will behave with unknown data; if not 100% (what is expected), the result was not that good (8%), mainly due to the terrestrial class. Once we have four classes (which output should add 1.0), we can set up a lower threshold for a class to be considered valid (for example, 0.4):\n\n\n\nimage.png\n\n\nNow, the Test accuracy will go up to 97%.\n\n\n\nimage.png\n\n\nYou should also use your device (which is still connected to the Studio) and perform some Live Classification.\n\nBe aware that here you will capture real data with your device and upload it to the Studio, where an inference will be taken using the trained model (But the model is NOT in your device)."
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#deploy",
    "href": "content/xiao_motion_class_ad.html#deploy",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.12 Deploy",
    "text": "7.12 Deploy\nNow it is time for magic˜! The Studio will package all the needed libraries, preprocessing functions, and trained models, downloading them to your computer. You should select the option Arduino Library, and at the bottom, choose Quantized (Int8) and Build. A Zip file will be created and downloaded to your computer.\n\n\n\nimage.png\n\n\nOn your Arduino IDE, go to the Sketch tab, select the option Add.ZIP Library, and Choose the.zip file downloaded by the Studio:\n\n\n\nimage.png"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#inference",
    "href": "content/xiao_motion_class_ad.html#inference",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.13 Inference",
    "text": "7.13 Inference\nNow, it is time for a real test. We will make inferences that are wholly disconnected from the Studio. Let’s change one of the code examples created when you deploy the Arduino Library.\nIn your Arduino IDE, go to the File/Examples tab and look for your project, and on examples, select nano_ble_sense_accelerometer:\n\n\n\nimage.png\n\n\nOf course, this is not your board, but we can have the code working with only a few changes.\nFor example, at the beginning of the code, you have the library related to Arduino Sense IMU:\n/* Includes --------------------------------------------------------------- */\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include &lt;Arduino_LSM9DS1.h&gt;\nChange the “includes” portion with the code related to the IMU:\n#include &lt;XIAO-ESP32S3-Motion-Classification_inferencing.h&gt;\n#include \"I2Cdev.h\"\n#include \"MPU6050.h\"\n#include \"Wire.h\"\nChange the Constant Defines\n/* Constant defines ------------------------------------------------------- */\nMPU6050 imu;\nint16_t ax, ay, az;\n\n#define ACC_RANGE           1 // 0: -/+2G; 1: +/-4G\n#define CONVERT_G_TO_MS2    (9.81/(16384/(1.+ACC_RANGE)))\n#define MAX_ACCEPTED_RANGE  (2*9.81)+(2*9.81)*ACC_RANGE\nOn the setup function, initiate the IMU set the off-set values and range:\n// initialize device\nSerial.println(\"Initializing I2C devices...\");\nWire.begin();\nimu.initialize();\ndelay(10);\n\n//Set MCU 6050 OffSet Calibration \nimu.setXAccelOffset(-4732);\nimu.setYAccelOffset(4703);\nimu.setZAccelOffset(8867);\nimu.setXGyroOffset(61);\nimu.setYGyroOffset(-73);\nimu.setZGyroOffset(35);\n\nimu.setFullScaleAccelRange(ACC_RANGE);\nAt the loop function, the buffers buffer[ix], buffer[ix + 1], and buffer[ix + 2] will receive the 3-axis data captured by the accelerometer. On the original code, you have the line:\nIMU.readAcceleration(buffer[ix], buffer[ix + 1], buffer[ix + 2]);\nChange it with this block of code:\nimu.getAcceleration(&ax, &ay, &az);       \nbuffer[ix + 0] = ax;\nbuffer[ix + 1] = ay;\nbuffer[ix + 2] = az;\nYou should change the order of the following two blocks of code. First, you make the conversion to raw data to “Meters per squared second (ms2)”, followed by the test regarding the maximum acceptance range (that here is in ms2, but on Arduino, was in Gs):\nbuffer[ix + 0] *= CONVERT_G_TO_MS2;\nbuffer[ix + 1] *= CONVERT_G_TO_MS2;\nbuffer[ix + 2] *= CONVERT_G_TO_MS2;\n\nfor (int i = 0; i &lt; 3; i++) {\n     if (fabs(buffer[ix + i]) &gt; MAX_ACCEPTED_RANGE) {\n        buffer[ix + i] = ei_get_sign(buffer[ix + i]) * MAX_ACCEPTED_RANGE;\n     }\n}\nAnd that is it! You can now upload the code to your device and proceed with the inferences. The complete code is available on the project’s GitHub.\nNow you should try your movements, seeing the result of the inference of each class on the images:\n\n\n\n\nAnd of course some “anomaly”, for example, puting the XIAO upside-down. The anomaly score will be over 1:"
  },
  {
    "objectID": "content/xiao_motion_class_ad.html#conclusion.",
    "href": "content/xiao_motion_class_ad.html#conclusion.",
    "title": "7  Motion Classification and Anomaly Detection",
    "section": "7.14 Conclusion.",
    "text": "7.14 Conclusion.\nRegarding the IMU, this project used the low-cost MPU6050 but could also use other IMUs, for example, the LCM20600 (6-axis), which is part of the Seeed Grove - IMU 9DOF (lcm20600+AK09918). You can took advantage of this senso, witch has integrated a Grove connector, which can be helpful in the case you use the XIAO with an extension board, as shown below:\n\n\n\nGrove-ICM2060-small.jpg\n\n\nYou can follow the instructions here to connect the IMU with the MCU. Only note that for using the Grove ICM20600 Accelerometer, it is essential to update the files I2Cdev.cpp and I2Cdev.h that you will download from the library provided by Seeed Studio. For that, replace both files from this link. You can find on the GitHub project a sketch for testing the IMU: accelerometer_test.ino.\n\nOn the projet’s GitHub repository, you will find the last version of all codes and other docs: XIAO-ESP32S3 - IMU."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "TinyML4D\nTinyML Made Easy, an eBook collection of a series of Hands-On tutorials, is part of the TinyML4D, an initiative to make Embedded Machine Learning (TinyML) education available to everyone, explicitly enabling innovative solutions for the unique challenges Developing Countries face."
  },
  {
    "objectID": "references.html#to-learn-more",
    "href": "references.html#to-learn-more",
    "title": "References",
    "section": "To learn more:",
    "text": "To learn more:\n\nOnline Courses\n\nHarvard School of Engineering and Applied Sciences - CS249r: Tiny Machine Learning\nProfessional Certificate in Tiny Machine Learning (TinyML) – edX/Harvard\nIntroduction to Embedded Machine Learning - Coursera/Edge Impulse\nComputer Vision with Embedded Machine Learning - Coursera/Edge Impulse\nUNIFEI-IESTI01 TinyML: “Machine Learning for Embedding Devices”\n\n\n\nBooks\n\n“Python for Data Analysis by Wes McKinney”\n“Deep Learning with Python” by François Chollet - GitHub Notebooks\n“TinyML” by Pete Warden, Daniel Situnayake\n“TinyML Cookbook 2nd Edition” by Gian Marco Iodice\n“Technical Strategy for AI Engineers, In the Era of Deep Learning” by Andrew Ng\n“AI at the Edge” book by Daniel Situnayake, Jenny Plunkett\n“MACHINE LEARNING SYSTEMS for TinyML” Collaborative effort\n\n\n\nProjects Repository\n\nEdge Impulse Expert Network"
  }
]